{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_05 - Generative models.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJTPHuKWSAa5"
      },
      "source": [
        "# **Generative models tutorial**\n",
        "In today's tutorial you will learn how to use deep generative models to generate handwritten digits.\n",
        "\n",
        "We will use [**TensorFlow**](https://ekababisong.org/gcp-ml-seminar/tensorflow/) framework and [**Keras**](https://keras.io/) open-source library to rapidly prototype deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VAn2qHwSK92"
      },
      "source": [
        "# **Useful modules import**\n",
        "First of all, it is necessary to import useful modules used during the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUe9W0qB8jPV"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myhe7QzlEB8j"
      },
      "source": [
        "# **Utility functions**\n",
        "Execute the following code to define some utility functions used in the tutorial:\n",
        "- **plot_2d_data** plots 2D labeled data;\n",
        "- **plot_history** draws in a graph the loss trend over epochs on both training and validation sets. Moreover, if provided, it draws in the same graph also the trend of the given metric;\n",
        "- **plot_generated_images** visualizes a set of generated images;\n",
        "- **plot_gan_losses** draws in a graph the generator and discriminator loss trends over epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSFMBWrTEF5q"
      },
      "source": [
        "def plot_2d_data(data_2d,y,titles=None,figsize=(7,7)):\n",
        "  _,axs=plt.subplots(1,len(data_2d),figsize=figsize)\n",
        "\n",
        "  for i in range(len(data_2d)):\n",
        "    if (titles!=None):\n",
        "      axs[i].set_title(titles[i])\n",
        "    scatter=axs[i].scatter(data_2d[i][:,0],data_2d[i][:,1],s=1,c=y[i],cmap=plt.cm.Paired)\n",
        "    axs[i].legend(*scatter.legend_elements())\n",
        "\n",
        "def plot_history(history,metric=None):\n",
        "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "  epoch_count=len(history.history['loss'])\n",
        "\n",
        "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],label='train_loss',color='orange')\n",
        "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],label='val_loss',color = line1.get_color(), linestyle = '--')\n",
        "  ax1.set_xlim([1,epoch_count])\n",
        "  ax1.set_ylim([0, max(max(history.history['loss']),max(history.history['val_loss']))])\n",
        "  ax1.set_ylabel('loss',color = line1.get_color())\n",
        "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
        "  ax1.set_xlabel('Epochs')\n",
        "  _=ax1.legend(loc='lower left')\n",
        "\n",
        "  if (metric!=None):\n",
        "    ax2 = ax1.twinx()\n",
        "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],label='train_'+metric)\n",
        "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],label='val_'+metric,color = line2.get_color(), linestyle = '--')\n",
        "    ax2.set_ylim([0, max(max(history.history[metric]),max(history.history['val_'+metric]))])\n",
        "    ax2.set_ylabel(metric,color=line2.get_color())\n",
        "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
        "    _=ax2.legend(loc='upper right')\n",
        "\n",
        "def plot_generated_images(generated_images, nrows, ncols,no_space_between_plots=False, figsize=(10, 10)):\n",
        "  _, axs = plt.subplots(nrows, ncols,figsize=figsize,squeeze=False)\n",
        "\n",
        "  for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "      axs[i,j].axis('off')\n",
        "      axs[i,j].imshow(generated_images[i][j], cmap='gray')\n",
        "\n",
        "  if no_space_between_plots:\n",
        "    plt.subplots_adjust(wspace=0,hspace=0)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_gan_losses(d_losses,g_losses):\n",
        "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "  epoch_count=len(d_losses)\n",
        "\n",
        "  line1,=ax1.plot(range(1,epoch_count+1),d_losses,label='discriminator_loss',color='orange')\n",
        "  ax1.set_ylim([0, max(d_losses)])\n",
        "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
        "  _=ax1.legend(loc='lower left')\n",
        "\n",
        "  ax2 = ax1.twinx()\n",
        "  line2,=ax2.plot(range(1,epoch_count+1),g_losses,label='generator_loss')\n",
        "  ax2.set_xlim([1,epoch_count])\n",
        "  ax2.set_ylim([0, max(g_losses)])\n",
        "  ax2.set_xlabel('Epochs')\n",
        "  ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
        "  _=ax2.legend(loc='upper right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1TUo_9gSVrl"
      },
      "source": [
        "# **Dataset**\n",
        "The [**digits MNIST**](http://yann.lecun.com/exdb/mnist/) dataset, containing 28x28 grayscale images of the 10 digits, will be used.\n",
        "\n",
        "The goal is to developd and train deep generative models to generate images representing realistic handwritten digits.\n",
        "\n",
        "The following code loads in memory the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxb5bmFMTOYG"
      },
      "source": [
        "category_count=10 #Number of digit categories\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
        "\n",
        "print('Train data flatten shape: ',train_x.shape)\n",
        "print('Train label shape: ',train_y.shape)\n",
        "print('Test data flatten shape: ',test_x.shape)\n",
        "print('Test label shape: ',test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV1m0tVeTgT1"
      },
      "source": [
        "### **Visualization**\n",
        "Randomly selected images can be shown by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SKWjHAsTiU2"
      },
      "source": [
        "image_count=10\n",
        "\n",
        "_, axs = plt.subplots(1, image_count,figsize=(15, 10))\n",
        "for i in range(image_count):\n",
        "  random_idx=random.randint(0,train_x.shape[0])\n",
        "  axs[i].imshow(train_x[random_idx],cmap='gray')\n",
        "  axs[i].axis('off')\n",
        "  axs[i].set_title(train_y[random_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7rWiGfsn0zz"
      },
      "source": [
        "### **Split data into training and validation sets**\n",
        "In order to avoid overfitting during training, it is necessary to have a separate dataset (called validation set), in addition to the training and test datasets, to choose the optimal value for the hyperparameters.\n",
        "\n",
        "For this reason, *train_x* and *train_y* are divided into training and validation sets using the [**train_test_split**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function provided by Scikit-learn.\n",
        "\n",
        "The *val_size* variable represents the percentage (or the absolute number) of patterns to include in the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn0ycXm6n_qY"
      },
      "source": [
        "val_size=10000\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size = val_size,random_state = 1,shuffle=True)\n",
        "\n",
        "print('Train data flatten shape: ',train_x.shape)\n",
        "print('Train label shape: ',train_y.shape)\n",
        "print('Validation data flatten shape: ',val_x.shape)\n",
        "print('Validation label shape: ',val_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-01WrFET4ke"
      },
      "source": [
        "### **Intensity range normalization**\n",
        "Pixel intensity is usually represented as discrete values in the range [0;255]. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4Z5WtXZT5Te"
      },
      "source": [
        "print('Min value: ',train_x.min())\n",
        "print('Max value: ',train_x.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiIk4qRBT_vm"
      },
      "source": [
        "Such values could produce math range errors with the activation function or make training unstable. To overcome these issues, a simple normalization step can be applied by dividing all values by 255 to get continuous values in the range [0;1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNmO6732T--2"
      },
      "source": [
        "train_x = train_x/255.0\n",
        "val_x = val_x/255.0\n",
        "test_x = test_x/255.0\n",
        "\n",
        "print('Min value: ',train_x.min())\n",
        "print('Max value: ',train_x.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HhdPHrtU4Vv"
      },
      "source": [
        "### **Image linearization**\n",
        "The images need to be converted from 2D matrices to vectors before they can be used as input of flatten networks.\n",
        "\n",
        "The following code use the Numpy function [**reshape**](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) to flatten the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzGsRNd-U5Ym"
      },
      "source": [
        "original_image_shape=(train_x.shape[1], train_x.shape[2])\n",
        "\n",
        "train_x_flatten=np.reshape(train_x,(train_x.shape[0],-1))\n",
        "val_x_flatten=np.reshape(val_x,(val_x.shape[0],-1))\n",
        "test_x_flatten=np.reshape(test_x,(test_x.shape[0],-1))\n",
        "\n",
        "print('Train data flatten shape: ',train_x_flatten.shape)\n",
        "print('Validation data flatten shape: ',val_x_flatten.shape)\n",
        "print('Test data flatten shape: ',test_x_flatten.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WepiqxoUOjzL"
      },
      "source": [
        "# **Variational autoencoder (VAE)**\n",
        "In this section a variational autoencoder is trained to generate handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eACoW6oXM-o"
      },
      "source": [
        "## **Model definition**\n",
        "The following function creates a variational autoencoder given:\n",
        "- the number of input features (*input_count*);\n",
        "- the number of neurons for each hidden layer (*neuron_count_per_hidden_layer*);\n",
        "- the dimension of the latent space (*encoded_dim*);\n",
        "- the string identifier of the activation function of the hidden layers (*hidden_activation*);\n",
        "- the string identifier of the activation function of the output layer (*output_activation*).\n",
        "\n",
        "The function returns the encoder and the decoder models as well as the whole autoencoder.\n",
        "\n",
        "The following image shows the architecture of a generic VAE.\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/GenerativeModels/vae_architecture.png width=\"500\">\n",
        "\n",
        "In this case, the Keras [**Sequential**](https://keras.io/guides/sequential_model/) class cannot be used because the last layer of the encoder is connected to both mean and variance layers. \n",
        "\n",
        "For these situations, Keras provides the [**Model**](https://keras.io/api/models/model/) class to group layers into an object with training and inference features. It can be created by passing the input and output layers to the constructor.\n",
        "\n",
        "[**Keras layers API**](https://keras.io/api/layers/) offers a wide range of built-in layers ready for use, including:\n",
        "- [**Input**](https://keras.io/api/layers/core_layers/input/) - the input of the model. Note that, you can also omit the **Input** layer. In that case the model doesn't have any weights until the first call to a training/evaluation method (since it is not yet built);\n",
        "- [**Dense**](https://keras.io/api/layers/core_layers/dense/) - a fully-connected layer;\n",
        "- [**Lamda**](https://keras.io/api/layers/core_layers/lambda/) - to wrap simple expressions as a Layer object.\n",
        "\n",
        "<u>Note that, instead of variance, the encoder returns its natural logarithm to bring stability and ease of training. When needed, it will be transformed back to the original space using the following properties:</u>\n",
        "\n",
        "$$\\sigma^2=e^{ln(\\sigma^2)}$$\n",
        "$$\\sigma=e^{\\frac{1}{2}\\cdot ln(\\sigma^2)}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUsEp0cuXSf3"
      },
      "source": [
        "def build_vae(input_count,neuron_count_per_hidden_layer,encoded_dim,hidden_activation,output_activation):\n",
        "    #Encoder\n",
        "    encoder_input = layers.Input(shape=input_count, name='encoder_input')\n",
        "    \n",
        "    prev_layer=encoder_input\n",
        "    for neuron_count in neuron_count_per_hidden_layer:\n",
        "      hidden_layer=layers.Dense(neuron_count,activation=hidden_activation)(prev_layer)\n",
        "      prev_layer=hidden_layer\n",
        "    \n",
        "    mu = layers.Dense(encoded_dim, name='mu')(prev_layer)\n",
        "    log_var = layers.Dense(encoded_dim, name='log_var')(prev_layer)\n",
        "    \n",
        "    encoder = keras.Model(encoder_input, [mu, log_var], name='encoder')\n",
        "\n",
        "    #Decoder\n",
        "    decoder_input = layers.Input(shape=(encoded_dim,), name='decoder_input')\n",
        "\n",
        "    prev_layer=decoder_input\n",
        "    for neuron_count in reversed(neuron_count_per_hidden_layer):\n",
        "      hidden_layer=layers.Dense(neuron_count,activation=hidden_activation)(prev_layer)\n",
        "      prev_layer=hidden_layer\n",
        "    \n",
        "    decoder_output_layer=layers.Dense(input_count,activation=output_activation, name='decoder_output')(prev_layer)\n",
        "\n",
        "    decoder = keras.Model(decoder_input, decoder_output_layer, name='decoder')\n",
        "\n",
        "    #Sampling layer\n",
        "    s = layers.Lambda(sampling, output_shape=(encoded_dim,), name='s')([mu, log_var])\n",
        "\n",
        "    #VAE\n",
        "    vae=keras.Model(encoder.input, decoder(s),name='vae')\n",
        "    \n",
        "    return vae,encoder,decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2KOWw1SsxI7"
      },
      "source": [
        "The following sampling function (called by the **Lamda** layer) samples random points from the latent space using the *reparameterization trick*:\n",
        "\n",
        "$$\\mathbf{s= \\boldsymbol{\\sigma_x} \\odot \\boldsymbol{\\varepsilon} + \\boldsymbol{\\mu}_x, \\boldsymbol{\\varepsilon} \\sim \\mathcal{N} (0,1)}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKe0LzP2sxyb"
      },
      "source": [
        "def sampling(args):\n",
        "    mu, log_var = args\n",
        "    batch_size = K.shape(mu)[0]\n",
        "    dim = K.int_shape(mu)[1]\n",
        "    epsilon = K.random_normal(shape=(batch_size, dim), mean=0., stddev=1.0)\n",
        "    return K.exp(0.5 * log_var) * epsilon + mu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HC_ntgshO4n"
      },
      "source": [
        "## **Model creation**\n",
        "The following code creates a variational autoencoder by calling the **build_vae** function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WZhajLOhQ-1"
      },
      "source": [
        "vae, vae_encoder, vae_decoder=build_vae(train_x_flatten.shape[1],[256,128],2,'sigmoid','sigmoid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-djBlWSsiJb"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed using the [**summary**](https://keras.io/api/models/model/#summary-method) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar_fPjVJrUWE"
      },
      "source": [
        "vae.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQUDkAJFsmC0"
      },
      "source": [
        "The summary is useful for simple models, but can be confusing for complex models.\n",
        "\n",
        "Function [**keras.utils.plot_model**](https://keras.io/api/utils/model_plotting_utils/) creates a plot of the neural network graph that can make more complex models easier to understand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeipIuf4snDc"
      },
      "source": [
        "keras.utils.plot_model(vae,show_shapes=True, show_layer_names=True,expand_nested=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40eGhRvYtBWz"
      },
      "source": [
        "## **Loss function definition**\n",
        "To train a VAE, an ad-hoc combined loss function must be defined. It is composed by:\n",
        "- a *reconstruction* loss to measure the similarity between the input and the generated output;\n",
        "- a *regularization* loss to evaluate how close the distribution returned by the encoder is to the standard normal distribution. It is computed as the KL divergence between the returned distribution and a standard normal distribution:\n",
        "\n",
        "$$KL(\\mathcal{N} (\\mathbf{\\boldsymbol{\\mu}_x},\\mathbf{\\boldsymbol{\\sigma}_x}) \\parallel \\mathcal{N} (\\mathbf{0},\\mathbf{1}))=\\frac{1}{2}\\sum_{i=1}^{k}\\mu_i^2+\\sigma_i^2-\\log{\\sigma_i^2}-1$$\n",
        "\n",
        "<u>Note that:</u>\n",
        "- *kl_coefficient* is a hyper-parameter to weight the contribution of the regularization term;\n",
        "- unlike \"traditional\" losses, VAE loss not only depends on the model output but also on the model input and on the encoder output (*mu* and *log_var*);\n",
        "- since the reconstruction loss is defined as the sum (and not the average) of the error of all generated elements (i.e., pixels), the [**mean_squared_error**](https://keras.io/api/losses/regression_losses/#mean_squared_error-function) output is multiplied by the number of elements.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT2HWr2xBLn4"
      },
      "source": [
        "def vae_loss(vae_input,vae_ouput,mu,log_var,kl_coefficient):\n",
        "  #Reconstruction loss\n",
        "  reconstruction_loss = keras.losses.mean_squared_error(vae_input,vae_ouput) * train_x_flatten.shape[1]\n",
        "\n",
        "  #Regularization loss\n",
        "  kl_loss = 0.5 * K.sum(K.square(mu) + K.exp(log_var) - log_var - 1, axis = -1)\n",
        "\n",
        "  #Combined loss\n",
        "  return reconstruction_loss + kl_coefficient*kl_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWqkjIh4674N"
      },
      "source": [
        "## **Model compilation**\n",
        "The compilation is the final step in configuring the model for training. \n",
        "\n",
        "The following code use the [**compile**](https://keras.io/api/models/model_training_apis/#compile-method) method to compile the model.\n",
        "\n",
        "When a loss function does not fulfill  the signature **loss=fn(y_true, y_pred)** it cannot be directly passed to the **compile** method. In this case, the [**add_loss**](https://keras.io/api/losses/#the-addloss-api) method can be used with the loss function passed as parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTRYmq-1BxYo"
      },
      "source": [
        "kl_coefficient=1\n",
        "\n",
        "#Information needed to compute the loss function\n",
        "vae_input=vae.input\n",
        "vae_output=vae.output\n",
        "mu=vae.get_layer('mu').output\n",
        "log_var=vae.get_layer('log_var').output\n",
        "\n",
        "vae.add_loss(vae_loss(vae_input,vae_output,mu,log_var,kl_coefficient))\n",
        "vae.compile(optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFab7jtJBfzc"
      },
      "source": [
        "## **Training**\n",
        "Now we are ready to train our model by calling the [**fit**](https://keras.io/api/models/model_training_apis/#fit-method) method.\n",
        "\n",
        "It trains the model for a fixed number of epochs (*epoch_count*) using the training set (*train_x_flatten*) divided into mini-batches of *batch_size* elements. During the training process, the performances will be evaluated on both training and validation (*val_x_flatten*) sets.\n",
        "\n",
        "Break training when a metric or the loss has stopped improving on the validation set, helps to avoid overfitting.\n",
        "\n",
        "For this purpose, Keras provides a class called [**EarlyStopping**](https://keras.io/api/callbacks/early_stopping/). Important class parameters are:\n",
        "- *monitor* - the name of the metric or the loss to be observed; \n",
        "- *patience* - the number of epochs with no improvement after which training will be stopped;\n",
        "- *restore_best_weights* - whether to restore model weights from the epoch with the best value of the monitored quantity.\n",
        "\n",
        "Once created an instance of the **EarlyStopping** class, it can be passed to the **fit** method in the *callbacks* parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHZQV_3NBgpJ"
      },
      "source": [
        "epoch_count = 100\n",
        "batch_size=100\n",
        "patience=5\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "history = vae.fit(train_x_flatten,validation_data=(val_x_flatten,None),epochs=epoch_count,batch_size=batch_size,callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8TnyXtoD7Is"
      },
      "source": [
        "We can learn a lot about our model by observing the graph of its performance over time during training.\n",
        "\n",
        "The **fit** method returns an object (*history*) containing loss and metrics values at successive epochs for both training and validation sets.\n",
        "\n",
        "The following code calls the **plot_history** function defined above to draw in a graph the loss over epochs on both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lig5S-A1D7xK"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKLOKVLsw96P"
      },
      "source": [
        "## **Performance evaluation on the test set**\n",
        "The performance on the test set can be easily measured by calling the **evaluate** method of the autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbDJTfihw-SN"
      },
      "source": [
        "test_loss = vae.evaluate(test_x_flatten, None, batch_size=batch_size,verbose=0)\n",
        "print('Test loss: {:.3f}'.format(test_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JWiV1BmEb65"
      },
      "source": [
        "## **Reduced space visualization**\n",
        "The [**predict**](https://keras.io/api/models/model_training_apis/#predict-method) method of the *encoder* can be used to obtain *mu* and *log_var* values representing training, validation and test sets in the latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mqhe_KyEcwR"
      },
      "source": [
        "train_x_latent = vae_encoder.predict(train_x_flatten)\n",
        "val_x_latent = vae_encoder.predict(val_x_flatten)\n",
        "test_x_latent = vae_encoder.predict(test_x_flatten)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9e1h28qk3b8"
      },
      "source": [
        "The following code visualizes training, validation and test sets in the latent space by plotting *mu* values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrHDH30X2o_5"
      },
      "source": [
        "plot_2d_data([train_x_latent[0],val_x_latent[0],test_x_latent[0]],[train_y,val_y,test_y],['Train','Validation','Test'],(18,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--Zfclb-pZyv"
      },
      "source": [
        "## **Generated images**\n",
        "The **predict** method of the *decoder* can be used to generate a handwritten digit from random noise.\n",
        "\n",
        "The following code visualizes a randomly generated image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ8JQhaXJFns"
      },
      "source": [
        "random_sample = np.array([[random.normalvariate(0,1), random.normalvariate(0,1)]])\n",
        "\n",
        "print('Random sample: ',random_sample)\n",
        "\n",
        "decoded_x = vae_decoder.predict(random_sample)\n",
        "digit = decoded_x[0].reshape(original_image_shape)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(digit, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_Id7t70tD73"
      },
      "source": [
        "Running the code below will show a continuous distribution of the different digit classes, with each digit morphing into another across the 2D latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l1JFPO9cIAw"
      },
      "source": [
        "n = 20 # number of images per row and column\n",
        "limit=3 # random values are sampled from the range [-limit,+limit]\n",
        "\n",
        "grid_x = np.linspace(-limit,limit, n) \n",
        "grid_y = np.linspace(limit,-limit, n)\n",
        "\n",
        "generated_images=[]\n",
        "for i, yi in enumerate(grid_y):\n",
        "  single_row_generated_images=[]\n",
        "  for j, xi in enumerate(grid_x):\n",
        "    random_sample = np.array([[xi, yi]])\n",
        "    decoded_x = vae_decoder.predict(random_sample)\n",
        "    single_row_generated_images.append(decoded_x[0].reshape(original_image_shape))\n",
        "  generated_images.append(single_row_generated_images)      \n",
        "\n",
        "plot_generated_images(generated_images,n,n,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0DHGWHcONIl"
      },
      "source": [
        "# **Exercise 1: conditional variational autoencoder (CVAE)**\n",
        "Define and train a conditional variational autoencoder to generate handwritten digits given the digit label as input type:\n",
        "1. define a CVAE model implementing the **build_cvae** function;\n",
        "2. execute the training process;\n",
        "3. generate different handwritten digits using the CVAE decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yt5eUWMdf6q"
      },
      "source": [
        "## **Digit labels one hot encoding**\n",
        "To avoid the model to misinterpret the digit labels, labels are conveniently converted into one hot encoding representation using the [**to_categorical**](https://keras.io/api/utils/python_utils/#to_categorical-function) function provided by Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTQIHn7OdiHM"
      },
      "source": [
        "train_y_one_hot = to_categorical(train_y,category_count)\n",
        "val_y_one_hot=to_categorical(val_y,category_count)\n",
        "test_y_one_hot=to_categorical(test_y,category_count)\n",
        "\n",
        "print('Train label one hot encoding shape: ',train_y_one_hot.shape)\n",
        "print('Validation label one hot encoding shape: ',val_y_one_hot.shape)\n",
        "print('Test label one hot encoding shape: ',test_y_one_hot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr-H-4BpA3tb"
      },
      "source": [
        "## **Model definition**\n",
        "Implement the following function to create a CVAE model given:\n",
        "- the number of input features (*input_count*);\n",
        "- the dimension of input type (*condition_count*);\n",
        "- the number of neurons for each hidden layer (*neuron_count_per_hidden_layer*);\n",
        "- the dimension of the latent space (*encoded_dim*);\n",
        "- the string identifier of the activation function of the hidden layers (*hidden_activation*);\n",
        "- the string identifier of the activation function of the output layer (*output_activation*).\n",
        "\n",
        "It could be useful to start from function used to build a VAE (**build_vae**).\n",
        "\n",
        "As for VAEs, Keras **Sequential** class cannot be used because the layers are not stacked and some of them receive multiple inputs or return multiple outputs. The **Model** class can be used instead.\n",
        "\n",
        "Both encoder and decoder need to receive two inputs. To this purpose, the Keras [**Concatenate**](https://keras.io/api/layers/merging_layers/concatenate/) layer can be used to concatenate multiple inputs into a single tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2BIXj1ZQ8uO"
      },
      "source": [
        "def build_cvae(input_count,condition_count,neuron_count_per_hidden_layer,encoded_dim,hidden_activation,output_activation):\n",
        "    #..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZiEkq4ZSTmh"
      },
      "source": [
        "## **Model creation**\n",
        "The following code creates a conditional VAE by calling the **build_cvae** function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23QWzYN2Sezo"
      },
      "source": [
        "cvae, cvae_encoder, cvae_decoder=build_cvae(train_x_flatten.shape[1],category_count,[256,128],2,'sigmoid','sigmoid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDUXiIuBS5x3"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sg0XkuSS6_p"
      },
      "source": [
        "cvae.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8y4Tuj5S_O_"
      },
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVcd9tunTAO3"
      },
      "source": [
        "keras.utils.plot_model(cvae,show_shapes=True, show_layer_names=True,expand_nested=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IkKHQlbYSPn"
      },
      "source": [
        "## **Model compilation**\n",
        "The following code compiles the model as already done for the variational autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7btxEo4YTdX"
      },
      "source": [
        "kl_coefficient=1\n",
        "\n",
        "cvae_input=cvae.input[0]\n",
        "cvae_output=cvae.output\n",
        "mu=cvae.get_layer('mu').output\n",
        "log_var=cvae.get_layer('log_var').output\n",
        "\n",
        "cvae.add_loss(vae_loss(cvae_input,cvae_output,mu,log_var,kl_coefficient))\n",
        "cvae.compile(optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otefs1yzZQdx"
      },
      "source": [
        "## **Training**\n",
        "Now we are ready to train our model by calling the **fit** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AptjjwfUZQ7B"
      },
      "source": [
        "epoch_count = 100\n",
        "batch_size=100\n",
        "patience=5\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "history = cvae.fit([train_x_flatten,train_y_one_hot],\n",
        "                   validation_data=([val_x_flatten,val_y_one_hot],None),\n",
        "                   epochs=epoch_count,\n",
        "                   batch_size=batch_size,\n",
        "                   callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GJRLQ0hZuN1"
      },
      "source": [
        "The following code calls the **plot_history** function defined above to draw in a graph the loss over epochs on both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6hZc0h3Zwfz"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzYnoUZbZ6P-"
      },
      "source": [
        "## **Performance evaluation on the test set**\n",
        "The **evaluate** method of the autoencoder is used to measure the performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uf-e9Cq7vA6"
      },
      "source": [
        "test_loss = cvae.evaluate([test_x_flatten, test_y_one_hot],None, batch_size=batch_size,verbose=0)\n",
        "print('Test loss: {:.3f}'.format(test_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBaljSnI8QsY"
      },
      "source": [
        "## **Reduced space visualization**\n",
        "Obtain *mu* and *log_var* values representing training, validation and test sets in the latent space by calling the **predict** method of the CVAE *encoder*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqp3yLu1Z876"
      },
      "source": [
        "train_x_latent = #...\n",
        "val_x_latent = #...\n",
        "test_x_latent = #..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycC96Wza8ks1"
      },
      "source": [
        "The following code visualizes training, validation and test sets in the latent space by plotting *mu* values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l36wgZvl3ZMq"
      },
      "source": [
        "plot_2d_data([train_x_latent[0],val_x_latent[0],test_x_latent[0]],[train_y,val_y,test_y],['Train','Validation','Test'],(18,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgX69gRl90Oi"
      },
      "source": [
        "## **Generated images**\n",
        "Use the **predict** method of the CVAE *decoder* to visualize a randomly generated handwritten digit (of a specific category: *digit_label*).\n",
        "\n",
        "It could be useful to start from the code used to visualize a handwritten digit generated by a VAE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPC25t03agHd"
      },
      "source": [
        "digit_label=8\n",
        "\n",
        "#..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIliV_1m_edR"
      },
      "source": [
        "Running the code below will explore the latent space by varying the first dimension and maintaining constant the second one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsDifgtouX9p"
      },
      "source": [
        "n = 10  # number of images per row and column\n",
        "limit=3 # random values are sampled from the range [-limit,+limit]\n",
        "second_dim_const=0  # constant value of the second latent dimension\n",
        "\n",
        "grid_x = np.linspace(-limit,limit, n) \n",
        "\n",
        "generated_images=[]\n",
        "for digit_label in range(category_count):\n",
        "  digit_lable_one_hot=to_categorical(digit_label, category_count).reshape(1,-1)\n",
        "  \n",
        "  single_row_generated_images=[]\n",
        "  for i, xi in enumerate(grid_x):\n",
        "    random_sample = np.array([[xi, second_dim_const]])\n",
        "    decoded_x = cvae_decoder.predict([random_sample, digit_lable_one_hot])\n",
        "    single_row_generated_images.append(decoded_x[0].reshape(original_image_shape))\n",
        "  generated_images.append(single_row_generated_images)      \n",
        "\n",
        "plot_generated_images(generated_images,n,n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOUn1AeVHc_D"
      },
      "source": [
        "Running the code below will explore the latent space by varying the second dimension and maintaining constant the first one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNKVfHElvTpI"
      },
      "source": [
        "n = 10  # number of images per row and column\n",
        "limit=3 # random values are sampled from the range [-limit,+limit]\n",
        "first_dim_const=0  # constant value of the second latent dimension\n",
        "\n",
        "grid_y = np.linspace(-limit,limit, n) \n",
        "\n",
        "generated_images=[]\n",
        "for digit_label in range(category_count):\n",
        "  digit_lable_one_hot=to_categorical(digit_label, category_count).reshape(1,-1)\n",
        "  \n",
        "  single_row_generated_images=[]\n",
        "  for i, yi in enumerate(grid_y):\n",
        "    random_sample = np.array([[first_dim_const, yi]])\n",
        "    decoded_x = cvae_decoder.predict([random_sample, digit_lable_one_hot])\n",
        "    single_row_generated_images.append(decoded_x[0].reshape(original_image_shape))\n",
        "  generated_images.append(single_row_generated_images)      \n",
        "\n",
        "plot_generated_images(generated_images,n,n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwXKlCn4XwB7"
      },
      "source": [
        "# **Exercise 2**\n",
        "Train VAE and CVAE to generate images similar to [**fashion MNIST**](https://github.com/zalandoresearch/fashion-mnist) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvRTjfqVA8DR"
      },
      "source": [
        "# **Generative adversarial network (GAN)**\n",
        "In this section a generative adversarial network is trained to generate handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0-grViFA_uR"
      },
      "source": [
        "## **Model definition**\n",
        "The following function creates a generative adversarial network given:\n",
        "- the size of the input noise (*input_noise_dim*);\n",
        "- the number of neurons for each hidden layer (*neuron_count_per_hidden_layer*);\n",
        "- the dimension of the output (*output_dim*);\n",
        "- the string identifier of the activation function of the hidden layers (*hidden_activation*);\n",
        "- the string identifier of the activation function of the output layer of the generator (*generator_output_activation*).\n",
        "\n",
        "The function returns the generator and the discriminator models as well as the whole GAN.\n",
        "\n",
        "In Keras, a sequential is a stack of layers where each layer has exactly one input and one output. It can be created by passing a list of layers to the  constructor [**keras.Sequential**](https://keras.io/guides/sequential_model/).\n",
        "\n",
        "To combine generator and discriminator together forming the GAN, the **Model** class provided by Keras is used. Input and output layers are passed to the constructor, then it groups layers into an object with training and inference features.\n",
        "\n",
        "The GAN model will be used to train the model weights in the generator, using the output and error calculated by the discriminator model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4od6h_H3poPu"
      },
      "source": [
        "def build_gan(input_noise_dim,neuron_count_per_hidden_layer,output_dim,hidden_activation,generator_output_activation):\n",
        "  #Generator\n",
        "  generator = keras.Sequential(name='generator')\n",
        "  generator.add(layers.Input(shape=input_noise_dim, name='generator_input'))\n",
        "  for neuron_count in neuron_count_per_hidden_layer:\n",
        "      generator.add(layers.Dense(neuron_count, activation=hidden_activation))\n",
        "      \n",
        "  generator.add(layers.Dense(output_dim, activation=generator_output_activation,name='generator_output'))\n",
        "\n",
        "  #Discriminator\n",
        "  discriminator = keras.Sequential(name='discriminator')\n",
        "  discriminator.add(layers.Input(shape=output_dim,name='discriminator_input'))\n",
        "  for neuron_count in reversed(neuron_count_per_hidden_layer):\n",
        "      discriminator.add(layers.Dense(neuron_count, activation=hidden_activation))\n",
        "      \n",
        "  discriminator.add(layers.Dense(1, activation='sigmoid',name='discriminator_output'))\n",
        "\n",
        "  #GAN\n",
        "  gan = keras.Model(generator.input, discriminator(generator.output),name='gan')\n",
        "\n",
        "  return gan,generator,discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv7KaVCBHpIZ"
      },
      "source": [
        "## **Model creation**\n",
        "The following code creates a GAN by calling the **build_gan** function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoLNsJdEsxTd"
      },
      "source": [
        "input_noise_dim=100\n",
        "\n",
        "gan,gan_generator,gan_discriminator=build_gan(input_noise_dim,[256,512,1024],train_x_flatten.shape[1],'relu','sigmoid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8E2GsRcGKW0"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLBpcw75GK6r"
      },
      "source": [
        "gan.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIWKs9_hvA45"
      },
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sExpGo7OGVqi"
      },
      "source": [
        "keras.utils.plot_model(gan,show_shapes=True, show_layer_names=True,expand_nested=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp0-kfenvCwF"
      },
      "source": [
        "## **Model compilation**\n",
        "The following code compiles the discriminator and the whole generative adversarial network.\n",
        "\n",
        "Because the discriminator model is trained separately, its weights are marked as not trainable in the GAN model to ensure that only the weights of the generator model are updated. This change to the trainability of the discriminator weights only has an effect when training the combined GAN model, not when training the discriminator standalone.\n",
        "\n",
        "<u>Note that, *trainable* flag is evaluated during the model compilation.</u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SErKuo74vDfM"
      },
      "source": [
        "gan_discriminator.compile(loss='binary_crossentropy', optimizer='sgd')\n",
        "\n",
        "gan_discriminator.trainable = False\n",
        "gan.compile(loss='binary_crossentropy', optimizer='sgd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvvFUPoivXrJ"
      },
      "source": [
        "## **Training**\n",
        "Training a GAN model is not easy and cannot be done simply using the **fit** method.\n",
        "\n",
        "To simplify and generalize the training function, it is subdivided into different sub-functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbQ2b83wMcT-"
      },
      "source": [
        "### **Random selection of real batches**\n",
        "At each iteration, a batch of real inputs from the dataset is required. This can be achieved by selecting a random sample of images from the dataset each time.\n",
        "\n",
        "The following code defines some functions useful to randomly select batches of real images from the dataset at each iteration:\n",
        "- **chunks** divides \"on the fly\" a *list* in subsets of length *n*;\n",
        "- **get_random_batch_indices** returns batches of *batch_size* random indices given the cardinality of the dataset (*data_count*);\n",
        "- **get_gan_real_batch** creates a batch with real images with the corresponding labels (1=real)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cdREC1FvjuD"
      },
      "source": [
        "def chunks(list, n):\n",
        "    for i in range(0, len(list), n):\n",
        "        yield list[i:i + n]\n",
        "\n",
        "def get_random_batch_indices(data_count,batch_size):\n",
        "    list_indices=list(range(0,data_count))\n",
        "    random.shuffle(list_indices)\n",
        "    return list(chunks(list_indices, batch_size))\n",
        "\n",
        "def get_gan_real_batch(dataset_x,batch_indices,label):\n",
        "  batch_x = dataset_x[batch_indices]\n",
        "  batch_y=np.full(len(batch_indices),label)\n",
        "\n",
        "  return batch_x,batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SjQU2_GTfcs"
      },
      "source": [
        "### **Generation of fake batches**\n",
        "At each iteration, the discriminator is trained on two different mini-batches: real and fake.\n",
        "\n",
        "The following code defines functions useful to generate batches of fake images:\n",
        "- **get_gan_random_input** returns a tensor (*batch_size*$\\times$*noise_dim*) of random noise to use as generator input;\n",
        "- **get_gan_fake_batch** returns a batch of fake images, created using the generator, and the corresponding labels (0=fake)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kLmd5unUrJB"
      },
      "source": [
        "def get_gan_random_input(batch_size,noise_dim,*_):\n",
        "  return np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
        "\n",
        "def get_gan_fake_batch(generator,batch_size,generator_input):\n",
        "  batch_x = generator.predict(generator_input)\n",
        "  batch_y=np.zeros(batch_size)\n",
        "\n",
        "  return batch_x,batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAC5wqBDuf0L"
      },
      "source": [
        "### **Concatenation of real and fake batches**\n",
        "Before training the discriminator, real and fake batches need to be concatenated. The following function concatenates a real and a fake batch into a single batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP1CI6FgulaR"
      },
      "source": [
        "def concatenate_gan_batches(real_batch_x,fake_batch_x):\n",
        "  return np.concatenate((real_batch_x, fake_batch_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOYOHXppQbEs"
      },
      "source": [
        "### **Train function**\n",
        "The following function train a generative adversarial network given:\n",
        "- the GAN model (*gan*);\n",
        "- the generator model (*generator*);\n",
        "- the discriminator model (*discriminator*);\n",
        "- the training set (*train_x*);\n",
        "- the number of example contained in the training set (*train_data_count*);\n",
        "- the size of the input noise (*input_noise_dim*);\n",
        "- the number of epochs (*epoch_count*);\n",
        "- the batch size (*batch_size*);\n",
        "- the function to generate random noise to use as input for the generator (*get_random_input_func*);\n",
        "- the function to create a real batch from the training set (*get_real_batch_func*);\n",
        "- the function to generate a fake batch with the generator (*get_fake_batch_func*);\n",
        "- the function to concatenate a real and a fake batch into a single batch (*concatenate_batches_func*);\n",
        "- the dimension of additional information (*condition_count*). It will be used only to train conditional generative adversarial networks (see Exercises 3 and 5);\n",
        "- a flag to decide if using or not *one-sided label smoothing* (*use_one_sided_labels*);\n",
        "- the update frequency of example images (*plt_frq*);\n",
        "- the number of examples visualized (*plt_example_count*);\n",
        "- the original shape of the example images (*example_shape*).\n",
        "\n",
        "At each iteration:\n",
        "1. a batch of real images is randomly selected from the dataset (*real_batch_x* and *real_batch_y*);\n",
        "2. a batch of fake images is generated using the generator (*fake_batch_x* and *fake_batch_y*);\n",
        "3. the two batches are concatenated forming a single batch (*discriminator_batch_x* and *discriminator_batch_x*);\n",
        "4. the discriminator is trained on this batch and its weights are updated using the [**train_on_batch**](https://keras.io/api/models/model_training_apis/#trainonbatch-method) method obtaining the corresponding training loss value (*d_loss*);\n",
        "5. a batch of noise data is randomly generated to be used as input of the generator (*gan_batch_x* and *gan_batch_y*);\n",
        "6. the generator is trained on this batch and its weights are updated using the **train_on_batch** method obtaining the corresponding training loss value (*g_loss*);\n",
        "7. discriminator and generator losses are aggregated to compute the corresponding epoch average losses (*avg_d_loss* and *avg_g_loss*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pan-w2t3SyBN"
      },
      "source": [
        "def train_gan(gan,generator,discriminator,train_x,train_data_count,input_noise_dim,epoch_count, batch_size,\n",
        "              get_random_input_func,get_real_batch_func,get_fake_batch_func,concatenate_batches_func,condition_count=-1,\n",
        "              use_one_sided_labels=False,plt_frq=None,plt_example_count=10,example_shape=(28,28)):\n",
        "    iteration_count = int(train_data_count / batch_size)\n",
        "    \n",
        "    print('Epochs: ', epoch_count)\n",
        "    print('Batch size: ', batch_size)\n",
        "    print('Iterations: ', iteration_count)\n",
        "    print('')\n",
        "    \n",
        "    #Plot generated images\n",
        "    if plt_frq!=None:\n",
        "      print('Before training:')\n",
        "      noise_to_plot = get_random_input_func(plt_example_count, input_noise_dim,condition_count)\n",
        "      generated_output = generator.predict(noise_to_plot)\n",
        "      generated_images = generated_output.reshape(plt_example_count, example_shape[0], example_shape[1])\n",
        "      plot_generated_images([generated_images],1,plt_example_count,figsize=(15, 5))\n",
        "          \n",
        "    d_epoch_losses=[]\n",
        "    g_epoch_losses=[]\n",
        "    for e in range(1, epoch_count+1):\n",
        "        start_time = time.time()\n",
        "        avg_d_loss=0\n",
        "        avg_g_loss=0\n",
        "\n",
        "        # Training indices are shuffled and grouped into batches\n",
        "        batch_indices=get_random_batch_indices(train_data_count,batch_size)\n",
        "\n",
        "        for i in range(iteration_count):\n",
        "            current_batch_size=len(batch_indices[i])\n",
        "\n",
        "            # 1. create a batch with real images from the training set\n",
        "            real_batch_x,real_batch_y=get_real_batch_func(train_x,batch_indices[i],0.9 if use_one_sided_labels else 1)\n",
        "                        \n",
        "            # 2. create noise vectors for the generator and generate the images from the noise\n",
        "            generator_input=get_random_input_func(current_batch_size, input_noise_dim,condition_count)\n",
        "            fake_batch_x,fake_batch_y=get_fake_batch_func(generator,current_batch_size,generator_input)\n",
        "\n",
        "            # 3. concatenate real and fake batches into a single batch\n",
        "            discriminator_batch_x = concatenate_batches_func(real_batch_x, fake_batch_x)\n",
        "            discriminator_batch_y= np.concatenate((real_batch_y, fake_batch_y))\n",
        "\n",
        "            # 4. train discriminator\n",
        "            d_loss = discriminator.train_on_batch(discriminator_batch_x, discriminator_batch_y)\n",
        "            \n",
        "            # 5. create noise vectors for the generator\n",
        "            gan_batch_x = get_random_input_func(current_batch_size, input_noise_dim,condition_count)\n",
        "            gan_batch_y = np.ones(current_batch_size)    #Flipped labels\n",
        "\n",
        "            # 6. train generator\n",
        "            g_loss = gan.train_on_batch(gan_batch_x, gan_batch_y)\n",
        "\n",
        "            # 7. avg losses\n",
        "            avg_d_loss+=d_loss*current_batch_size\n",
        "            avg_g_loss+=g_loss*current_batch_size\n",
        "            \n",
        "        avg_d_loss/=train_data_count\n",
        "        avg_g_loss/=train_data_count\n",
        "\n",
        "        d_epoch_losses.append(avg_d_loss)\n",
        "        g_epoch_losses.append(avg_g_loss)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        print('Epoch: {0} exec_time={1:.1f}s d_loss={2:.3f} g_loss={3:.3f}'.format(e,end_time - start_time,avg_d_loss,avg_g_loss))\n",
        "\n",
        "        # Update the plots\n",
        "        if plt_frq!=None and e%plt_frq == 0:\n",
        "            generated_output = generator.predict(noise_to_plot)\n",
        "            generated_images = generated_output.reshape(plt_example_count, example_shape[0], example_shape[1])\n",
        "            plot_generated_images([generated_images],1,plt_example_count,figsize=(15, 5))\n",
        "    \n",
        "    return d_epoch_losses,g_epoch_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh5r_tDXsb-w"
      },
      "source": [
        "### **Execute training**\n",
        "The following code train our model by calling the **train_gan** function defined above.\n",
        "\n",
        "To reduce the time needed to execute each epoch, the validation set is used instead of the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKGdj4oFscrX"
      },
      "source": [
        "epoch_count=10\n",
        "batch_size=100\n",
        "\n",
        "d_epoch_losses,g_epoch_losses=train_gan(gan,\n",
        "                                        gan_generator,\n",
        "                                        gan_discriminator,\n",
        "                                        val_x_flatten,\n",
        "                                        val_x_flatten.shape[0],\n",
        "                                        input_noise_dim,\n",
        "                                        epoch_count,\n",
        "                                        batch_size,\n",
        "                                        get_gan_random_input,\n",
        "                                        get_gan_real_batch,\n",
        "                                        get_gan_fake_batch,\n",
        "                                        concatenate_gan_batches,\n",
        "                                        plt_frq=1,\n",
        "                                        plt_example_count=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao7RDSDwt_RZ"
      },
      "source": [
        "The following code calls the **plot_gan_losses** function defined above to draw in a graph the discriminator and generator loss trends over epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5nVfNgBuASi"
      },
      "source": [
        "plot_gan_losses(d_epoch_losses,g_epoch_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z78hl3QKwsGv"
      },
      "source": [
        "## **Tips and tricks for training**\n",
        "Training stable GAN models can be very challenging. The reason they are difficult to train is that the training process is inherently unstable because both generator and discriminator models are trained simultaneously in a game. This means that improvements to one model come at the expense of the other model.\n",
        "\n",
        "The goal of training two models involves finding a point of equilibrium between the two competing concerns.\n",
        "\n",
        "GANs particularly suffer of the following problems:\n",
        "- non-convergence: the models do not converge and worse they become unstable;\n",
        "- mode collapse: the generator produces limited modes;\n",
        "- slow training: the gradient to train the generator vanished.\n",
        "\n",
        "Practitioners use several tricks to improve the performance of GANs. It can be difficult to tell how effective some of these tricks are; many of them seem to help in some contexts and hurt in others.\n",
        "\n",
        "In this section some of the common practical tips to train stable GAN models are reported. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLU7BEINI34T"
      },
      "source": [
        "### **Normalize the inputs**\n",
        "Normalize inputs to the range [-1,1] and use *tanh* as activation function in the output layer of the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1snRBzLcDGh"
      },
      "source": [
        "train_x_flatten = (train_x_flatten*2)-1\n",
        "val_x_flatten = (val_x_flatten*2)-1\n",
        "test_x_flatten = (test_x_flatten*2)-1\n",
        "\n",
        "generator_output_activation='tanh'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oegTM-s0dhef"
      },
      "source": [
        "### **Avoid sparse gradients**\n",
        "The stability of the GAN training suffers sparse gradients. To reduce this problem, practitioners suggest to use LeakyReLU (instead of ReLU) in the hidden layers of both generator and discriminator with a slope coefficient of 0.2.\n",
        "\n",
        "Keras provides the [**LeakyReLU**](https://keras.io/api/layers/activation_layers/leaky_relu/) class to implement such activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2juo4AGteZ-x"
      },
      "source": [
        "hidden_activation=layers.LeakyReLU(alpha=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oUJzHlxDG1M"
      },
      "source": [
        "### **One-sided label smoothing**\n",
        "Deep networks may suffer from overconfidence. For example, it uses very few features to classify an object.\n",
        "\n",
        "If the discriminator depends on a small set of features to detect real images, the generator may just produce these features only to exploit the discriminator obtaining no long term benefit.\n",
        "\n",
        "*One-sided label smoothing* can be used to avoid the problem by replacing the target of real examples with a value slightly less than 1, such as 0.9. This prevents extreme extrapolation behavior in the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOZVqwqbFcjs"
      },
      "source": [
        "use_one_sided_labels=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWoXKosxFsnI"
      },
      "source": [
        "### **Use Adam optimizer**\n",
        "The use of Adam optimizer with a learning rate of 0.0002 and a momentum ($\\beta_1$) of 0.5 seems usually works better than other methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVQUqCIuHInY"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lydO2UCUHc7z"
      },
      "source": [
        "### **Execute training**\n",
        "The following code train our model by calling the **train_gan** function defined above.\n",
        "\n",
        "To reduce the time needed to execute each epoch, the validation set is used instead of the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaZNdvv4Hdvx"
      },
      "source": [
        "epoch_count=30\n",
        "batch_size=100\n",
        "\n",
        "gan,gan_generator,gan_discriminator=build_gan(input_noise_dim,\n",
        "                                              [256,512,1024],\n",
        "                                              train_x_flatten.shape[1],\n",
        "                                              hidden_activation,\n",
        "                                              generator_output_activation)\n",
        "\n",
        "gan_discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "gan_discriminator.trainable = False\n",
        "gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "d_epoch_losses,g_epoch_losses=train_gan(gan,\n",
        "                                        gan_generator,\n",
        "                                        gan_discriminator,\n",
        "                                        val_x_flatten,\n",
        "                                        val_x_flatten.shape[0],\n",
        "                                        input_noise_dim,\n",
        "                                        epoch_count,\n",
        "                                        batch_size,\n",
        "                                        get_gan_random_input,\n",
        "                                        get_gan_real_batch,\n",
        "                                        get_gan_fake_batch,\n",
        "                                        concatenate_gan_batches,\n",
        "                                        use_one_sided_labels=use_one_sided_labels,\n",
        "                                        plt_frq=1,\n",
        "                                        plt_example_count=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxUIBsFKRDM5"
      },
      "source": [
        "The following code calls the **plot_gan_losses** function defined above to draw in a graph the discriminator and generator loss trends over epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1M_RXaaRGHR"
      },
      "source": [
        "plot_gan_losses(d_epoch_losses,g_epoch_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4rOSBP2J7Bb"
      },
      "source": [
        "## **Generated images**\n",
        "The following code visualizes a randomly generated handwritten digit obtained calling the **predict** method of the *generator*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7ch-WExKDjw"
      },
      "source": [
        "noise = np.random.normal(0, 1, size=(1, input_noise_dim))\n",
        "\n",
        "generated_x = gan_generator.predict(noise)\n",
        "digit = generated_x[0].reshape(original_image_shape)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(digit, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa1VNZfA3QaS"
      },
      "source": [
        "Running the code below will show handwritten digits randomly generated by the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh9WiMav3a5E"
      },
      "source": [
        "n = 10 # number of images per row and column\n",
        "\n",
        "generated_images=[]\n",
        "for i in range(n):\n",
        "  noise = np.random.normal(0, 1, size=(n, input_noise_dim))\n",
        "  generated_x = gan_generator.predict(noise)\n",
        "  generated_images.append([g.reshape(original_image_shape) for g in generated_x])\n",
        "\n",
        "plot_generated_images(generated_images,n,n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY2zp2OqF_PC"
      },
      "source": [
        "# **Exercise 3: conditional generative adversarial network (cGAN)**\n",
        "Define and train a conditional generative adversarial network to generate handwritten digits given the digit label as additional information:\n",
        "1. define a cGAN model implementing the **build_cgan** function;\n",
        "2. execute the training process;\n",
        "3. generate different handwritten digits using the cGAN generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G9qBXVmGGwJ"
      },
      "source": [
        "## **Model definition**\n",
        "Implement the following function to create a cGAN model given:\n",
        "- the size of the input noise (*input_noise_dim*);\n",
        "- the dimension of additional information (*condition_dim*);\n",
        "- the number of neurons for each hidden layer (*neuron_count_per_hidden_layer*);\n",
        "- the dimension of the output (*output_dim*);\n",
        "- the string identifier of the activation function of the hidden layers (*hidden_activation*);\n",
        "- the string identifier of the activation function of the output layer of the generator (*generator_output_activation*).\n",
        "\n",
        "It could be useful to start from functions used to build a GAN (**build_gan**) and a cVAE (**build_cvae**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrmHoBO4GARO"
      },
      "source": [
        "def build_cgan(input_noise_dim,condition_dim,neuron_count_per_hidden_layer,output_dim,hidden_activation,generator_output_activation):\n",
        "  #..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdpT1mUfGLyq"
      },
      "source": [
        "## **Model creation**\n",
        "Call the **build_cgan** function to create a cGAN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSDv_iYMGRTv"
      },
      "source": [
        "input_noise_dim=100\n",
        "hidden_activation=layers.LeakyReLU(alpha=0.2)\n",
        "generator_output_activation='tanh'\n",
        "\n",
        "cgan,cgan_generator,cgan_discriminator=build_cgan(input_noise_dim,\n",
        "                                                  category_count,\n",
        "                                                  [256,512,1024],\n",
        "                                                  train_x_flatten.shape[1],\n",
        "                                                  hidden_activation,\n",
        "                                                  generator_output_activation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQbUOsCDGcnp"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjH-W_bpGdJ3"
      },
      "source": [
        "cgan.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PLHpNeUGWDQ"
      },
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONDid9WhQ-Sa"
      },
      "source": [
        "keras.utils.plot_model(cgan,show_shapes=True, show_layer_names=True,expand_nested=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n17CDiAGGosp"
      },
      "source": [
        "## **Model compilation**\n",
        "Compile the model for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYHSohtDGpRh"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "cgan_discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "cgan_discriminator.trainable = False\n",
        "cgan.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFDIsacmfEp1"
      },
      "source": [
        "## **Training**\n",
        "To train the cGAN model the **train_gan** function previously defined can be used. \n",
        "\n",
        "Before that, some sub-functions previously defined for GANs need to be redefined. This is necessary because the input of cGANs contains an extra information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xykctGQ8fIRK"
      },
      "source": [
        "### **Random selection of real batches**\n",
        "The following code defines a function to create a batch with real inputs with the corresponding labels (1=real). Each real input is composed by a real image and the one hot encoding representation of its category (additional information)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US7ActUefJnQ"
      },
      "source": [
        "def get_cgan_real_batch(dataset,batch_indices,label):\n",
        "  dataset_input=dataset[0]\n",
        "  dataset_condition_info=dataset[1]\n",
        "  batch_x =[dataset_input[batch_indices],dataset_condition_info[batch_indices]]\n",
        "  batch_y=np.full(len(batch_indices),label)\n",
        "\n",
        "  return batch_x,batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cliXQLLFgApd"
      },
      "source": [
        "### **Generation of fake batches**\n",
        "The following code defines functions useful to generate batches of fake inputs for cGANs:\n",
        "- **get_cgan_random_input** returns a tensor (*batch_size*$\\times$*noise_dim*) of random noise and corresponding additional information to be used as generator input;\n",
        "- **get_cgan_fake_batch** returns a batch of fake inputs and the corresponding labels (0=fake). Each fake input is composed by a fake image, created by the generator, and a randomly generated one hot encoding vector representing its category (additional information).\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IilxYNFggEy4"
      },
      "source": [
        "def get_cgan_random_input(batch_size,noise_dim,condition_count):\n",
        "  noise=np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
        "  condition_info= to_categorical(np.random.randint(0, condition_count, size=batch_size),condition_count)\n",
        "\n",
        "  return [noise,condition_info]\n",
        "\n",
        "def get_cgan_fake_batch(generator,batch_size,generator_input):\n",
        "  batch_x = [generator.predict(generator_input),generator_input[1]]\n",
        "  batch_y=np.zeros(batch_size)\n",
        "\n",
        "  return batch_x,batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu8foFndvkVF"
      },
      "source": [
        "### **Concatenation of real and fake batches**\n",
        "The following function concatenates a real and a fake batch into a single batch. The resulting batch is composed by images and additional information separately concatenated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdm9ga7DvlF9"
      },
      "source": [
        "def concatenate_cgan_batches(real_batch_x,fake_batch_x):\n",
        "  batch_input = np.concatenate((real_batch_x[0], fake_batch_x[0]))\n",
        "  batch_condition_info =np.concatenate((real_batch_x[1], fake_batch_x[1]))\n",
        "\n",
        "  return [batch_input,batch_condition_info]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiIBmlpEWx38"
      },
      "source": [
        "### **Execute training**\n",
        "Execute training calling the **train_gan** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-WTcK-fH4hj"
      },
      "source": [
        "epoch_count=50\n",
        "batch_size=100\n",
        "\n",
        "d_epoch_losses,g_epoch_losses=train_gan(cgan,\n",
        "                                        cgan_generator,\n",
        "                                        cgan_discriminator,\n",
        "                                        [val_x_flatten,val_y_one_hot],\n",
        "                                        val_x_flatten.shape[0],\n",
        "                                        input_noise_dim,\n",
        "                                        epoch_count,\n",
        "                                        batch_size,\n",
        "                                        get_cgan_random_input,\n",
        "                                        get_cgan_real_batch,\n",
        "                                        get_cgan_fake_batch,\n",
        "                                        concatenate_cgan_batches,\n",
        "                                        condition_count=category_count,\n",
        "                                        use_one_sided_labels=True,\n",
        "                                        plt_frq=1,\n",
        "                                        plt_example_count=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM39uWrtJ0Vy"
      },
      "source": [
        "Call the **plot_gan_losses** function to draw the discriminator and generator loss trends over epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPBNmcivJ1JZ"
      },
      "source": [
        "plot_gan_losses(d_epoch_losses,g_epoch_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAqN__Ce0ZxV"
      },
      "source": [
        "## **Generated images**\n",
        "The following code visualizes a randomly generated handwritten digit (of a specific category: *digit_label*) obtained calling the **predict** method of the *generator*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsTBQDb50b86"
      },
      "source": [
        "digit_label=0\n",
        "\n",
        "noise = np.random.normal(0, 1, size=(1, input_noise_dim))\n",
        "digit_label_one_hot=to_categorical(digit_label, category_count).reshape(1,-1)\n",
        "\n",
        "generated_x = cgan_generator.predict([noise,digit_label_one_hot])\n",
        "digit = generated_x[0].reshape(original_image_shape)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(digit, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1qXuIqm1P3U"
      },
      "source": [
        "Running the code below will show a row of randomly generated images for each digit category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upHYJuFm6g0X"
      },
      "source": [
        "n = 10 # number of images per digit category\n",
        "\n",
        "generated_images=[]\n",
        "for digit_label in range(category_count):\n",
        "  noise = np.random.normal(0, 1, size=(n, input_noise_dim))\n",
        "  digit_label_one_hot=to_categorical(np.full(n,digit_label), category_count)\n",
        "  generated_x = cgan_generator.predict([noise,digit_label_one_hot])\n",
        "  generated_images.append([g.reshape(original_image_shape) for g in generated_x])\n",
        "\n",
        "plot_generated_images(generated_images,category_count,n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIRShlZTRSTu"
      },
      "source": [
        "# **Exercise 4: deep convolutional generative adversarial network (DCGAN)**\n",
        "Define and train a deep convolutional generative adversarial network to generate handwritten digits:\n",
        "1. define a DCGAN model implementing the **build_dcgan** function;\n",
        "2. execute the training process;\n",
        "3. generate different handwritten digits using the DCGAN generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xjj-GjQR4yp"
      },
      "source": [
        "## **Model definition**\n",
        "Implement the following function to create a DCGAN model given:\n",
        "- the size of the input noise (*input_noise_dim*).\n",
        "\n",
        "It could be useful to start from function used to build a GAN (**build_gan**).\n",
        "\n",
        "The following image shows the architecture of both DCGAN generator and discriminator.\n",
        "\n",
        "![alt text](https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/GenerativeModels/dcgan_generator.png)\n",
        "\n",
        "![alt text](https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/GenerativeModels/dcgan_discriminator.png)\n",
        "\n",
        "Keras offers a wide range of built-in layers ready for use, including:\n",
        "- [**Reshape**](https://keras.io/api/layers/reshaping_layers/reshape/) - a simple layer used to reshape inputs into a given shape;\n",
        "- [**Conv2DTranspose**](https://keras.io/api/layers/convolution_layers/convolution2d_transpose/) - a 2D transposed convolution layer;\n",
        "- [**BatchNormalization**](https://keras.io/api/layers/normalization_layers/batch_normalization/) - a layer to apply *batch normalization* to inputs;\n",
        "- [**LeakyReLU**](https://keras.io/api/layers/activation_layers/leaky_relu/) - a simple layer to apply *LeakyReLU* activation function to inputs;\n",
        "- [**Conv2D**](https://keras.io/api/layers/convolution_layers/convolution2d/) - a 2D convolution layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrcN3CqoR522"
      },
      "source": [
        "def build_dcgan(input_noise_dim):\n",
        "  #..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1j6IXxaVzBR"
      },
      "source": [
        "## **Model creation**\n",
        "Call the **build_dcgan** function to create a DCGAN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsNlW0EbV3Ye"
      },
      "source": [
        "input_noise_dim=100\n",
        "\n",
        "dcgan,dcgan_generator,dcgan_discriminator=build_dcgan(input_noise_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btzv3El3WSYY"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX7M2fdJWTEw"
      },
      "source": [
        "dcgan.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEz_jWlPGXjm"
      },
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PEYIBn2WcFH"
      },
      "source": [
        "keras.utils.plot_model(dcgan,show_shapes=True, show_layer_names=True,expand_nested=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr9V56G2WjEQ"
      },
      "source": [
        "## **Model compilation**\n",
        "Compile the model for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM_AhrDyWj5R"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "dcgan_discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "dcgan_discriminator.trainable = False\n",
        "dcgan.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A88lFgnEcNsI"
      },
      "source": [
        "## **Dataset preprocessing**\n",
        "The 2D datasets need to be preprocessed before they can be used for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkfSSOdAd3Cx"
      },
      "source": [
        "### **Image shape**\n",
        "In case of grayscale images, it is necessary to add a new unit axis to explicitly represent single channel images.\n",
        "\n",
        "By executing the following code, the shape of the images is updated from $W\\times H$ to $W\\times H\\times 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "343mJHDyeMOg"
      },
      "source": [
        "train_x=np.expand_dims(train_x,axis=3)\n",
        "val_x=np.expand_dims(val_x,axis=3)\n",
        "test_x=np.expand_dims(test_x,axis=3)\n",
        "print('Train shape: ',train_x.shape)\n",
        "print('Validation shape: ',val_x.shape)\n",
        "print('Test shape: ',test_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1vCG09JdZl5"
      },
      "source": [
        "### **Intensity range normalization**\n",
        "The 2D datasets need to be normalized to the range [-1,1] before they can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdYl5_YwcOFx"
      },
      "source": [
        "train_x = (train_x*2)-1\n",
        "val_x = (val_x*2)-1\n",
        "test_x = (test_x*2)-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHsmK3rSakbB"
      },
      "source": [
        "## **Training**\n",
        "To train the DCGAN model the **train_gan** function previously defined can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNRNnh3pbuz6"
      },
      "source": [
        "epoch_count=50\n",
        "batch_size=100\n",
        "\n",
        "d_epoch_losses,g_epoch_losses=train_gan(dcgan,\n",
        "                                        dcgan_generator,\n",
        "                                        dcgan_discriminator,\n",
        "                                        val_x,\n",
        "                                        val_x.shape[0],\n",
        "                                        input_noise_dim,\n",
        "                                        epoch_count,\n",
        "                                        batch_size,\n",
        "                                        get_gan_random_input,\n",
        "                                        get_gan_real_batch,\n",
        "                                        get_gan_fake_batch,\n",
        "                                        concatenate_gan_batches,\n",
        "                                        use_one_sided_labels=True,\n",
        "                                        plt_frq=1,\n",
        "                                        plt_example_count=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi0zjqwHy4dg"
      },
      "source": [
        "Call the **plot_gan_losses** function to draw the discriminator and generator loss trends over epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmEkdZNSy4td"
      },
      "source": [
        "plot_gan_losses(d_epoch_losses,g_epoch_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDbfrAFOy433"
      },
      "source": [
        "## **Generated images**\n",
        "The following code visualizes a randomly generated handwritten digit obtained calling the **predict** method of the *generator*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vy3AZrkzOGE"
      },
      "source": [
        "noise = np.random.normal(0, 1, size=(1, input_noise_dim))\n",
        "\n",
        "generated_x = dcgan_generator.predict(noise)\n",
        "digit = generated_x[0].reshape(original_image_shape)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(digit, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHnz3pNKzil1"
      },
      "source": [
        "Running the code below will show handwritten digits randomly generated by the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA7Ux4rLzlSq"
      },
      "source": [
        "n = 10 # number of images per row and column\n",
        "\n",
        "generated_images=[]\n",
        "for i in range(n):\n",
        "  noise = np.random.normal(0, 1, size=(n, input_noise_dim))\n",
        "  generated_x = dcgan_generator.predict(noise)\n",
        "  generated_images.append([g.reshape(original_image_shape) for g in generated_x])\n",
        "\n",
        "plot_generated_images(generated_images,n,n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbeORYgYzzLc"
      },
      "source": [
        "# **Exercise 5: conditional deep convolutional generative adversarial network (cDCGAN)**\n",
        "Define and train a conditional deep convolutional generative adversarial network to generate handwritten digits:\n",
        "1. define a cDCGAN model implementing the **build_cdcgan** function;\n",
        "2. execute the training process;\n",
        "3. generate different handwritten digits using the DCGAN generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTC7EQcFz9MY"
      },
      "source": [
        "## **Model definition**\n",
        "Implement the following function to create a cDCGAN model given:\n",
        "- the size of the input noise (*input_noise_dim*);\n",
        "- the dimension of additional information (*condition_dim*).\n",
        "\n",
        "It could be useful to start from functions used to build a cGAN (**build_cgan**) and a DCGAN (**build_dcgan**).\n",
        "\n",
        "The following image shows the architecture of both cDCGAN generator and discriminator.\n",
        "\n",
        "![alt text](https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/GenerativeModels/cdcgan_generator.png)\n",
        "\n",
        "![alt text](https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/GenerativeModels/cdcgan_discriminator.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgIwmCRu0ETx"
      },
      "source": [
        "def build_cdcgan(input_noise_dim,condition_dim):\n",
        "  #..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gghTViYs7fPh"
      },
      "source": [
        "## **Model creation**\n",
        "Call the **build_cdcgan** function to create a cDCGAN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HMy49ek7g9l"
      },
      "source": [
        "input_noise_dim=100\n",
        "\n",
        "cdcgan,cdcgan_generator,cdcgan_discriminator=build_cdcgan(input_noise_dim,category_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvpp0wBY7unf"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUkcJ34O7vUg"
      },
      "source": [
        "cdcgan.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5D721ptGZTu"
      },
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmeVcUAV7vbt"
      },
      "source": [
        "keras.utils.plot_model(cdcgan,show_shapes=True, show_layer_names=True,expand_nested=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCUOae718JLV"
      },
      "source": [
        "## **Model compilation**\n",
        "Compile the model for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtahY68C8KAG"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "cdcgan_discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "cdcgan_discriminator.trainable = False\n",
        "cdcgan.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4fivlcD8QEP"
      },
      "source": [
        "## **Training**\n",
        "To train the cDCGAN model the **train_gan** function previously defined can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwcONHGz8QU2"
      },
      "source": [
        "epoch_count=50\n",
        "batch_size=100\n",
        "\n",
        "d_epoch_losses,g_epoch_losses=train_gan(cdcgan,\n",
        "                                        cdcgan_generator,\n",
        "                                        cdcgan_discriminator,\n",
        "                                        [val_x,val_y_one_hot],\n",
        "                                        val_x.shape[0],\n",
        "                                        input_noise_dim,\n",
        "                                        epoch_count,\n",
        "                                        batch_size,\n",
        "                                        get_cgan_random_input,\n",
        "                                        get_cgan_real_batch,\n",
        "                                        get_cgan_fake_batch,\n",
        "                                        concatenate_cgan_batches,\n",
        "                                        condition_count=category_count,\n",
        "                                        use_one_sided_labels=True,\n",
        "                                        plt_frq=1,\n",
        "                                        plt_example_count=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sloFxEPX9Zj1"
      },
      "source": [
        "Call the **plot_gan_losses** function to draw the discriminator and generator loss trends over epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1MtY_2Q9Z1n"
      },
      "source": [
        "plot_gan_losses(d_epoch_losses,g_epoch_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g28OxWt9aJW"
      },
      "source": [
        "## **Generated images**\n",
        "The following code visualizes a randomly generated handwritten digit (of a specific category: *digit_label*) obtained calling the **predict** method of the *generator*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IushDLEQ9mO2"
      },
      "source": [
        "digit_label=0\n",
        "\n",
        "noise = np.random.normal(0, 1, size=(1, input_noise_dim))\n",
        "digit_label_one_hot=to_categorical(digit_label, category_count).reshape(1,-1)\n",
        "\n",
        "generated_x = cdcgan_generator.predict([noise,digit_label_one_hot])\n",
        "digit = generated_x[0].reshape(original_image_shape)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(digit, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSE4iSal9s-9"
      },
      "source": [
        "Running the code below will show a row of randomly generated images for each digit category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvUPw1hS9tiV"
      },
      "source": [
        "n = 10 # number of images per digit category\n",
        "\n",
        "generated_images=[]\n",
        "for digit_label in range(category_count):\n",
        "  noise = np.random.normal(0, 1, size=(n, input_noise_dim))\n",
        "  digit_label_one_hot=to_categorical(np.full(n,digit_label), category_count)\n",
        "  generated_x = cdcgan_generator.predict([noise,digit_label_one_hot])\n",
        "  generated_images.append([g.reshape(original_image_shape) for g in generated_x])\n",
        "\n",
        "plot_generated_images(generated_images,category_count,n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMG-eJFGbMxy"
      },
      "source": [
        "# **Exercise 6**\n",
        "Train GAN, cGAN, DCGAN and cDCGAN to generate images similar to **fashion MNIST** dataset."
      ]
    }
  ]
}