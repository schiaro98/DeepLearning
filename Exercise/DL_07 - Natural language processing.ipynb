{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_07 - Natural language processing.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural language processing tutorial**\n",
        "In today's tutorial we will design and train deep neural networks to solve a text classification problem.\n",
        "\n",
        "We will use [**TensorFlow**](https://ekababisong.org/gcp-ml-seminar/tensorflow/) framework and [**Keras**](https://keras.io/) open-source library to rapidly prototype deep neural networks."
      ],
      "metadata": {
        "id": "qJNWcJsAx8M2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSNTqqAh8fNi"
      },
      "source": [
        "# **Preliminary operations**\n",
        "The following code downloads all the necessary material into the remote machine. At the end of the execution select the **File** tab to verify that everything has been correctly downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0vJwc1fGYt6"
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "\n",
        "!tar -xzf aclImdb_v1.tar.gz\n",
        "\n",
        "!rm aclImdb_v1.tar.gz\n",
        "!rm -rf aclImdb/train/unsup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxNUGVV98byK"
      },
      "source": [
        "# **Useful modules import**\n",
        "First of all, it is necessary to import useful modules used during the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzs2CezCBkGc"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.spatial import distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhy07t6uRIZ1"
      },
      "source": [
        "# **Utility functions**\n",
        "Execute the following code to define some utility functions used in the tutorial:\n",
        "- **convert_dataset_to_list** converts an instance of the TensorFlow class [**Dataset**](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) containing text data and labels into two lists, the former containing text strings and the latter containing the corresponding labels;\n",
        "- **plot_histograms** plots multiple histograms;\n",
        "- **plot_history** draws in a graph the loss trend over epochs on both training and validation sets. Moreover, if provided, it draws in the same graph also the trend of the given metric;\n",
        "- **plot_embedded_similar_words** plots an embedded vector space highlighting most and less similar words to a selected one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-cmCszORKI9"
      },
      "source": [
        "def convert_dataset_to_list(dataset):\n",
        "  data = []\n",
        "  labels=[]\n",
        "  for text_batch, label_batch in dataset:\n",
        "    for i in range(text_batch.shape[0]):\n",
        "      data.append(text_batch.numpy()[i])\n",
        "      labels.append(label_batch.numpy()[i])\n",
        "\n",
        "  return data,labels\n",
        "\n",
        "def plot_histograms(hist_list,title_list,x_label=None,y_label=None,max_y=None,bins=range(0, 1500, 50),figsize=(20,6)):\n",
        "  _,axs=plt.subplots(1,len(hist_list),figsize=figsize)\n",
        "\n",
        "  for i, hist in enumerate(hist_list):\n",
        "    axs[i].set_title(title_list[i])\n",
        "    if x_label!=None:\n",
        "      axs[i].set_xlabel('# words per review')\n",
        "    if y_label!=None:\n",
        "      axs[i].set_ylabel('count')\n",
        "    if max_y!=None:\n",
        "      axs[i].set_ylim(0,max_y)\n",
        "    axs[i].hist(hist, bins=bins)\n",
        "\n",
        "def plot_history(history,metric=None):\n",
        "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "  epoch_count=len(history.history['loss'])\n",
        "\n",
        "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],label='train_loss',color='orange')\n",
        "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],label='val_loss',color = line1.get_color(), linestyle = '--')\n",
        "  ax1.set_xlim([1,epoch_count])\n",
        "  ax1.set_ylim([0, max(max(history.history['loss']),max(history.history['val_loss']))])\n",
        "  ax1.set_ylabel('loss',color = line1.get_color())\n",
        "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
        "  ax1.set_xlabel('Epochs')\n",
        "  _=ax1.legend(loc='lower left')\n",
        "\n",
        "  if (metric!=None):\n",
        "    ax2 = ax1.twinx()\n",
        "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],label='train_'+metric)\n",
        "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],label='val_'+metric,color = line2.get_color(), linestyle = '--')\n",
        "    ax2.set_ylim([0, max(max(history.history[metric]),max(history.history['val_'+metric]))])\n",
        "    ax2.set_ylabel(metric,color=line2.get_color())\n",
        "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
        "    _=ax2.legend(loc='upper right')\n",
        "\n",
        "def plot_embedded_similar_words(unique_reshaped_embedded_x,unique_reshaped_embedded_x_sorted_indices,sorted_val_unique_words,similar_count,figsize=(25,10),point_size=5):\n",
        "  most_similar_point_coords=unique_reshaped_embedded_x[unique_reshaped_embedded_x_sorted_indices[:similar_count]]\n",
        "  less_similar_point_coords=unique_reshaped_embedded_x[unique_reshaped_embedded_x_sorted_indices[-similar_count:]]\n",
        "\n",
        "  point_colors=['blue' if i in unique_reshaped_embedded_x_sorted_indices[:similar_count] else 'red' if i in unique_reshaped_embedded_x_sorted_indices[-similar_count:] else 'gray' for i in range(unique_reshaped_embedded_x.shape[0])]\n",
        "\n",
        "  _,axs=plt.subplots(1,3,figsize=figsize)\n",
        "  axs[0].scatter(unique_reshaped_embedded_x[:,0],unique_reshaped_embedded_x[:,1],c=point_colors,s=point_size)\n",
        "  axs[0].set_title('Embedded vector space')\n",
        "\n",
        "  axs[1].scatter(most_similar_point_coords[:,0],most_similar_point_coords[:,1],c='blue',s=point_size)\n",
        "  axs[1].set_title('Embedded most similar words')\n",
        "  for i, label in enumerate(sorted_val_unique_words[:similar_count]):\n",
        "    axs[1].annotate(label, (most_similar_point_coords[i][0], most_similar_point_coords[i][1]),fontsize=14)\n",
        "\n",
        "  axs[2].scatter(less_similar_point_coords[:,0],less_similar_point_coords[:,1],c='red',s=point_size)\n",
        "  axs[2].set_title('Embedded less similar words')\n",
        "  for i, label in enumerate(sorted_val_unique_words[-similar_count:]):\n",
        "    axs[2].annotate(label, (less_similar_point_coords[i][0], less_similar_point_coords[i][1]),fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**\n",
        "This tutorial uses the [Stanford’s large movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/) for binary sentiment text classification.\n",
        "\n",
        "The data set contains  a set of 25000 highly polar movie reviews for training, and 25000 for testing.\n",
        "\n",
        "The following code loads in memory the dataset using the [**text_dataset_from_directory**](https://keras.io/api/data_loading/text/#textdatasetfromdirectory-function) function returning an instance of the TensorFlow class **Dataset**."
      ],
      "metadata": {
        "id": "mYXfa91k2Bz1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7NFhpG6JOl3"
      },
      "source": [
        "train_dataset = text_dataset_from_directory('aclImdb/train')\n",
        "test_dataset = text_dataset_from_directory('aclImdb/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **element_spec** attribute can be used to get the type specification of the elements of the dataset. In our case each element is a review and its label: 1 for “positive” and 0 for “negative”."
      ],
      "metadata": {
        "id": "Id0F7N7W61Z2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WyfUy_SUznR"
      },
      "source": [
        "train_dataset.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code converts the training and test **Dataset** instances into lists of reviews and corresponding labels."
      ],
      "metadata": {
        "id": "z-2L0ETzLDfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_reviews,train_y=convert_dataset_to_list(train_dataset)\n",
        "test_reviews,test_y=convert_dataset_to_list(test_dataset)\n",
        "\n",
        "print('Training review count: ',len(train_reviews))\n",
        "print('Training label count: ',len(train_y))\n",
        "print('Test review count: ',len(test_reviews))\n",
        "print('Test label count: ',len(test_y))"
      ],
      "metadata": {
        "id": "ytTcOEoBLELf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualization**\n",
        "The first *review_count* training reviews can be shown by executing the following code."
      ],
      "metadata": {
        "id": "kbkHxTAs85dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_count=5\n",
        "\n",
        "for i in range(review_count):\n",
        "  print(train_y[i],train_reviews[i])"
      ],
      "metadata": {
        "id": "s69QIu33Nd_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN6fYit_MZiP"
      },
      "source": [
        "## **Data preparation**\n",
        "Most machine learning algorithms require data to be formatted in a specific way, so datasets generally require some amount of preparation before they can yield useful insights. Some datasets have values that are missing, invalid, or otherwise difficult for an algorithm to process."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Decode UTF-8-encoded string**\n",
        "The 'b' character before review strings means that they are encoded using UTF-8 format. \n",
        "\n",
        "The following code converts strings from UTF-8 to Unicode format using the [**decode**](https://docs.python.org/3/library/stdtypes.html#bytes.decode) method. "
      ],
      "metadata": {
        "id": "pIm1KLSyOKw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_train_reviews=[review.decode('utf-8') for review in train_reviews]\n",
        "prepared_test_reviews=[review.decode('utf-8') for review in test_reviews]\n",
        "\n",
        "for i in range(review_count):\n",
        "  print(train_y[i],prepared_train_reviews[i])"
      ],
      "metadata": {
        "id": "RaCwCuHASwdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove HTML line break tag**\n",
        "The HTML line break tag is present in most of the movie reviews.\n"
      ],
      "metadata": {
        "id": "QLjJrDw_ERzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "br_tag_count=0\n",
        "for review in prepared_train_reviews:\n",
        "  br_tag_count+=review.count('<br />')\n",
        "print('HTML line break tag occurrences in training set reviews: ',br_tag_count)"
      ],
      "metadata": {
        "id": "oGhJ9bfjV6ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because it is not an English vocabulary word, it is better to replace it with a blank space using the [**replace**](https://docs.python.org/3/library/stdtypes.html#str.replace) method."
      ],
      "metadata": {
        "id": "Cwl5PUw8XL1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_train_reviews=[review.replace('<br />', ' ') for review in prepared_train_reviews]\n",
        "prepared_test_reviews=[review.replace('<br />', ' ') for review in prepared_test_reviews]"
      ],
      "metadata": {
        "id": "ZqexUBQoUIpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Split data into training and validation sets**\n",
        "In order to avoid overfitting during training, it is necessary to have a separate dataset (called validation set), in addition to the training and test datasets, to choose the optimal value for the hyperparameters. \n",
        "\n",
        "For this reason, *prepared_train_reviews* and *train_y* are divided into two subsets: training and validation sets. \n",
        "\n",
        "Scikit-learn library provides the function [**train_test_split**](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to separate a dataset into two parts.\n",
        "\n",
        "The *val_size* variable represents the percentage (or the absolute number) of patterns to include in the validation set.\n",
        "\n",
        "By default, **train_test_split** mixes patterns in order to avoid that returned datasets contain patterns belonging only to a subset of the classes."
      ],
      "metadata": {
        "id": "ZMFi2Av0kKKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_size=5000\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(prepared_train_reviews, train_y, test_size=val_size, random_state=42,shuffle=True)\n",
        "\n",
        "print('Training review count: ',len(train_x))\n",
        "print('Training label count: ',len(train_y))\n",
        "print('Validation review count: ',len(val_x))\n",
        "print('Validation label count: ',len(val_y))"
      ],
      "metadata": {
        "id": "3udY_E7OlE4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Convert label lists into Numpy arrays**\n",
        "It is convenient to keep the data in the form of Numpy arrays instead of lists. While reviews need further processing before they can be transformed into Numpy arrays, the labels can be already converted.\n",
        "\n",
        "The following code converts the labels from lists of integers into Numpy arrays using the Numpy [**array**](https://numpy.org/doc/stable/reference/generated/numpy.array.html) function."
      ],
      "metadata": {
        "id": "g2pGwENFfHZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_y=np.array(train_y)\n",
        "val_y=np.array(val_y)\n",
        "test_y=np.array(test_y)\n",
        "\n",
        "print('Training label shape: ',train_y.shape)\n",
        "print('Validation label shape: ',val_y.shape)\n",
        "print('Test label shape: ',test_y.shape)"
      ],
      "metadata": {
        "id": "wJ4kcakvfIOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text tokenization**\n",
        "Raw text cannot be directly fed into deep learning models. Text data must be encoded as numbers before it can be used as input or output for machine learning and deep learning models.\n",
        "\n",
        "Keras provides the [**Tokenizer**](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) class for preparing text documents for deep learning. It allows to vectorize a text corpus, by turning each text into a sequence of integers where each integer is the index of a word (or token) in a dictionary.\n",
        "\n",
        "The following code creates a new instance of the **Tokenizer** class."
      ],
      "metadata": {
        "id": "KU2ER_gvaKAJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrJfu3OGTQGG"
      },
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [**fit_on_texts**](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts) method can be used to create the internal vocabulary based on training word frequency."
      ],
      "metadata": {
        "id": "_EOjwm2EnAL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(train_x)"
      ],
      "metadata": {
        "id": "SkCkPKLonA71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The internal vocabulary can be accessed through the *word_counts* attribute: an ordered dictionary containing all words used to create the vocabulary and their corresponding frequency."
      ],
      "metadata": {
        "id": "IvlndWQUo2uB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If_9FgLPTfYd"
      },
      "source": [
        "print(tokenizer.word_counts)\n",
        "print('Word count: ',len(tokenizer.word_counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another important attribute is *word_index*, a dictionary of words and their uniquely assigned integers."
      ],
      "metadata": {
        "id": "_f9Z8OM0rXdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "wGVPcprwrYMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the **Tokenizer** has been fit on training data, the [**texts_to_sequences**](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences) method can be used to encode documents of the training, validation and test datasets by transforming each movie review into a sequence of integers. \n",
        "\n",
        "The *num_words* parameter represents the maximum number of words to keep, based on word frequency."
      ],
      "metadata": {
        "id": "p8wRJCHGnfQe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxVeM8vt4Tfr"
      },
      "source": [
        "num_words=50000\n",
        "\n",
        "tokenizer.num_words=num_words\n",
        "\n",
        "tokenized_train_x=tokenizer.texts_to_sequences(train_x)\n",
        "tokenized_val_x=tokenizer.texts_to_sequences(val_x)\n",
        "tokenized_test_x=tokenizer.texts_to_sequences(prepared_test_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code shows a result of the tokenization process on a training movie review."
      ],
      "metadata": {
        "id": "fSYOvM_Kus94"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex5FwKJi5zWc"
      },
      "source": [
        "idx=0\n",
        "print(tokenized_train_x[idx])\n",
        "print('Review length: ',len(tokenized_train_x[idx]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To recover the original text, the [**sequences_to_texts**](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#sequences_to_texts) method can be used."
      ],
      "metadata": {
        "id": "atuJ2xHquxXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.sequences_to_texts([tokenized_train_x[idx]]))"
      ],
      "metadata": {
        "id": "DIrkMGE8uyD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make all reviews of the same length**\n",
        "As shown by the following code, each text sequence has (in most cases) a different number of words."
      ],
      "metadata": {
        "id": "RwlvAoVHwUQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_histograms([[len(x) for x in tokenized_train_x],[len(x) for x in tokenized_val_x],[len(x) for x in tokenized_test_x]],\n",
        "                ['Training set','Validation set','Test set'],x_label='# words per review',y_label='count',max_y=8000,bins=range(0, 1500, 50),figsize=(20,6))"
      ],
      "metadata": {
        "id": "9kS8WxhJ5SSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid this problem, the [**pad_sequences**](https://keras.io/api/preprocessing/timeseries/#padsequences-function) function can be used. It transforms each sequence into a Numpy array of predefined length (*maxlen* parameter):\n",
        "- sequences that are shorter than *maxlen* are padded with zeros;\n",
        "- sequences longer than *maxlen* are truncated."
      ],
      "metadata": {
        "id": "HbbGQp7KCeM_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XWqq8136QYZ"
      },
      "source": [
        "maxlen=500\n",
        "\n",
        "padded_train_x=pad_sequences(tokenized_train_x, maxlen=maxlen)\n",
        "padded_val_x=pad_sequences(tokenized_val_x, maxlen=maxlen)\n",
        "padded_test_x=pad_sequences(tokenized_test_x, maxlen=maxlen)\n",
        "\n",
        "print('Training feature shape: ',padded_train_x.shape)\n",
        "print('Validation feature shape: ',padded_val_x.shape)\n",
        "print('Test feature shape: ',padded_test_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep RNN**\n",
        "In this section a deep RNN is implemented to binary classify movie reviews."
      ],
      "metadata": {
        "id": "M-XEDIL6Ec3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model definition**\n",
        "The following function creates a deep RNN model given:\n",
        "- the number of timesteps in each input sequence (*timesteps*);\n",
        "- the number of features in each timestep (*feature_count*);\n",
        "- the number of units for each RNN layer (*unit_count_per_rnn_layer*).\n",
        "\n",
        "The model returns a single target value given an entire sequence as input (*many-to-one*).\n",
        "\n",
        "<u>Note that, the number of timesteps in each input sequence (*timesteps*) is set in advance only because it improves performance during training by creating tensors of fixed shapes. A *None* value can be used to admit variable-length input sequences.</u>\n",
        "\n",
        "In Keras, a sequential is a stack of layers where each layer has exactly one input and one output. It can be created by passing a list of layers to the  constructor [**keras.Sequential**](https://keras.io/guides/sequential_model/).\n",
        "\n",
        "[**Keras layers API**](https://keras.io/api/layers/) offers a wide range of built-in layers ready for use, including:\n",
        "- [**Input**](https://keras.io/api/layers/core_layers/input/) - the input of the model. Note that, you can also omit the **Input** layer. In that case the model doesn't have any weights until the first call to a training/evaluation method (since it is not yet built);\n",
        "- [**SimpleRNN**](https://keras.io/api/layers/recurrent_layers/simple_rnn/) - a fully-connected RNN where the output is to be fed back to input;\n",
        "- [**Dense**](https://keras.io/api/layers/core_layers/dense/) - a fully-connected layer.\n",
        "\n",
        "The *return_sequences* parameter of the **SimpleRNN** layer serves to return the full output time sequence (True), or only the last output (False)."
      ],
      "metadata": {
        "id": "XzMYwB7ZJR4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_deep_rnn(timesteps,feature_count,unit_count_per_rnn_layer=[1]):\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Input(shape=(timesteps,feature_count)))\n",
        "\n",
        "  for i in range(len(unit_count_per_rnn_layer)):\n",
        "    model.add(layers.SimpleRNN(unit_count_per_rnn_layer[i],activation='sigmoid',return_sequences=i<(len(unit_count_per_rnn_layer)-1)))\n",
        "\n",
        "  if unit_count_per_rnn_layer[-1]>1:\n",
        "    model.add(layers.Dense(1,activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "PrDUi5AXEhiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model creation**\n",
        "The following code creates a deep RNN model by calling the **build_deep_rnn** function defined above."
      ],
      "metadata": {
        "id": "q-2FHFjxKQLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deep_rnn=build_deep_rnn(maxlen,1,unit_count_per_rnn_layer=[8])"
      ],
      "metadata": {
        "id": "nKIMX0gWElkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ],
      "metadata": {
        "id": "R5XWv8lhKWUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deep_rnn.summary()"
      ],
      "metadata": {
        "id": "YjkZuu85EwVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ],
      "metadata": {
        "id": "_ZGh3wjyKbl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(deep_rnn,show_shapes=True,show_layer_names=False)"
      ],
      "metadata": {
        "id": "PZsY88iVEz7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model compilation**\n",
        "The compilation is the final step in configuring the model for training. \n",
        "\n",
        "The following code use the [**compile**](https://keras.io/api/models/model_training_apis/#compile-method) method to compile the model.\n",
        "The important arguments are:\n",
        "- the optimization algorithm (*optimizer*);\n",
        "- the loss function (*loss*);\n",
        "- the metrics used to evaluate the performance of the model (*metrics*).\n",
        "\n",
        "The most common [optimization algorithms](https://keras.io/api/optimizers/#available-optimizers), [loss functions](https://keras.io/api/losses/#available-losses) and [metrics](https://keras.io/api/metrics/#available-metrics) are already available in Keras. You can either pass them to **compile** as an instance or by the corresponding string identifier. In the latter case, the default parameters will be used."
      ],
      "metadata": {
        "id": "mlZc9z9zKmrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deep_rnn.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "I_6YSoawE5ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data shape expansion**\n",
        "Recurrent neural networks expect the input data to be provided with a specific array structure in the form of: [*samples*, *time steps*, *features*] while our data is in the form: [*samples*, *time steps*].\n",
        "\n",
        "We can transform our data into the expected structure using the Numpy function [**expand_dims**](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html)."
      ],
      "metadata": {
        "id": "Ad3PUthiFNaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_padded_train_x =np.expand_dims(padded_train_x, axis=2)\n",
        "expanded_padded_val_x =np.expand_dims(padded_val_x, axis=2)\n",
        "expanded_padded_test_x =np.expand_dims(padded_test_x, axis=2)\n",
        "\n",
        "print('Expanded training feature shape: ',expanded_padded_train_x.shape)\n",
        "print('Expanded validation feature shape: ',expanded_padded_val_x.shape)\n",
        "print('Expanded test feature shape: ',expanded_padded_test_x.shape)"
      ],
      "metadata": {
        "id": "GCctWJF1FRkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**\n",
        "Now we are ready to train our model by calling the [**fit**](https://keras.io/api/models/model_training_apis/#fit-method) method.\n",
        "\n",
        "It trains the model for a fixed number of epochs (*epoch_count*) using the training set (*expanded_padded_train_x*) divided into mini-batches of *batch_size* elements. During the training process, the performances will be evaluated on both training and validation (*expanded_padded_val_x*) sets.\n",
        "\n",
        "Break training when a metric or the loss has stopped improving on the validation set, helps to avoid overfitting.\n",
        "\n",
        "For this purpose, Keras provides a class called [**EarlyStopping**](https://keras.io/api/callbacks/early_stopping/). Important class parameters are:\n",
        "- *monitor* - the name of the metric or the loss to be observed; \n",
        "- *patience* - the number of epochs with no improvement after which training will be stopped;\n",
        "- *restore_best_weights* - whether to restore model weights from the epoch with the best value of the monitored quantity.\n",
        "\n",
        "Once created an instance of the **EarlyStopping** class, it can be passed to the **fit** method in the *callbacks* parameter."
      ],
      "metadata": {
        "id": "fzrl5QgwMuuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = 20\n",
        "batch_size = 1000\n",
        "patience=5\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "history = deep_rnn.fit(expanded_padded_train_x,train_y,validation_data=(expanded_padded_val_x,val_y),epochs=epoch_count,batch_size = batch_size,callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "M7q56LALE-RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize the training process**\n",
        "We can learn a lot about our model by observing the graph of its performance over time during training.\n",
        "\n",
        "The **fit** method returns an object (*history*) containing loss and metrics values at successive epochs for both training and validation sets.\n",
        "\n",
        "The following code calls the **plot_history** function defined above to draw in a graph the loss and accuracy trend over epochs on both training and validation sets."
      ],
      "metadata": {
        "id": "iJx3RxBoNKb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history,metric='accuracy')"
      ],
      "metadata": {
        "id": "o4ZF6JhvHyPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Performance evaluation on the test set**\n",
        "The performance on the test set can be easily measured by calling the **evaluate** method."
      ],
      "metadata": {
        "id": "HIRG4CTwNZ5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = deep_rnn.evaluate(expanded_padded_test_x, test_y, batch_size=batch_size,verbose=0)\n",
        "print('Loss: {:.3f} Accuracy: {:.3f}'.format(results[0],results[1]))"
      ],
      "metadata": {
        "id": "4imbONpAH29m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results obtained are very poor. This is mainly due to the *problem of sparsity*. "
      ],
      "metadata": {
        "id": "sDYiZbLoOJ-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem of sparsity**\n",
        "\n",
        "With text tokenization, each word has been turned into an integer (representing the index of the word in a dictionary) but such integer values are not able to well represent the meaning similarity between words. \n",
        "\n",
        "For instance, as shown in the following cell, although ‘brilliant’ and ‘beautiful’ have similar meaning, their tokens are more distant than ‘brilliant’ and ‘horrible’."
      ],
      "metadata": {
        "id": "V3QOPwvfospL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_word='brilliant'\n",
        "second_word='beautiful'\n",
        "third_word='horrible'\n",
        "\n",
        "first_word_token=tokenizer.texts_to_sequences([first_word])[0][0]\n",
        "second_word_token=tokenizer.texts_to_sequences([second_word])[0][0]\n",
        "third_word_token=tokenizer.texts_to_sequences([third_word])[0][0]\n",
        "\n",
        "print('Distance between \\'{}\\' and \\'{}\\': |{}-{}|={}'.format(first_word,second_word,first_word_token,second_word_token,abs(first_word_token-second_word_token)))\n",
        "print('Distance between \\'{}\\' and \\'{}\\': |{}-{}|={}'.format(first_word,third_word,first_word_token,third_word_token,abs(first_word_token-third_word_token)))"
      ],
      "metadata": {
        "id": "QxB2_fIegUl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word embedding**\n",
        "*Word embedding* is a class of approaches for representing words using a dense vector representation. Individual words are represented as real-valued vectors in a predefined vector space where a real-valued vector encodes the meaning of the corresponding word such that words similar in meaning are closer in the vector space.\n",
        "\n",
        "A word embedding can be learned as part of a deep learning model where the position of a word within the vector space is learned from text and is based on the words that surround the word when it is used."
      ],
      "metadata": {
        "id": "FusICipZk5F8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model definition**\n",
        "The following function creates a word embedding model given:\n",
        "- the size of the vocabulary (*vocab_size*);\n",
        "- the size of the embedded vector space (*output_dim*);\n",
        "- the length of the input sequences (*input_length*).\n",
        "\n",
        "Keras offers an [**Embedding**](https://keras.io/api/layers/core_layers/embedding/#embedding) layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer (as we already done with the **Tokenizer**). \n",
        "\n",
        "It is a flexible layer that can be used in different ways:\n",
        "- it can be used alone to learn a word embedding that can be saved and used in another model later;\n",
        "- it can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
        "\n",
        "Usually the **Embedding** layer is defined as the first layer of a network specifying three arguments:\n",
        "- *input_dim*: the size of the vocabulary in the text data (*num_words* variable used in text tokenization);\n",
        "- *output_dim*: the size of the vector space in which words will be embedded;\n",
        "- *input_length*: the length of input sequences (*maxlen* variable).\n",
        "\n",
        "The output of the **Embedding** layer is a 2D array with one embedding for each word in the input sequence.\n",
        "\n",
        "To directly connect an **Embedding** layer to a fully-connected layer, you must first flatten the 2D output to a 1D array using the [**Flatten**](https://keras.io/api/layers/reshaping_layers/flatten/) layer."
      ],
      "metadata": {
        "id": "zdNJbyoE-Eqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_model(vocab_size,output_dim,input_length):\n",
        "  embedding_model=keras.Sequential()\n",
        "  embedding_model.add(layers.Input(shape=(input_length)))\n",
        "  embedding_layer=layers.Embedding(vocab_size, output_dim, input_length=input_length)\n",
        "  embedding_model.add(embedding_layer)\n",
        "  embedding_model.add(layers.Flatten())\n",
        "  embedding_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "            \n",
        "  return embedding_model,embedding_layer"
      ],
      "metadata": {
        "id": "6LI4gE03qE00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model creation**\n",
        "The following code creates a word embedding model by calling the **build_embedding_model** function defined above. The *embedded_vector_dim* variable represents the size of the vector space of the **Embedding** layer. "
      ],
      "metadata": {
        "id": "5bD-29_ifQEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_vector_dim=2\n",
        "\n",
        "emb_model,emb_layer=build_embedding_model(num_words,embedded_vector_dim,maxlen)"
      ],
      "metadata": {
        "id": "C-bcxhitrDxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ],
      "metadata": {
        "id": "eUmEHSMwgCUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_model.summary()"
      ],
      "metadata": {
        "id": "cmCx0aZJre4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ],
      "metadata": {
        "id": "LFlqEPqEgPXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(emb_model,show_shapes=True,show_layer_names=False)"
      ],
      "metadata": {
        "id": "eIdBlccDsfp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model compilation**\n",
        "The following code compiles the model as already done before."
      ],
      "metadata": {
        "id": "lBfvl_QugYkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Y13gKco-stSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**\n",
        "Now we are ready to train our word embedding model on the *padded_train_x* dataset by calling the **fit** method."
      ],
      "metadata": {
        "id": "2ULL4DTygo3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = 50\n",
        "batch_size = 1000\n",
        "patience=5\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "history = emb_model.fit(padded_train_x,train_y,validation_data=(padded_val_x,val_y),epochs=epoch_count,batch_size = batch_size,callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "XzYQfkv-sy30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize the training process**\n",
        "The following code calls the **plot_history** function defined above to draw in a graph the loss and accuracy trend over epochs on both training and validation sets."
      ],
      "metadata": {
        "id": "PeKuEG2xhRQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history,metric='accuracy')"
      ],
      "metadata": {
        "id": "z-cQJNCEtahz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data embedding**\n",
        "To apply the trained **Embedding** layer to our data, it is necessary get its output. To do this, we need to create a new Keras [**Model**](https://keras.io/api/models/model/) and setting:\n",
        "- its inputs equal to the input of the trained embedding model (*emb_model*);\n",
        "- its outputs equal to the output of the trained **Embedding** layer (*emb_layer*).\n",
        "\n",
        "By executing the following code, the new **Model** is created."
      ],
      "metadata": {
        "id": "bMuLws1Lh1Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_layer_model=keras.Model(inputs=emb_model.input,outputs=emb_layer.output)"
      ],
      "metadata": {
        "id": "eTJqV0TvAvMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [**predict**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) method of the **Model** created above can be used to generate the word embeddings (*embedded_train_y*, *embedded_val_y* and *embedded_test_y*) of the training, validation and test sets (*padded_train_x*, *padded_val_x* and *padded_test_x*)."
      ],
      "metadata": {
        "id": "vHty7V36mxe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_train_x=emb_layer_model.predict(padded_train_x)\n",
        "embedded_val_x=emb_layer_model.predict(padded_val_x)\n",
        "embedded_test_x=emb_layer_model.predict(padded_test_x)\n",
        "\n",
        "print('Embedded training feature shape: ',embedded_train_x.shape)\n",
        "print('Embedded validation feature shape: ',embedded_val_x.shape)\n",
        "print('Embedded test feature shape: ',embedded_test_x.shape)"
      ],
      "metadata": {
        "id": "axegJy3BBCKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embedded vector space visualization**\n",
        "To use Matplotlib functionalities to plot embedded vectors, the data need to be reshaped from the form [*samples*, *time steps*, *features*] to the form [*words*, *features*].\n",
        "\n",
        "We can transform our data into the expected structure using the Numpy function [**reshape**](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html).\n",
        "\n",
        "Due to the size of the training and test sets, here we focused only on the visualization of the validation set that is the smallest one."
      ],
      "metadata": {
        "id": "UmvGdKV5o7wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_embedded_val_x=embedded_val_x.reshape(-1,embedded_val_x.shape[-1])\n",
        "\n",
        "print('Embedded validation feature shape: ',reshaped_embedded_val_x.shape)"
      ],
      "metadata": {
        "id": "wOyMkc9RrW9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code visualize the embedded validation set."
      ],
      "metadata": {
        "id": "Kywq9vCvxruL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(reshaped_embedded_val_x[:,0],reshaped_embedded_val_x[:,1],c='gray',s=3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_In9bhKXxT0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Compute distance between embedded vectors**\n",
        "To evaluate if the problem of sparsity has been solved by word embedding, the Euclidean distance between the embedded vectors of the words used before ('brilliant', 'beautiful' and 'horrible') will be computed.\n",
        "\n",
        "To be used as input of the trained **Embedding** layer, the three word tokens are encapsulated at the beginning of a sequence of *maxlen* elements zero initialized.\n",
        "\n",
        "The embedded vectors of the three words can be shown executing the following code.\n"
      ],
      "metadata": {
        "id": "wSbidwMzfSeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_token_sequence=np.zeros((1,maxlen))\n",
        "word_token_sequence[0,0]=first_word_token\n",
        "word_token_sequence[0,1]=second_word_token\n",
        "word_token_sequence[0,2]=third_word_token\n",
        "\n",
        "emb_word_token_sequence=emb_layer_model.predict(word_token_sequence)\n",
        "\n",
        "first_word_embedding=emb_word_token_sequence[0][0]\n",
        "second_word_embedding=emb_word_token_sequence[0][1]\n",
        "third_word_embedding=emb_word_token_sequence[0][2]\n",
        "\n",
        "print('\\'{}\\' embedding: {}'.format(first_word,first_word_embedding))\n",
        "print('\\'{}\\' embedding: {}'.format(second_word,second_word_embedding))\n",
        "print('\\'{}\\' embedding: {}'.format(third_word,third_word_embedding))"
      ],
      "metadata": {
        "id": "CkoDzGwNfZVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [**euclidean**](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html) function of the [**SciPy**](https://scipy.org/) library is used to compute the Euclidean distance between the three word embeddings."
      ],
      "metadata": {
        "id": "4ZRgV5YlnLHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Euclidean distance between \\'{}\\' and \\'{}\\' embeddings: {:.2f}'.format(first_word,second_word,distance.euclidean(first_word_embedding,second_word_embedding)))\n",
        "print('Euclidean distance between \\'{}\\' and \\'{}\\' embeddings: {:.2f}'.format(first_word,third_word,distance.euclidean(first_word_embedding,third_word_embedding)))"
      ],
      "metadata": {
        "id": "by7PQRfnnLz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differently from token distances, ‘brilliant’ and ‘beautiful’ embeddings are much closer than ‘brilliant’ and ‘horrible’ ones."
      ],
      "metadata": {
        "id": "-SGTUptIrLkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualize most and less similar words**\n",
        "In this section, to confirm the importance of word embedding, once a word is selected (*selected_word*) its most and less similar words in the embedded vector space will visualized.\n",
        "\n",
        "First of all, the token of the *selected_word* must be derived using the **Tokenizer** created above in the text tokenization step."
      ],
      "metadata": {
        "id": "BrDkCtw9yPb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_word='brilliant'\n",
        "\n",
        "selected_word_token=tokenizer.texts_to_sequences([selected_word])\n",
        "\n",
        "print('\\'{}\\'={}'.format(selected_word,selected_word_token[0][0]))"
      ],
      "metadata": {
        "id": "0b-ypt7Y1naN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Numpy **zeros** function is used to create an array of *maxlen* elements starting with the *selected_word_token* and followed by all zeros."
      ],
      "metadata": {
        "id": "7_AAav1M0XAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_token_sequence=np.zeros((1,maxlen))\n",
        "word_token_sequence[0,0]=selected_word_token[0][0]\n",
        "\n",
        "print('Sequence shape: ',word_token_sequence.shape)"
      ],
      "metadata": {
        "id": "jmnQVplh3jiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corresponding embedded vector can be generated using the **predict** method of the trained **Embedding** layer."
      ],
      "metadata": {
        "id": "sagxG5ai2kR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_word_token_sequence=emb_layer_model.predict(word_token_sequence)\n",
        "embedded_selected_word=emb_word_token_sequence[0,0]\n",
        "\n",
        "print('Embedded sequence shape: ',emb_word_token_sequence.shape)\n",
        "print('\\'{}\\' embedding={}'.format(selected_word,embedded_selected_word))"
      ],
      "metadata": {
        "id": "F4ag5-MQ4e4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the most and less similar words, it is necessary to compute the distance in the embedded vector space between the selected word and all words in the validation set. \n",
        "\n",
        "Since the validation set contains multiple instances of the same word, to avoid wasting time computing multiple times the same distance, only an instance of each embedded vector is chosen using the Numpy [**unique**](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) function."
      ],
      "metadata": {
        "id": "ZkmJTKQ94grT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_reshaped_embedded_val_x,unique_reshaped_embedded_val_x_indices = np.unique(reshaped_embedded_val_x,return_index=True, axis=0)\n",
        "\n",
        "print('Unique embedded validation set shape: ',unique_reshaped_embedded_val_x.shape)"
      ],
      "metadata": {
        "id": "m8UrTwcODqGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute the Euclidean distance between the embedded vector of the selected word (*embedded_selected_word*) and the embedded vectors of all unique words in the validation set (*unique_reshaped_embedded_val_x*), the [**cdist**](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html) function of the SciPy library is used."
      ],
      "metadata": {
        "id": "k_uAimJhXnbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distances=distance.cdist([embedded_selected_word], unique_reshaped_embedded_val_x)\n",
        "\n",
        "print('Distances shape: ',distances.shape)"
      ],
      "metadata": {
        "id": "AxcqsFGa_5Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to recover the unique words of the validation set starting from their embedded vectors.\n",
        "\n",
        "To do this, the following steps are executed:\n",
        "1. the indices of the unique embedded vectors in the validation set are sorted in ascending order (according to their distance from the selected word) using the Numpy function [**argsort**](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html);\n",
        "2. the sorted indices are backward mapped into 2D indices indicating the corresponding sequence ID in the validation set (*sorted_val_sequence_indices*) and the word ID inside the sequence (*sorted_val_sequence_word_indices*);\n",
        "3. the sorted unique word tokens (*sorted_val_unique_tokens*) corresponding to the sorted unique embedded vectors are taken;\n",
        "4. the sorted unique words (*sorted_val_unique_words*) corresponding to the sorted unique word tokens are recovered using the **sequences_to_texts** method of the **Tokenizer**. "
      ],
      "metadata": {
        "id": "3-64is-NYk9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "unique_reshaped_embedded_val_x_sorted_indices=np.argsort(distances[0])\n",
        "\n",
        "#2\n",
        "sorted_val_sequence_indices=unique_reshaped_embedded_val_x_indices[unique_reshaped_embedded_val_x_sorted_indices]//embedded_val_x.shape[1]\n",
        "sorted_val_sequence_word_indices=unique_reshaped_embedded_val_x_indices[unique_reshaped_embedded_val_x_sorted_indices]%embedded_val_x.shape[1]\n",
        "\n",
        "#3\n",
        "sorted_val_unique_tokens=padded_val_x[sorted_val_sequence_indices,sorted_val_sequence_word_indices]\n",
        "\n",
        "#4\n",
        "sorted_val_unique_words=tokenizer.sequences_to_texts([sorted_val_unique_tokens])[0].split()\n",
        "\n",
        "print(sorted_val_unique_words)"
      ],
      "metadata": {
        "id": "fARjje5QC6KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code visualize the embedded vectors of the validation set, highlighting the most and less similar words to the selected one (*selected_word*). "
      ],
      "metadata": {
        "id": "hQjM1Hjr_l-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_count=10\n",
        "\n",
        "plot_embedded_similar_words(unique_reshaped_embedded_val_x,unique_reshaped_embedded_val_x_sorted_indices,sorted_val_unique_words,similar_count,figsize=(20,6))"
      ],
      "metadata": {
        "id": "5xAmxmj2CX65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep RNN with word embedding**\n",
        "In this section a deep RNN is combined with word embedding to binary classify movie reviews."
      ],
      "metadata": {
        "id": "fOBnsn95UfZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model creation**\n",
        "The following code creates a deep RNN model by calling the **build_deep_rnn** function defined above.\n",
        "\n",
        "In this case the number of features used to represent each word (*feature_count*) is equal to the size of the embedded vector space (*embedded_vector_dim*)."
      ],
      "metadata": {
        "id": "p07bOCleGvxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_deep_rnn=build_deep_rnn(maxlen,embedded_vector_dim,unit_count_per_rnn_layer=[8])"
      ],
      "metadata": {
        "id": "KmWtlDY7G97W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ],
      "metadata": {
        "id": "ibIw4VW-Hc8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_deep_rnn.summary()"
      ],
      "metadata": {
        "id": "uVkVWyLBHflH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ],
      "metadata": {
        "id": "Qrmb60nBNdLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(emb_deep_rnn,show_shapes=True,show_layer_names=False)"
      ],
      "metadata": {
        "id": "WuMpNMM5NdTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model compilation**\n",
        "The following code compiles the model as already done before."
      ],
      "metadata": {
        "id": "sVkSTimkIuqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_deep_rnn.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "XgKHmzQCI13f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**\n",
        "Now the model is ready to be trained on the embedded vectors by calling the **fit** method."
      ],
      "metadata": {
        "id": "dec0QyySKCZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = 50\n",
        "batch_size = 1000\n",
        "patience=5\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "history = emb_deep_rnn.fit(embedded_train_x,train_y,validation_data=(embedded_val_x,val_y),epochs=epoch_count,batch_size = batch_size,callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "JyJOC6ZzKLoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize the training process**\n",
        "The following code calls the **plot_history** function defined above to draw in a graph the loss and accuracy trend over epochs on both training and validation sets."
      ],
      "metadata": {
        "id": "BJ2ON2vzN41S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history,metric='accuracy')"
      ],
      "metadata": {
        "id": "1YmvFRNMN9FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Performance evaluation on the test set**\n",
        "The performance on the test set can be measured by calling the **evaluate** method."
      ],
      "metadata": {
        "id": "ZY4z7o2yNzlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = emb_deep_rnn.evaluate(embedded_test_x, test_y, batch_size=batch_size,verbose=0)\n",
        "print('Loss: {:.3f} Accuracy: {:.3f}'.format(results[0],results[1]))"
      ],
      "metadata": {
        "id": "BY1mQb9zOHFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOXcpBhHDPng"
      },
      "source": [
        "## **Classify user-defined movie reviews**\n",
        "To evaluate the accuracy of the trained model on new data, please write, in the cell below, a positive and a negative movie review (in English)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDYC-X8nEt3Y"
      },
      "source": [
        "reviews=['...',\n",
        "         '...']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By executing the following code, the reviews will be:\n",
        "1. tokenized;\n",
        "2. padded;\n",
        "3. expanded by inserting a new axis. "
      ],
      "metadata": {
        "id": "2qM6gdjy6Sgj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq6dKvNIFByY"
      },
      "source": [
        "#1\n",
        "tokenized_reviews=tokenizer.texts_to_sequences(reviews)\n",
        "print(tokenized_reviews)\n",
        "\n",
        "#2\n",
        "padded_reviews=pad_sequences(tokenized_reviews, maxlen=maxlen)\n",
        "print('Padded review shape: ',padded_reviews.shape)\n",
        "\n",
        "3#\n",
        "expanded_padded_reviews =np.expand_dims(padded_reviews, axis=2)\n",
        "print('Expanded review shape: ',expanded_padded_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedded vectors can be derived by applying the trained **Embedding** layer to the pre-processed reviews."
      ],
      "metadata": {
        "id": "KA-kJSyW9nTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_reviews=emb_layer_model.predict(expanded_padded_reviews)\n",
        "\n",
        "print('Embedded review shape: ',embedded_reviews.shape)"
      ],
      "metadata": {
        "id": "3IxIwG1W4xaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the classification results can be obtained using its **predict** method."
      ],
      "metadata": {
        "id": "ip9c7f2_-LFd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmP8fymHF42o"
      },
      "source": [
        "review_preds=emb_deep_rnn.predict(embedded_reviews)\n",
        "\n",
        "for i,p in enumerate(review_preds):\n",
        "  print('\\'{}\\' {} [{:.2f}]'.format(reviews[i],'POSITIVE' if p>=0.5 else 'NEGATIVE',p[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1**\n",
        "Keras provides specific layers to implement *Long Short-Term Memory* ([**LSTM**](https://keras.io/api/layers/recurrent_layers/lstm/)) and *Gated Recurrent Units* ([**GRU**](https://keras.io/api/layers/recurrent_layers/gru/)) networks.\n",
        "\n",
        "Evaluate the performance of LSTM and GRU networks on binary sentiment text classification problem using the embedded vectors as input. \n",
        "\n",
        "Function **build_deep_rnn** defined above can be used as starting point by replacing the **SimpleRNN** layers with **LSTM** or **GRU** layers."
      ],
      "metadata": {
        "id": "qyaVBXt3hwKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 2**\n",
        "Define and train a 1D CNN to binary classify movie reviews:\n",
        "1. define a 1D CNN model implementing the **build_1d_cnn** function;\n",
        "2. execute the training process;\n",
        "3. evaluate the performance of the trained model on the test set."
      ],
      "metadata": {
        "id": "ymXyIZcw-hFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model definition**\n",
        "Implement the following function to create a 1D CNN model given:\n",
        "- the number of words in each input review (*word_count*);\n",
        "- the number of features used to represent each word (*feature_count*).\n",
        "\n",
        "The model returns a single target value given a movie review as input.\n",
        "\n",
        "To create 1d convolutional layers, the [**Conv1D**](https://keras.io/api/layers/convolution_layers/convolution1d/) class provided by Keras can be used. An example on how to use 1D convolutional layers for text classification is reported [here](https://keras.io/examples/nlp/text_classification_from_scratch/#build-a-model)."
      ],
      "metadata": {
        "id": "SCQ10Um5rCYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_1d_cnn(timesteps,feature_count):\n",
        "  #..."
      ],
      "metadata": {
        "id": "-9qVC4x_m-d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model creation**\n",
        "The following code creates a 1D CNN by calling the **build_1d_cnn** function defined above."
      ],
      "metadata": {
        "id": "bXzYcNgbtP19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_1d_cnn=build_1d_cnn(maxlen,embedded_vector_dim)"
      ],
      "metadata": {
        "id": "pId97zpangiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ],
      "metadata": {
        "id": "dTHlNputtLls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_1d_cnn.summary()"
      ],
      "metadata": {
        "id": "HMLq5tw2njr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ],
      "metadata": {
        "id": "8m6ZWvqZxb0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(emb_1d_cnn,show_shapes=True,show_layer_names=False)"
      ],
      "metadata": {
        "id": "f1-ygSVDnn3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model compilation**\n",
        "The following code compiles the model as already done for the deep RNN."
      ],
      "metadata": {
        "id": "gViRstvAxhIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_1d_cnn.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "zV2nnZWFoRs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**\n",
        "Now we are ready to train our model by calling the **fit** method."
      ],
      "metadata": {
        "id": "T8RQ_mJ6xrA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = 50\n",
        "batch_size = 1000\n",
        "patience=5\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "history = emb_1d_cnn.fit(embedded_train_x,train_y,validation_data=(embedded_val_x,val_y),epochs=epoch_count,batch_size = batch_size,callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "VwZe5mnnoU3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize the training process**\n",
        "Call the **plot_history** function to draw the loss and accuracy trend over epochs on both training and validation sets."
      ],
      "metadata": {
        "id": "ZYyz4S1Cxzrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history,metric='accuracy')"
      ],
      "metadata": {
        "id": "V2kjnPnfoZUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Performance evaluation on the test set**\n",
        "The **evaluate** method of the 1D CNN model is used to measure the performance on the test set."
      ],
      "metadata": {
        "id": "W3m1SW4nx6X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = emb_1d_cnn.evaluate(embedded_test_x, test_y, batch_size=batch_size,verbose=0)\n",
        "print('Loss: {:.3f} Accuracy: {:.3f}'.format(results[0],results[1]))"
      ],
      "metadata": {
        "id": "AIFsKlWuoc0F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
