{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_03 - Recurrent neural networks.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfzepzFx_T52"
      },
      "source": [
        "# **Recurrent neural networks tutorial**\n",
        "In today's tutorial we will discover how to develop Recurrent Neural Networks (RNNs) using [**Keras**](https://keras.io/) deep learning library to address time series forecasting problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bzdLdFdCJ_F"
      },
      "source": [
        "# **Preliminary operations**\n",
        "The following code downloads all the necessary material into the remote machine. At the end of the execution select the **File** tab to verify that everything has been correctly downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4yPUUXvCMXE"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\n",
        "\n",
        "!gzip -d Metro_Interstate_Traffic_Volume.csv.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFeMIDxWCXR1"
      },
      "source": [
        "# **Useful modules import**\n",
        "First of all, it is necessary to import useful modules used during the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-OGESCgCX9V"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from ipywidgets import interact, fixed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa4NIR2oCewV"
      },
      "source": [
        "# **Utility functions**\n",
        "Execute the following code to define some utility functions used in the tutorial:\n",
        "- **plot_history** draws in a graph the loss trend over epochs on both training and validation sets. Moreover, if provided, it draws in the same graph also the trend of the given metric;\n",
        "- **plot_single_time_sequence** plots a selected time sequence with the corresponding actual and predicted values;\n",
        "- **plot_time_series** plots the actual and the predicted time series;\n",
        "- **prepare_multiple_targets** prepares the target sequence to have multiple steps to be used to forecast multiple future time steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztCcZNKuCsn1"
      },
      "source": [
        "def plot_history(history,metric=None):\n",
        "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "  epoch_count=len(history.history['loss'])\n",
        "\n",
        "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],label='train_loss',color='orange')\n",
        "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],label='val_loss',color = line1.get_color(), linestyle = '--')\n",
        "  ax1.set_xlim([1,epoch_count])\n",
        "  ax1.set_ylim([0, max(max(history.history['loss']),max(history.history['val_loss']))])\n",
        "  ax1.set_ylabel('loss',color = line1.get_color())\n",
        "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
        "  ax1.set_xlabel('Epochs')\n",
        "  _=ax1.legend(loc='lower left')\n",
        "\n",
        "  if (metric!=None):\n",
        "    ax2 = ax1.twinx()\n",
        "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],label='train_'+metric)\n",
        "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],label='val_'+metric,color = line2.get_color(), linestyle = '--')\n",
        "    ax2.set_ylim([0, max(max(history.history[metric]),max(history.history['val_'+metric]))])\n",
        "    ax2.set_ylabel(metric,color=line2.get_color())\n",
        "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
        "    _=ax2.legend(loc='upper right')\n",
        "\n",
        "def plot_single_time_sequence(y,y_pred,t_y_pred,timesteps,figsize=(20, 10)):\n",
        "  plt.figure(figsize=figsize)\n",
        "\n",
        "  y_range=range(t_y_pred,t_y_pred+timesteps)\n",
        "  plt.plot(y_range,y[y_range], '.-',label='Time sequence')\n",
        "\n",
        "  if y_pred.ndim==1:\n",
        "    plt.scatter(t_y_pred+timesteps,y[t_y_pred+timesteps],marker='x',label='Actual values')\n",
        "    plt.scatter(t_y_pred+timesteps,y_pred[t_y_pred],c='r',marker='o',label='Predicted values')\n",
        "  else:\n",
        "    y_pred_range=range(t_y_pred+timesteps,t_y_pred+timesteps+y_pred.shape[1])\n",
        "    plt.scatter(y_pred_range,y[y_pred_range],marker='x',label='Actual values')\n",
        "    plt.scatter(y_pred_range,y_pred[t_y_pred],c='r',marker='o',label='Predicted values')\n",
        "\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.xlabel('t', fontsize=14)\n",
        "  plt.ylabel('x(t)', fontsize=14)\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "def plot_time_series(y,y_pred,t1_y,t2_y,timesteps,figsize=(20, 10)):\n",
        "  plt.figure(figsize=figsize)\n",
        "\n",
        "  y_range=range(t1_y,t2_y)\n",
        "  plt.plot(y_range,y[y_range], '.-',label='Actual values')\n",
        "  \n",
        "  y_pred_range=range(t1_y+timesteps,t2_y)\n",
        "  plt.plot(y_pred_range,y_pred[[t-timesteps for t in y_pred_range]], '.-r',label='Predicted values')\n",
        "  \n",
        "  plt.grid(True)\n",
        "  \n",
        "  plt.xlabel('t', fontsize=14)\n",
        "  plt.ylabel('x(t)', fontsize=14)\n",
        "\n",
        "  plt.legend(loc='upper left')\n",
        "\n",
        "def prepare_multiple_targets(single_targets,multistep_count):\n",
        "  single_target_row_count=single_targets.shape[0]\n",
        "  \n",
        "  multistep_targets=[]\n",
        "  for i in range(single_target_row_count):\n",
        "    target=np.zeros(multistep_count)\n",
        "    for j in range(multistep_count):\n",
        "      idx=i+j\n",
        "      if idx<single_target_row_count:\n",
        "        target[j]=single_targets[idx]\n",
        "    multistep_targets.append(target)  \n",
        "\n",
        "  return np.array(multistep_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABNeosJYDPNl"
      },
      "source": [
        "# **Dataset**\n",
        "This tutorial uses the [Metro Interstate Traffic Volume Data Set](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume) manteined by the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), a public repository containing hundreds of databases useful for the machine learning community.\n",
        "\n",
        "The data set includes hourly Minneapolis-St Paul traffic volume for westbound I-94 from 2012-2018. It contains 48204 istances with 9 attributes:\n",
        "- *holiday*: categorical US national holidays plus regional holiday;\n",
        "- *temp*: Numeric average temperature in kelvin;\n",
        "- *rain_1h*: numeric amount in mm of rain that occurred in the hour;\n",
        "- *snow_1h*: numeric amount in mm of snow that occurred in the hour;\n",
        "- *clouds_all*: numeric percentage of cloud cover;\n",
        "- *weather_main*: categorical short textual description of the current weather;\n",
        "- *weather_description*: Categorical longer textual description of the current weather;\n",
        "- *date_time*: date and hour of the data collected in local CST time;\n",
        "- *traffic_volume*: Numeric hourly I-94 ATR 301 reported westbound traffic volume.\n",
        "\n",
        "The dataset is stored in a CSV file and can be easily loaded in memory using [**pandas**](https://pandas.pydata.org/), a software library for data manipulation and analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXktaHeSEHsV"
      },
      "source": [
        "dataframe = pd.read_csv('Metro_Interstate_Traffic_Volume.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xa_P0pIEO8c"
      },
      "source": [
        "The variable *dataframe* is an instance of the pandas class [**DataFrame**](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html), a 2-dimensional labeled data structure with columns of potentially different types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOtoOOQ_FAuU"
      },
      "source": [
        "## **Visualization**\n",
        "The first *row_count* selected rows can be shown by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLzaDz6YFDfV"
      },
      "source": [
        "row_count=5\n",
        "\n",
        "dataframe.head(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkx8UXrXGnlN"
      },
      "source": [
        "## **Statistics**\n",
        "The [**info**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) method can be used to print a brief summary of a **DataFrame** including the index and the type of each column, the non-null values and the memory usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6NJI4WDGrD8"
      },
      "source": [
        "dataframe.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCo5CP9fGzZs"
      },
      "source": [
        "Note that, there are no missing values in the dataset.\n",
        "\n",
        "To show the overall statistics of the dataset can be used the method [**describe**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uGXoKx4HcEU"
      },
      "source": [
        "dataframe.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRvRve2kHoQl"
      },
      "source": [
        "From the statistics it is clear how each feature covers a very different range.\n",
        "\n",
        "The method [**hist**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html) draws a histogram for each column in the **DataFrame**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5vTr2pKHv9k"
      },
      "source": [
        "dataframe.hist(bins=50, figsize=(20,15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJizdJwEJlno"
      },
      "source": [
        "## **Data preparation**\n",
        "Most machine learning algorithms require data to be formatted in a specific way, so datasets generally require some amount of preparation before they can yield useful insights. Some datasets have values that are missing, invalid, or otherwise difficult for an algorithm to process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-r5sQDBxh-U"
      },
      "source": [
        "### **Encode cyclical data**\n",
        "Traffic volume is strongly related to the time of the day (e.g., 9 A.M. or 10 P.M.), the day of the week (e.g., Monday or Saturday) and the time of the year (e.g., January or August).\n",
        "\n",
        "The following code extracts the hours from the *date_time* column and plots them in a graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEDyu-6C2UJH"
      },
      "source": [
        "hour=dataframe['date_time'].astype('datetime64').dt.hour\n",
        "hour[:160].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lgGQgfl3nqr"
      },
      "source": [
        "The graph report the hourly data for a week: a cycle between 0 and 23 that repeats 7 times presenting a **jump discontinuity** at the end of each day, when the hour value goes from  23  to  00.\n",
        "\n",
        "Presenting cyclical data to a machine learning algorithm is a problem. For instance, it would consider the difference between 23 and 00 greater than that between 22 and 23.\n",
        "\n",
        "A common method for encoding cyclical data is to transform the data into two dimensions using a sine and cosine transformation.\n",
        "\n",
        "The hour sine and cosine values are computed and plotted by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn9OV-Uv756C"
      },
      "source": [
        "hour_sin = np.sin(2 * np.pi * hour/23.0)\n",
        "hour_cos = np.cos(2 * np.pi * hour/23.0)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.xlabel('hour_sin')\n",
        "plt.ylabel('hour_cos')\n",
        "plt.scatter(hour_sin,hour_cos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJP--dl1-jCR"
      },
      "source": [
        "As expected, the hour information are encoded as a cycle.\n",
        "\n",
        "The two new features (*hour_sin* and *hour_cos*) can be inserted in the **DataFrame** using the [**insert**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.insert.html) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoEW-LJJ0GhL"
      },
      "source": [
        "prepared_dataframe=dataframe.copy()\n",
        "\n",
        "prepared_dataframe.insert(len(prepared_dataframe.columns)-1,'hour_sin', hour_sin)\n",
        "prepared_dataframe.insert(len(prepared_dataframe.columns)-1,'hour_cos', hour_cos)\n",
        "\n",
        "prepared_dataframe.head(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftn13hknYsCd"
      },
      "source": [
        "The same thing can be done with the day of the week and the month by executing the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kO-mSjEZSjc"
      },
      "source": [
        "day_week=dataframe['date_time'].astype('datetime64').dt.dayofweek\n",
        "day_week_sin = np.sin(2 * np.pi * day_week/7.0)\n",
        "day_week_cos = np.cos(2 * np.pi * day_week/7.0)\n",
        "\n",
        "month=dataframe['date_time'].astype('datetime64').dt.month\n",
        "month_sin = np.sin(2 * np.pi * month/12.0)\n",
        "month_cos = np.cos(2 * np.pi * month/12.0)\n",
        "\n",
        "prepared_dataframe.insert(len(prepared_dataframe.columns)-1,'day_week_sin', day_week_sin)\n",
        "prepared_dataframe.insert(len(prepared_dataframe.columns)-1,'day_week_cos', day_week_cos)\n",
        "\n",
        "prepared_dataframe.insert(len(prepared_dataframe.columns)-1,'month_sin', month_sin)\n",
        "prepared_dataframe.insert(len(prepared_dataframe.columns)-1,'month_cos', month_cos)\n",
        "\n",
        "prepared_dataframe.head(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1iJLk9-ICn8"
      },
      "source": [
        "### **Remove unuseful columns**\n",
        "The *date_time* column contains no useful information. It can be removed from the dataset using the [**drop**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6It-1iLaINpE"
      },
      "source": [
        "prepared_dataframe=prepared_dataframe.drop(['date_time'],axis=1)\n",
        "prepared_dataframe.head(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onZy4hcyIW9s"
      },
      "source": [
        "### **Convert categorical data**\n",
        "The *holiday*, *weather_main* and *weather_description* columns are categorical, not numeric. Their conversion into numeric format can be done in two ways: \n",
        "- *label encoding*, converting each category to a number;\n",
        "- *one hot encoding*, converting each category value into a new column and assigns a 1 or 0 (True/False) value to the column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf2mSFXxIm_V"
      },
      "source": [
        "**Label encoding**\n",
        "\n",
        "First of all, if the column type is *object* and not *category*, the [**astype**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html) method can be used to convert a column to a category.\n",
        "\n",
        "Then the encoded values can be assigned to the corresponding column using the cat.codes accessor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTKq2lyBIv50"
      },
      "source": [
        "label_enc_dataframe=prepared_dataframe.copy()\n",
        "\n",
        "label_enc_dataframe['holiday'] = prepared_dataframe['holiday'].astype('category')\n",
        "label_enc_dataframe['weather_main'] = prepared_dataframe['weather_main'].astype('category')\n",
        "label_enc_dataframe['weather_description'] = prepared_dataframe['weather_description'].astype('category')\n",
        "\n",
        "label_enc_dataframe['holiday'] = label_enc_dataframe['holiday'].cat.codes\n",
        "label_enc_dataframe['weather_main'] = label_enc_dataframe['weather_main'].cat.codes\n",
        "label_enc_dataframe['weather_description'] = label_enc_dataframe['weather_description'].cat.codes\n",
        "label_enc_dataframe.head(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDIm5GXWJIms"
      },
      "source": [
        "Label encoding has the advantage that it is straightforward but it has the disadvantage that the numeric values can be “misinterpreted” by the algorithms. For example, the value of 0 is obviously less than the value of 4 but does that really correspond to reality (e.g., *weather_description*)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuGsOCYtJPRr"
      },
      "source": [
        "**One hot encoding**\n",
        "\n",
        "Pandas supports this feature using the [**get_dummies**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh0usHvEJWoT"
      },
      "source": [
        "one_hot_enc_dataframe=pd.get_dummies(prepared_dataframe, columns=['holiday', 'weather_main','weather_description'], prefix=['holiday', 'weather_main','weather_description'])\n",
        "one_hot_enc_dataframe.head(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB8qWyl6Jb4j"
      },
      "source": [
        "One hot encoding has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set (it depends by the number of categories in a column)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgSDL-0IJgos"
      },
      "source": [
        "**What is the best solution?**\n",
        "\n",
        "It depends on the specific dataset used.\n",
        "\n",
        "In this tutorial, because *holiday*, *weather_main* and *weather_description* columns contain several categories (12, 11 and 38, respectively), it is better to use the *label encoding* solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPA1jPkzJm0c"
      },
      "source": [
        "used_dataframe=label_enc_dataframe\n",
        "#used_dataframe=one_hot_enc_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2ir71XFLKhF"
      },
      "source": [
        "## **Split data into training, validation and test sets**\n",
        "In order to avoid overfitting during training and to evaluate the generalization capabilites of the models, it is necessary to divide the data into three disjoined datasets: training, validation and test sets.\n",
        "\n",
        "The *train_size_perc* and *val_size_perc* parameters represent the percentage of patterns to include in the training and validation sets, respectively. The remaining patterns will be used to create the test set.\n",
        "\n",
        "<u>Because it is necessary to maintain the data ordered, no mixing routine is used.</u>\n",
        "\n",
        "The Numpy representation of the **DataFrame** can be obtained using the [**values**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html) property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVliApKPLLOT"
      },
      "source": [
        "train_size_perc=0.5\n",
        "val_size_perc=0.25\n",
        "\n",
        "train_size=int(train_size_perc*len(used_dataframe))\n",
        "val_size=int(val_size_perc*len(used_dataframe))\n",
        "\n",
        "train_data=used_dataframe.values[:train_size, :]\n",
        "val_data=used_dataframe.values[train_size:(train_size+val_size), :]\n",
        "test_data=used_dataframe.values[(train_size+val_size):len(used_dataframe), :]\n",
        "\n",
        "print('Train data shape: ',train_data.shape)\n",
        "print('Validation data shape: ',val_data.shape)\n",
        "print('Test data shape: ',test_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MIOUSTsOU6T"
      },
      "source": [
        "## **Data normalization**\n",
        "It is good practice to normalize features that use different scales and ranges.\n",
        "\n",
        "Scikit-learn library provides the class [**StandardScaler**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to normalize features by removing the mean and scaling to unit variance.\n",
        "\n",
        "<u>Note that, the target values (the *traffic_volume* column) are copied before the normalization to use them unscaled during the performance evaluation.</u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pLvP2rvOj2D"
      },
      "source": [
        "train_y=np.copy(train_data[:,-1])\n",
        "val_y=np.copy(val_data[:,-1])\n",
        "test_y=np.copy(test_data[:,-1])\n",
        "\n",
        "scaler_x = StandardScaler()\n",
        "scaler_x.fit(train_data[:,:-1])\n",
        "train_data[:,:-1]=scaler_x.transform(train_data[:,:-1])\n",
        "val_data[:,:-1]=scaler_x.transform(val_data[:,:-1])\n",
        "test_data[:,:-1]=scaler_x.transform(test_data[:,:-1])\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "scaler_y.fit(train_data[:,-1].reshape(-1,1))\n",
        "train_data[:,-1]=scaler_y.transform(train_data[:,-1].reshape(-1,1)).squeeze()\n",
        "val_data[:,-1]=scaler_y.transform(val_data[:,-1].reshape(-1,1)).squeeze()\n",
        "test_data[:,-1]=scaler_y.transform(test_data[:,-1].reshape(-1,1)).squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srsirTKFQrmT"
      },
      "source": [
        "## **Time series data preparation**\n",
        "Time series data must be transformed into a structure of samples with input and output components before it can be used to fit a supervised learning model.\n",
        "\n",
        "Keras provides the [**TimeseriesGenerator**](https://keras.io/api/preprocessing/timeseries/#timeseriesgenerator-class) class to automatically transform time series data into samples, ready to train deep learning models given:\n",
        "- the input features (*data*);\n",
        "- the actual outputs (*targets*);\n",
        "- the length (number of timesteps, observations or rows) of the generated sequences (*length*);\n",
        "- the number of time sequence samples in each batch (*batch_size*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEPchc3SQtP6"
      },
      "source": [
        "timesteps=12\n",
        "batch_size=256\n",
        "\n",
        "train_data_gen = TimeseriesGenerator(data=train_data, targets=train_data[:,-1],\n",
        "                                     length=timesteps, batch_size=batch_size)\n",
        "val_data_gen = TimeseriesGenerator(data=val_data, targets=val_data[:,-1],\n",
        "                                  length=timesteps, batch_size=batch_size)\n",
        "test_data_gen= TimeseriesGenerator(data=test_data, targets=test_data[:,-1],\n",
        "                                    length=timesteps, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXliIGgTXzIW"
      },
      "source": [
        "The batch number of a **TimeseriesGenerator** instance can be obtained using the **len** function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-AvfhfbX5tm"
      },
      "source": [
        "print('Number of train batches: ',len(train_data_gen))\n",
        "print('Number of validation batches: ',len(val_data_gen))\n",
        "print('Number of test batches: ',len(test_data_gen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9DSw6OtYx3F"
      },
      "source": [
        "Batches of a **TimeseriesGenerator** instance can be accessed by referring to its index number. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCPZyeSlYTNB"
      },
      "source": [
        "x,y = train_data_gen[0]\n",
        "\n",
        "print('Time sequences feature batch shape: ',x.shape)\n",
        "print('Time sequences target batch shape: ',y.shape)\n",
        "\n",
        "print('First feature time sequence:\\n',x[0])\n",
        "print('First target value: ',y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr25_6WjTWPj"
      },
      "source": [
        "Once a **TimeseriesGenerator** instance has been defined, it can be used to train a neural network model as a data generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLkgQA6f-iCW"
      },
      "source": [
        "# **Simple RNN**\n",
        "In this section a simple *many-to-one* RNN will be created.\n",
        "\n",
        "![alt text](https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/RNN/ManyToOne_RNN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sMXY6w7_4Tm"
      },
      "source": [
        "## **Model definition**\n",
        "The following function creates a simple RNN model given:\n",
        "- the number of timesteps in each input sequence (*timesteps*);\n",
        "- the number of features in each timestep (*feature_count*).\n",
        "\n",
        "The model returns a single target value given an entire sequence as input (*many-to-one*).\n",
        "\n",
        "<u>Note that, the number of timesteps in each input sequence (*timesteps*) is set in advance only because it improves performance during training by creating tensors of fixed shapes. A *None* value can be used to admit variable-length input sequences.</u>\n",
        "\n",
        "In Keras, a sequential is a stack of layers where each layer has exactly one input and one output. It can be created by passing a list of layers to the  constructor [**keras.Sequential**](https://keras.io/guides/sequential_model/).\n",
        "\n",
        "[**Keras layers API**](https://keras.io/api/layers/) offers a wide range of built-in layers ready for use, including:\n",
        "- [**Input**](https://keras.io/api/layers/core_layers/input/) - the input of the model. Note that, you can also omit the **Input** layer. In that case the model doesn't have any weights until the first call to a training/evaluation method (since it is not yet built);\n",
        "- [**SimpleRNN**](https://keras.io/api/layers/recurrent_layers/simple_rnn/) - a fully-connected RNN where the output is to be fed back to input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osqQ0r4BAGYL"
      },
      "source": [
        "def build_simple_rnn(timesteps,feature_count):\n",
        "  model = keras.Sequential(\n",
        "      [\n",
        "        layers.Input(shape=(timesteps,feature_count)),\n",
        "        layers.SimpleRNN(1)\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTY6dvpNBc8L"
      },
      "source": [
        "## **Model creation**\n",
        "The following code creates a simple RNN model by calling the **build_simple_rnn** function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka6slUHoB-TZ"
      },
      "source": [
        "simple_rnn=build_simple_rnn(timesteps,train_data.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgYcQlfmCFKj"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phr91IgSCIRb"
      },
      "source": [
        "simple_rnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sZ4AVfKCMcz"
      },
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syXceChnCNIb"
      },
      "source": [
        "keras.utils.plot_model(simple_rnn,show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hpnaz9JCTrK"
      },
      "source": [
        "## **Model compilation**\n",
        "The compilation is the final step in configuring the model for training. \n",
        "\n",
        "The following code use the [**compile**](https://keras.io/api/models/model_training_apis/#compile-method) method to compile the model.\n",
        "The important arguments are:\n",
        "- the optimization algorithm (*optimizer*);\n",
        "- the loss function (*loss*);\n",
        "- the metrics used to evaluate the performance of the model (*metrics*).\n",
        "\n",
        "The most common [optimization algorithms](https://keras.io/api/optimizers/#available-optimizers), [loss functions](https://keras.io/api/losses/#available-losses) and [metrics](https://keras.io/api/metrics/#available-metrics) are already available in Keras. You can either pass them to **compile** as an instance or by the corresponding string identifier. In the latter case, the default parameters will be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABflOdvcCac7"
      },
      "source": [
        "simple_rnn.compile(loss='mse', optimizer='adam',metrics=[keras.metrics.RootMeanSquaredError(name='rmse')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSsLfo5aCrMD"
      },
      "source": [
        "## **Training**\n",
        "Now we are ready to train our model by calling the [**fit**](https://keras.io/api/models/model_training_apis/#fit-method) method.\n",
        "\n",
        "It trains the model for a fixed number of epochs (*epoch_count*) using the training and the validation generators (*train_data_gen* and *val_data_gen*).\n",
        "\n",
        "Break training when a metric or the loss has stopped improving on the validation set, helps to avoid overfitting.\n",
        "\n",
        "For this purpose, Keras provides a class called [**EarlyStopping**](https://keras.io/api/callbacks/early_stopping/). Important class parameters are:\n",
        "- *monitor* - the name of the metric or the loss to be observed; \n",
        "- *patience* - the number of epochs with no improvement after which training will be stopped;\n",
        "- *restore_best_weights* - whether to restore model weights from the epoch with the best value of the monitored quantity.\n",
        "\n",
        "Once created an instance of the **EarlyStopping** class, it can be passed to the **fit** method in the *callbacks* parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojKsnskMCt2b"
      },
      "source": [
        "epoch_count = 100\n",
        "patience=5\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "history = simple_rnn.fit(train_data_gen,validation_data=val_data_gen,epochs=epoch_count,callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxk15qBVYS_"
      },
      "source": [
        "### **Visualize the training process**\n",
        "We can learn a lot about our model by observing the graph of its performance over time during training.\n",
        "\n",
        "The **fit** method returns an object (*history*) containing loss and metrics values at successive epochs for both training and validation sets.\n",
        "\n",
        "The following code calls the **plot_history** function defined above to draw in a graph the loss and RMSE trend over epochs on both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upWB9MPBVfE0"
      },
      "source": [
        "plot_history(history,'rmse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MonPhp6XAyW"
      },
      "source": [
        "## **Performance evaluation**\n",
        "The following code calls the [**predict**](https://keras.io/api/models/model_training_apis/#predict-method) method to generate the predictions of the training, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq8FPsKIXFsi"
      },
      "source": [
        "scaled_train_y_pred=simple_rnn.predict(train_data_gen)\n",
        "scaled_val_y_pred=simple_rnn.predict(val_data_gen)\n",
        "scaled_test_y_pred=simple_rnn.predict(test_data_gen)\n",
        "\n",
        "print('Train predictions shape: ',scaled_train_y_pred.shape)\n",
        "print('Validation predictions shape: ',scaled_val_y_pred.shape)\n",
        "print('Test predictions shape: ',scaled_test_y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1DqkwVJfTwN"
      },
      "source": [
        "The returned predictions are scaled because the target values have been normalized before the training process. To obtain the unscaled predictions, it is sufficient to call the [**inverse_transform**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.inverse_transform) method of the **StandardScaler** instance with the scaled predictions as input.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X8fCmGZfUnN"
      },
      "source": [
        "train_y_pred=scaler_y.inverse_transform(scaled_train_y_pred)\n",
        "val_y_pred=scaler_y.inverse_transform(scaled_val_y_pred)\n",
        "test_y_pred=scaler_y.inverse_transform(scaled_test_y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwsOaLpQXvFE"
      },
      "source": [
        "### **RMSE**\n",
        "The regression accuracy can be measured using the RMSE.\n",
        "\n",
        "Scikit-learn library provides the function [**mean_squared_error**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) to compute MSE and RMSE metrics.\n",
        "\n",
        "If the *squared* parameter is set to False, the function returns the RMSE value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZNWbFMFXwpy"
      },
      "source": [
        "rmse_train = mean_squared_error(train_y[timesteps:],train_y_pred,squared=False)\n",
        "rmse_val = mean_squared_error(val_y[timesteps:],val_y_pred,squared=False)\n",
        "rmse_test = mean_squared_error(test_y[timesteps:],test_y_pred,squared=False)\n",
        "\n",
        "print('RMSE')\n",
        "print('Train: {:.2f}'.format(rmse_train))\n",
        "print('Validation: {:.2f}'.format(rmse_val))\n",
        "print('Test: {:.2f}'.format(rmse_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p89k4n471pJ_"
      },
      "source": [
        "### **Best and worst predictions**\n",
        "To visualize test predictions sorted by RMSE, its value needs to be computed for each test sequence and then sorted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFd2FVWo2dQi"
      },
      "source": [
        "rmse_test_sequences=np.sqrt(mean_squared_error(np.expand_dims(test_y[timesteps:],axis=1).transpose(),\n",
        "                                                test_y_pred.transpose(),multioutput='raw_values'))\n",
        "rmse_test_sequences_sorted_indices=np.argsort(rmse_test_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsXPpulk30dN"
      },
      "source": [
        "The following code shows the test predictions sorted by RMSE. The specific prediction can be selected by moving the slider from the best (left) to the worst (right) one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26fuT3jvrRL-"
      },
      "source": [
        "@interact(y=fixed(test_y),\n",
        "         y_pred=fixed(test_y_pred),\n",
        "         rmse_values=fixed(rmse_test_sequences),\n",
        "         rmse_sorted_indices=fixed(rmse_test_sequences_sorted_indices),\n",
        "         selected_index=widgets.IntSlider(min=0, max=rmse_test_sequences_sorted_indices.shape[0]-1, step=1, value=0,continuous_update=False))\n",
        "def interactive_plot(y,y_pred,rmse_values,rmse_sorted_indices,selected_index):\n",
        "  print('RMSE: {:.2f}'.format(rmse_values[rmse_sorted_indices[selected_index]]))\n",
        "  plot_single_time_sequence(y,y_pred,rmse_sorted_indices[selected_index],timesteps,(10,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwcyGN4GX2dU"
      },
      "source": [
        "### **Plot time sequence predictions**\n",
        "To better evaluate the model performance on the test set, it is useful to plot the actual and the predicted sequences.\n",
        "\n",
        "The following code plots a portion or the entire time series with the corresponding forecast given the initial (*t1*) and the final (*t2*) times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDoyTP-bX3Qt"
      },
      "source": [
        "t1=0    #0 to visualize the entire time series\n",
        "t2=200  #test_y.shape[0] to visualize the entire time series\n",
        "\n",
        "plot_time_series(test_y,test_y_pred,t1,t2,timesteps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZrZ2a0iwTU5"
      },
      "source": [
        "# **Deep RNN**\n",
        "In this section a deep RNN will be created.\n",
        "\n",
        "![alt text](https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/RNN/DeepRNN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZh4Uc9fwv9_"
      },
      "source": [
        "## **Model definition**\n",
        "The following function creates a deep DNN model given:\n",
        "- the number of timesteps in each input sequence (*timesteps*);\n",
        "- the number of features in each timestep (*feature_count*);\n",
        "- the number of units for each RNN layer (*unit_count_per_rnn_layer*).\n",
        "\n",
        "The model returns a single target value given an entire sequence as input (*many-to-one*).\n",
        "\n",
        "<u>Note that, the number of timesteps in each input sequence (*timesteps*) is set in advance only because it improves performance during training by creating tensors of fixed shapes. A *None* value can be used to admit variable-length input sequences.</u>\n",
        "\n",
        "The *return_sequences* parameter of the **SimpleRNN** layer serves to return the full output time sequence (True), or only the last output (False)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOX-kfOvxLnL"
      },
      "source": [
        "def build_deep_rnn(timesteps,feature_count,unit_count_per_rnn_layer=[128,128]):\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Input(shape=(timesteps,feature_count)))\n",
        "\n",
        "  for i in range(len(unit_count_per_rnn_layer)):\n",
        "    model.add(layers.SimpleRNN(unit_count_per_rnn_layer[i],return_sequences=True if i<(len(unit_count_per_rnn_layer)-1) else False))\n",
        "\n",
        "  if unit_count_per_rnn_layer[-1]>1:\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpFRCzLqzjsU"
      },
      "source": [
        "## **Model creation**\n",
        "The following code creates a deep RNN model by calling the **build_deep_rnn** function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OL1ZjF1zo2K"
      },
      "source": [
        "deep_rnn=build_deep_rnn(timesteps,train_data.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZqi3CAPz0xs"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ6P2up6z5uN"
      },
      "source": [
        "deep_rnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3avvVti0E43"
      },
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLGXRRUg0IU1"
      },
      "source": [
        "keras.utils.plot_model(deep_rnn,show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrDBEHUw0nzm"
      },
      "source": [
        "## **Model compilation**\n",
        "The following code compiles the model as already done before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC-lQIcz0xL7"
      },
      "source": [
        "deep_rnn.compile(loss='mse', optimizer='adam',metrics=[keras.metrics.RootMeanSquaredError(name='rmse')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B0vRjo-03fm"
      },
      "source": [
        "## **Training**\n",
        "Now we are ready to train our model by calling the **fit** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKBuPPii05om"
      },
      "source": [
        "epoch_count = 100\n",
        "patience=5\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "history = deep_rnn.fit(train_data_gen,validation_data=val_data_gen,epochs=epoch_count,callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFjmY-GX1C4y"
      },
      "source": [
        "### **Visualize the training process**\n",
        "The following code calls the **plot_history** function defined above to draw in a graph the loss and RMSE trend over epochs on both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l12V7_T-1IpM"
      },
      "source": [
        "plot_history(history,'rmse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM5mX-Yh1YP2"
      },
      "source": [
        "## **Performance evaluation**\n",
        "The following code calls the **predict** method to generate the predictions of the training, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7GK-WGU1dP0"
      },
      "source": [
        "scaled_train_y_pred=deep_rnn.predict(train_data_gen)\n",
        "scaled_val_y_pred=deep_rnn.predict(val_data_gen)\n",
        "scaled_test_y_pred=deep_rnn.predict(test_data_gen)\n",
        "\n",
        "print('Train predictions shape: ',scaled_train_y_pred.shape)\n",
        "print('Validation predictions shape: ',scaled_val_y_pred.shape)\n",
        "print('Test predictions shape: ',scaled_test_y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UfV400W1mRH"
      },
      "source": [
        "The returned predictions are scaled because the target values have been normalized before the training process. To obtain the unscaled predictions, it is sufficient to call the **inverse_transform** method of the **StandardScaler** instance with the scaled predictions as input. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_hol_Uq1qM1"
      },
      "source": [
        "train_y_pred=scaler_y.inverse_transform(scaled_train_y_pred)\n",
        "val_y_pred=scaler_y.inverse_transform(scaled_val_y_pred)\n",
        "test_y_pred=scaler_y.inverse_transform(scaled_test_y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkiEp3x31wGG"
      },
      "source": [
        "### **RMSE**\n",
        "The regression accuracy can be measured using the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho2_D_O91wNP"
      },
      "source": [
        "rmse_train = mean_squared_error(train_y[timesteps:],train_y_pred,squared=False)\n",
        "rmse_val = mean_squared_error(val_y[timesteps:],val_y_pred,squared=False)\n",
        "rmse_test = mean_squared_error(test_y[timesteps:],test_y_pred,squared=False)\n",
        "\n",
        "print('RMSE')\n",
        "print('Train: {:.2f}'.format(rmse_train))\n",
        "print('Validation: {:.2f}'.format(rmse_val))\n",
        "print('Test: {:.2f}'.format(rmse_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFhIpDPG-7Qa"
      },
      "source": [
        "### **Best and worst predictions**\n",
        "To visualize test predictions sorted by RMSE, its value needs to be computed for each test sequence and then sorted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IXx9CMs-744"
      },
      "source": [
        "rmse_test_sequences=np.sqrt(mean_squared_error(np.expand_dims(test_y[timesteps:],axis=1).transpose(),\n",
        "                                                test_y_pred.transpose(),multioutput='raw_values'))\n",
        "rmse_test_sequences_sorted_indices=np.argsort(rmse_test_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKetcDLK_DbX"
      },
      "source": [
        "The following code shows the test predictions sorted by RMSE. The specific prediction can be selected by moving the slider from the best (left) to the worst (right) one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlWn3lIuuFZo"
      },
      "source": [
        "@interact(y=fixed(test_y),\n",
        "         y_pred=fixed(test_y_pred),\n",
        "         rmse_values=fixed(rmse_test_sequences),\n",
        "         rmse_sorted_indices=fixed(rmse_test_sequences_sorted_indices),\n",
        "         selected_index=widgets.IntSlider(min=0, max=rmse_test_sequences_sorted_indices.shape[0]-1, step=1, value=0,continuous_update=False))\n",
        "def interactive_plot(y,y_pred,rmse_values,rmse_sorted_indices,selected_index):\n",
        "  print('RMSE: {:.2f}'.format(rmse_values[rmse_sorted_indices[selected_index]]))\n",
        "  plot_single_time_sequence(y,y_pred,rmse_sorted_indices[selected_index],timesteps,(10,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3-ZyxMX16jW"
      },
      "source": [
        "### **Plot time sequence predictions**\n",
        "The following code plots a portion or the entire time series with the corresponding forecast given the initial (*t1*) and the final (*t2*) times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q45Ww4J42AQl"
      },
      "source": [
        "t1=0    #0 to visualize the entire time series\n",
        "t2=200  #test_y.shape[0] to visualize the entire time series\n",
        "\n",
        "plot_time_series(test_y,test_y_pred,t1,t2,timesteps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aOPZMZe89hO"
      },
      "source": [
        "# **Exercise 1**\n",
        "Improve the performance of the deep RNN model. It is recommended to evaluate the following hyperparameters (listed in priority order):\n",
        "1. the number of units for each RNN layer (*unit_count_per_rnn_layer*);\n",
        "2. the number of timesteps in each input sequence (*timesteps*);\n",
        "3. the mini-batch size (*batch_size*);\n",
        "4. the number of training epochs (*epoch_count*).\n",
        "\n",
        "Note that, points 2 and 3 require to recreate the **TimeseriesGenerator** instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUBBSR3qxrCJ"
      },
      "source": [
        "# **Multi-step Forecasts**\n",
        "Neural networks can learn to map an input pattern to an output pattern of more than one feature. This can be used in time series forecasting to directly forecast multiple future time steps.\n",
        "\n",
        "This can be achieved either by directly outputting a vector from the model, by specifying the desired number of outputs as the number of nodes in the output layer.\n",
        "\n",
        "A limitation of the **TimeseriesGenerator** class is that it does not directly support multi-step outputs. Specifically, it will not create the multiple steps that may be required in the target sequence.\n",
        "\n",
        "Nevertheless, if we prepare the target sequence to have multiple steps, the **TimeseriesGenerator** class will use them as the output portion of each sample.\n",
        "\n",
        "The following code prepare the training, validation and test target sequences to have *multistep_count* steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqHfPxsBATuQ"
      },
      "source": [
        "multistep_count=24\n",
        "\n",
        "multistep_train_targets=prepare_multiple_targets(train_data[:,-1],multistep_count)\n",
        "multistep_val_targets=prepare_multiple_targets(val_data[:,-1],multistep_count)\n",
        "multistep_test_targets=prepare_multiple_targets(test_data[:,-1],multistep_count)\n",
        "\n",
        "print('Multistep train targets shape: ',multistep_train_targets.shape)\n",
        "print('Multistep validation targets shape: ',multistep_val_targets.shape)\n",
        "print('Multistep test targets shape: ',multistep_test_targets.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-XwhhTGzAT6"
      },
      "source": [
        "## **Time series data preparation**\n",
        "Now the time series data can be transformed into a structure of samples with input and output components using the **TimeseriesGenerator** class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyvJeLB9zUjH"
      },
      "source": [
        "timesteps=12\n",
        "batch_size=256\n",
        "\n",
        "multistep_train_data_gen = TimeseriesGenerator(data=train_data[:-multistep_count+1,:],\n",
        "                                               targets=multistep_train_targets[:-multistep_count+1],\n",
        "                                               length=timesteps, batch_size=batch_size)\n",
        "multistep_val_data_gen = TimeseriesGenerator(data=val_data[:-multistep_count+1,:],\n",
        "                                             targets=multistep_val_targets[:-multistep_count+1],\n",
        "                                             length=timesteps, batch_size=batch_size)\n",
        "multistep_test_data_gen= TimeseriesGenerator(data=test_data[:-multistep_count+1,:],\n",
        "                                             targets=multistep_test_targets[:-multistep_count+1],\n",
        "                                             length=timesteps, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUrx_IoLKPgn"
      },
      "source": [
        "The batch number of a **TimeseriesGenerator** instance can be obtained using the **len** function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoWXuOoWKQKl"
      },
      "source": [
        "print('Number of train batches: ',len(multistep_train_data_gen))\n",
        "print('Number of validation batches: ',len(multistep_val_data_gen))\n",
        "print('Number of test batches: ',len(multistep_test_data_gen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOQIsekzBzvf"
      },
      "source": [
        "Batches of a **TimeseriesGenerator** instance can be accessed by referring to its index number. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnNXQTCwUeVg"
      },
      "source": [
        "x,y = multistep_train_data_gen[0]\n",
        "\n",
        "print('Time sequences feature batch shape: ',x.shape)\n",
        "print('Time sequences target batch shape: ',y.shape)\n",
        "\n",
        "print('First feature time sequence:\\n',x[0])\n",
        "print('First target value:\\n',y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-RemmVWCRcp"
      },
      "source": [
        "## **Model definition**\n",
        "The following function creates a deep DNN model for multi-step forecasts given:\n",
        "- the number of timesteps in each input sequence (*timesteps*);\n",
        "- the number of features in each timestep (*feature_count*);\n",
        "- the number of multi-steps in each target sequence (*target_count*);\n",
        "- the number of units for each RNN layer (*unit_count_per_rnn_layer*).\n",
        "\n",
        "The model returns a multi-step target value given an entire sequence as input.\n",
        "\n",
        "<u>Note that, the number of timesteps in each input sequence (*timesteps*) is set in advance only because it improves performance during training by creating tensors of fixed shapes. A *None* value can be used to admit variable-length input sequences.</u>\n",
        "\n",
        "The *return_sequences* parameter of the **SimpleRNN** layer serves to return the full output time sequence (True), or only the last output (False)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO6_JM-CCR7_"
      },
      "source": [
        "def build_multistep_deep_rnn(timesteps,feature_count,target_count,unit_count_per_rnn_layer=[128,128]):\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Input(shape=(timesteps,feature_count)))\n",
        "\n",
        "  for i in range(len(unit_count_per_rnn_layer)):\n",
        "    model.add(layers.SimpleRNN(unit_count_per_rnn_layer[i],return_sequences=True if i<(len(unit_count_per_rnn_layer)-1) else False))\n",
        "\n",
        "  if unit_count_per_rnn_layer[-1]!=target_count:\n",
        "    model.add(layers.Dense(target_count))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz0X9YhuC6-A"
      },
      "source": [
        "## **Model creation**\n",
        "The following code creates a deep RNN model for multi-step forecasts by calling the **build_multistep_deep_rnn** function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-28EUdeBC7i3"
      },
      "source": [
        "multistep_deep_rnn=build_multistep_deep_rnn(timesteps,train_data.shape[1],multistep_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRnl6ihDDskA"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a-6zWFgDtBw"
      },
      "source": [
        "multistep_deep_rnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVGsDU3gDwL3"
      },
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km-kI5bBDwpu"
      },
      "source": [
        "keras.utils.plot_model(multistep_deep_rnn,show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFiyFbogELr4"
      },
      "source": [
        "## **Model compilation**\n",
        "The following code compiles the model as already done before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME7TxxWyEMYG"
      },
      "source": [
        "multistep_deep_rnn.compile(loss='mse', optimizer='adam',metrics=[keras.metrics.RootMeanSquaredError(name='rmse')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poK0T4WFESDv"
      },
      "source": [
        "## **Training**\n",
        "Now we are ready to train our model by calling the **fit** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO1CaNeEETAH"
      },
      "source": [
        "epoch_count = 100\n",
        "patience=5\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "history = multistep_deep_rnn.fit(multistep_train_data_gen,validation_data=multistep_val_data_gen,epochs=epoch_count,callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8lyiOZUEgsa"
      },
      "source": [
        "### **Visualize the training process**\n",
        "The following code calls the **plot_history** function defined above to draw in a graph the loss and RMSE trend over epochs on both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZGakBEOElcY"
      },
      "source": [
        "plot_history(history,'rmse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdNugLn3EpB_"
      },
      "source": [
        "## **Performance evaluation**\n",
        "The following code calls the **predict** method to generate the predictions of the training, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgdwOCGTEqjo"
      },
      "source": [
        "multistep_scaled_train_y_pred=multistep_deep_rnn.predict(multistep_train_data_gen)\n",
        "multistep_scaled_val_y_pred=multistep_deep_rnn.predict(multistep_val_data_gen)\n",
        "multistep_scaled_test_y_pred=multistep_deep_rnn.predict(multistep_test_data_gen)\n",
        "\n",
        "print('Multi-step train predictions shape: ',multistep_scaled_train_y_pred.shape)\n",
        "print('Multi-step validation predictions shape: ',multistep_scaled_val_y_pred.shape)\n",
        "print('Multi-step test predictions shape: ',multistep_scaled_test_y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1mkmRalFsRh"
      },
      "source": [
        "The returned predictions are scaled because the target values have been normalized before the training process. To obtain the unscaled predictions, it is sufficient to call the **inverse_transform** method of the **StandardScaler** instance with the scaled predictions as input. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QYsrV3sFs2Q"
      },
      "source": [
        "multistep_train_y_pred=scaler_y.inverse_transform(multistep_scaled_train_y_pred)\n",
        "multistep_val_y_pred=scaler_y.inverse_transform(multistep_scaled_val_y_pred)\n",
        "multistep_test_y_pred=scaler_y.inverse_transform(multistep_scaled_test_y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZZV2uiwLBVN"
      },
      "source": [
        "Moreover, it is necessary to prepare the training, validation and test targets to have *multistep_count* steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw5Lp1xvHvpI"
      },
      "source": [
        "multistep_train_y=prepare_multiple_targets(train_y,multistep_count)\n",
        "multistep_val_y=prepare_multiple_targets(val_y,multistep_count)\n",
        "multistep_test_y=prepare_multiple_targets(test_y,multistep_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMHdJJ3_FQTh"
      },
      "source": [
        "### **RMSE**\n",
        "The regression accuracy can be measured using the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhfaGcFcFRIP"
      },
      "source": [
        "multistep_rmse_train = mean_squared_error(multistep_train_y[timesteps:-multistep_count+1],multistep_train_y_pred,squared=False)\n",
        "multistep_rmse_val = mean_squared_error(multistep_val_y[timesteps:-multistep_count+1],multistep_val_y_pred,squared=False)\n",
        "multistep_rmse_test = mean_squared_error(multistep_test_y[timesteps:-multistep_count+1],multistep_test_y_pred,squared=False)\n",
        "\n",
        "print('RMSE')\n",
        "print('Multi-step train: {:.2f}'.format(multistep_rmse_train))\n",
        "print('Multi-step validation: {:.2f}'.format(multistep_rmse_val))\n",
        "print('Multi-step test: {:.2f}'.format(multistep_rmse_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRRIWxkPK-RJ"
      },
      "source": [
        "### **Best and worst predictions**\n",
        "To visualize test predictions sorted by RMSE, its value needs to be computed for each test sequence and then sorted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAjd6Ab1OIS-"
      },
      "source": [
        "rmse_test_sequences=np.sqrt(mean_squared_error(multistep_test_y[timesteps:-multistep_count+1].transpose(),\n",
        "                                                multistep_test_y_pred.transpose(),multioutput='raw_values'))\n",
        "rmse_test_sequences_sorted_indices=np.argsort(rmse_test_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sRBorv6uWhS"
      },
      "source": [
        "The following code shows the test predictions sorted by RMSE. The specific prediction can be selected by moving the slider from the best (left) to the worst (right) one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wYNDER7uW-V"
      },
      "source": [
        "@interact(y=fixed(test_y),\n",
        "         y_pred=fixed(multistep_test_y_pred),\n",
        "         rmse_values=fixed(rmse_test_sequences),\n",
        "         rmse_sorted_indices=fixed(rmse_test_sequences_sorted_indices),\n",
        "         selected_index=widgets.IntSlider(min=0, max=rmse_test_sequences_sorted_indices.shape[0]-1, step=1, value=0,continuous_update=False))\n",
        "def interactive_plot(y,y_pred,rmse_values,rmse_sorted_indices,selected_index):\n",
        "  print('RMSE: {:.2f}'.format(rmse_values[rmse_sorted_indices[selected_index]]))\n",
        "  plot_single_time_sequence(y,y_pred,rmse_sorted_indices[selected_index],timesteps,(10,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmRQi2EvcNcr"
      },
      "source": [
        "# **Exercise 2**\n",
        "Improve the performance of the deep RNN model on the multi-step forecast problem. It is recommended to evaluate the following hyperparameters (listed in priority order):\n",
        "1. the number of units for each RNN layer (*unit_count_per_rnn_layer*);\n",
        "2. the number of timesteps in each input sequence (*timesteps*);\n",
        "3. the mini-batch size (*batch_size*);\n",
        "4. the number of training epochs (*epoch_count*).\n",
        "\n",
        "Note that, points 2 and 3 require to recreate the **TimeseriesGenerator** instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeQIwrK2dAtS"
      },
      "source": [
        "# **Exercise 3**\n",
        "Keras provides specific layers to implement *Long Short-Term Memory* ([**LSTM**](https://keras.io/api/layers/recurrent_layers/lstm/)) and *Gated Recurrent Units* ([**GRU**](https://keras.io/api/layers/recurrent_layers/gru/)) networks.\n",
        "\n",
        "Evaluate the performance of LSTM and GRU networks on both single- and multi-step forecast problem. \n",
        "\n",
        "Functions **build_deep_rnn** and **build_multistep_deep_rnn** defined above can be used as starting point by replacing the **SimpleRNN** layers with **LSTM** or **GRU** layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN8roE7t-8lJ"
      },
      "source": [
        "# **Exercise 4**\n",
        "Solve another time series forecasting problem chosen from the following list:\n",
        "- [Beijing Multi-Site Air-Quality Data Data Set](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data);\n",
        "- [DJIA 30 Stock Time Series](https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231);\n",
        "- [Population Time Series Data](https://www.kaggle.com/census/population-time-series-data);\n",
        "- [House Property Sales Time Series](https://www.kaggle.com/htagholdings/property-sales);\n",
        "- [House Hold Energy Data - Time Series](https://www.kaggle.com/jaganadhg/house-hold-energy-data).\n"
      ]
    }
  ]
}