{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_02 - Convolutional neural networks.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjxAFzKb7hd3"
      },
      "source": [
        "# **Convolutional neural networks tutorial**\n",
        "In today's tutorial we will design and train two well-known Convolutional Neural Networks (*LeNet-5* and *AlexNet*) for image classification.\n",
        "\n",
        "We will use [**TensorFlow**](https://ekababisong.org/gcp-ml-seminar/tensorflow/) framework and [**Keras**](https://keras.io/) open-source library to rapidly prototype Deep Neural Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHYAz67yTKrT"
      },
      "source": [
        "# **Useful modules import**\n",
        "First of all, it is necessary to import useful modules used during the tutorial. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K73cJePNTPKQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhlYqvvzdbgY"
      },
      "source": [
        "# **Utility functions**\n",
        "Execute the following code to define some utility functions used in the tutorial:\n",
        "- **plot_history** draws in a graph the loss trend over epochs on both training and validation sets. Moreover, if provided, it draws in the same graph also the trend of the given metric;\n",
        "- **show_confusion_matrix** visualizes a 2D confusion matrix as a color-coded image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKWxfyS2dki5"
      },
      "source": [
        "def plot_history(history,metric=None):\n",
        "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "  epoch_count=len(history.history['loss'])\n",
        "\n",
        "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],label='train_loss',color='orange')\n",
        "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],label='val_loss',color = line1.get_color(), linestyle = '--')\n",
        "  ax1.set_xlim([1,epoch_count])\n",
        "  ax1.set_ylim([0, max(max(history.history['loss']),max(history.history['val_loss']))])\n",
        "  ax1.set_ylabel('loss',color = line1.get_color())\n",
        "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
        "  ax1.set_xlabel('Epochs')\n",
        "  _=ax1.legend(loc='lower left')\n",
        "\n",
        "  if (metric!=None):\n",
        "    ax2 = ax1.twinx()\n",
        "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],label='train_'+metric)\n",
        "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],label='val_'+metric,color = line2.get_color(), linestyle = '--')\n",
        "    ax2.set_ylim([0, max(max(history.history[metric]),max(history.history['val_'+metric]))])\n",
        "    ax2.set_ylabel(metric,color=line2.get_color())\n",
        "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
        "    _=ax2.legend(loc='upper right')\n",
        "\n",
        "def show_confusion_matrix(conf_matrix,class_names,figsize=(10,10)):\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  img=ax.matshow(conf_matrix)\n",
        "  tick_marks = np.arange(len(class_names))\n",
        "  _=plt.xticks(tick_marks, class_names,rotation=45)\n",
        "  _=plt.yticks(tick_marks, class_names)\n",
        "  _=plt.ylabel('Real')\n",
        "  _=plt.xlabel('Predicted')\n",
        "  \n",
        "  for i in range(len(class_names)):\n",
        "    for j in range(len(class_names)):\n",
        "        text = ax.text(j, i, '{0:.1%}'.format(conf_matrix[i, j]),\n",
        "                       ha='center', va='center', color='w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEsK-TZLTPfZ"
      },
      "source": [
        "# **Datasets**\n",
        "The [**tf.keras.datasets**](https://keras.io/api/datasets/) module contains four simple datasets useful to test CNNs:\n",
        "- [**digits MNIST**](http://yann.lecun.com/exdb/mnist/) - a dataset of 28x28 grayscale images of the 10 digits;\n",
        "- [**fashion MNIST**](https://github.com/zalandoresearch/fashion-mnist) - a dataset of 28x28 grayscale images of 10 fashion categories;\n",
        "- [**CIFAR10**](https://www.cs.toronto.edu/~kriz/cifar.html) - a dataset of 32x32 RGB images labeled over 10 categories;\n",
        "- [**CIFAR100**](https://www.cs.toronto.edu/~kriz/cifar.html) - a dataset of 32x32 RGB images labeled over 100 classes.\n",
        "\n",
        "The following code loads in memory the selected dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXoNm9jNScwd"
      },
      "source": [
        "dataset='mnist' #   'mnist' 'fashion_mnist' 'cifar10'   'cifar100'\n",
        "\n",
        "if dataset=='mnist':\n",
        "    (data_train_x,data_train_y), (data_test_x,data_test_y) = keras.datasets.mnist.load_data()\n",
        "    class_names=range(10)\n",
        "elif dataset=='fashion_mnist':\n",
        "    (data_train_x,data_train_y), (data_test_x,data_test_y) = keras.datasets.fashion_mnist.load_data()\n",
        "    class_names=('T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot')\n",
        "elif dataset=='cifar10':\n",
        "    (data_train_x,data_train_y), (data_test_x,data_test_y) = keras.datasets.cifar10.load_data()\n",
        "    class_names=('airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck')\n",
        "    data_train_y=data_train_y.squeeze()\n",
        "    data_test_y=data_test_y.squeeze()\n",
        "elif dataset=='cifar100':\n",
        "    (data_train_x,data_train_y), (data_test_x,data_test_y) = keras.datasets.cifar100.load_data()\n",
        "    class_names=('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle','bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel','can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock','cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur','dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster','house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion','lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse','mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear','pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine','possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose','sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake','spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table','tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout','tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman','worm')\n",
        "    data_train_y=data_train_y.squeeze()\n",
        "    data_test_y=data_test_y.squeeze()\n",
        "\n",
        "class_count=len(class_names)\n",
        "\n",
        "print('Train image shape: ',data_train_x.shape)\n",
        "print('Train label shape: ',data_train_y.shape)\n",
        "print('Test image shape: ',data_test_x.shape)\n",
        "print('Test label shape: ',data_test_y.shape)\n",
        "print('Number of classes: ',class_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CksvY_X2TeWg"
      },
      "source": [
        "## **Visualization**\n",
        "Some randomly selected images can be shown by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZsw1dA8TfrI"
      },
      "source": [
        "image_count=10\n",
        "\n",
        "_, axs = plt.subplots(1, image_count,figsize=(15, 10))\n",
        "for i in range(image_count):\n",
        "  random_idx=random.randint(0,data_train_x.shape[0])\n",
        "  axs[i].imshow(data_train_x[random_idx],cmap='gray')\n",
        "  axs[i].axis('off')\n",
        "  axs[i].set_title(class_names[data_train_y[random_idx]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPrPi0lX_Ly"
      },
      "source": [
        "## **Split data into training and validation sets**\n",
        "In order to avoid overfitting during training, it is necessary to have a separate dataset (called validation set), in addition to the training and test datasets, to choose the optimal value for the hyperparameters. \n",
        "\n",
        "For this reason, *data_train_x* is divided into two subsets: training and validation sets. \n",
        "\n",
        "Scikit-learn library provides the function [**train_test_split**](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to separate a dataset into two parts.\n",
        "\n",
        "The *val_size* variable represents the percentage (or the absolute number) of patterns to include in the validation set.\n",
        "\n",
        "By default, **train_test_split** mixes patterns in order to avoid that returned datasets contain patterns belonging only to a subset of the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMUcoUEVYBpC"
      },
      "source": [
        "val_size=10000\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(data_train_x, data_train_y, test_size=val_size, random_state=42,shuffle=True)\n",
        "train_x=np.array(train_x)\n",
        "val_x=np.array(val_x)\n",
        "\n",
        "test_x=data_test_x\n",
        "test_y=data_test_y\n",
        "\n",
        "print('Train shape: ',train_x.shape)\n",
        "print('Validation shape: ',val_x.shape)\n",
        "print('Test shape: ',test_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjeWdJF0ZaJi"
      },
      "source": [
        "## **Preprocessing**\n",
        "The acquired data are usually messy and come from different sources. To feed them into a ML model, they need to be standardized and cleaned up. For example, fully connected layers in CNNs require that all images are of the same size.\n",
        "\n",
        "Image preprocessing are the steps taken to format images before they are used by model training and inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SSeFEqts-3w"
      },
      "source": [
        "### **Image shape**\n",
        "In case of grayscale images, it is necessary to add a new unit axis to explicitly represent single channel images.\n",
        "\n",
        "By executing the following code, the shape of the images is updated from WxH to WxHx1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzWy6pbsZbOh"
      },
      "source": [
        "if (len(train_x.shape)==3):\n",
        "  train_x=np.expand_dims(train_x,axis=3)\n",
        "  val_x=np.expand_dims(val_x,axis=3)\n",
        "  test_x=np.expand_dims(test_x,axis=3)\n",
        "  print('Train shape: ',train_x.shape)\n",
        "  print('Validation shape: ',val_x.shape)\n",
        "  print('Test shape: ',test_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFsXNdmtYtTZ"
      },
      "source": [
        "### **Intensity range normalization**\n",
        "Pixel intensity is usually represented as discrete values in the range [0;255]. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fphivGSUY3zZ"
      },
      "source": [
        "print('Min value: ',train_x.min())\n",
        "print('Max value: ',train_x.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOycTZBNY4To"
      },
      "source": [
        "Such values could produce math range errors with the activation function or make training unstable. To overcome these issues, a simple normalization step can be applied by dividing all values by 255 to get continuous values in the range [0;1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJhuCDS3YvsQ"
      },
      "source": [
        "train_x=train_x/255\n",
        "val_x=val_x/255\n",
        "test_x=test_x/255\n",
        "print('Min value: ',train_x.min())\n",
        "print('Max value: ',train_x.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D5Cf6x8ZpZw"
      },
      "source": [
        "### **Spatial size**\n",
        "*LeNet-5* has been originally designed to receive as input a 32x32 image. Although the input shape can be changed during the creation of the model, we prefer to maintain the original input shape by adding a black border."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yQOuvifZrFw"
      },
      "source": [
        "if train_x.shape[1]<32 or train_x.shape[2]<32:\n",
        "  pad_h=int((32-train_x.shape[1])/2)\n",
        "  pad_w=int((32-train_x.shape[2])/2)\n",
        "  train_x=np.pad(train_x,((0,0),(pad_w,pad_w),(pad_h,pad_h),(0,0)),'constant',constant_values=0)\n",
        "  val_x=np.pad(val_x,((0,0),(pad_w,pad_w),(pad_h,pad_h),(0,0)),'constant',constant_values=0)\n",
        "  test_x=np.pad(test_x,((0,0),(pad_w,pad_w),(pad_h,pad_h),(0,0)),'constant',constant_values=0)\n",
        "  print('Train shape: ',train_x.shape)\n",
        "  print('Validation shape: ',val_x.shape)\n",
        "  print('Test shape: ',test_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZQbEtG6aGmn"
      },
      "source": [
        "# **LeNet-5**\n",
        "*LeNet-5* is a CNN introduced to recognize handwritted digits in images.\n",
        "\n",
        "It consists of:\n",
        "- three **convolutional** layers (C1, C3 and C5);\n",
        "- two **average pooling** layers (S2 and S4);\n",
        "- two **fully-connected** layers (F6 and Output).\n",
        "\n",
        "![alt text](https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/CNN/LeNet5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz8waUlMLcQB"
      },
      "source": [
        "## **Model definition**\n",
        "The following function creates an *LeNet-5* model given:\n",
        "- the shape of the input images (*input_shape*);\n",
        "- the number of output classes (*output_class_count*).\n",
        "\n",
        "In Keras, a sequential is a stack of layers where each layer has exactly one input and one output. It can be created by passing a list of layers to the  constructor [**keras.Sequential**](https://keras.io/guides/sequential_model/).\n",
        "\n",
        "[**Keras layers API**](https://keras.io/api/layers/) offers a wide range of built-in layers ready for use, including:\n",
        "- [**Input**](https://keras.io/api/layers/core_layers/input/) - the input of the model. Note that, you can also omit the Input layer. In that case the model doesn't have any weights until the first call to a training/evaluation method (since it is not yet built).\n",
        "- [**Conv2D**](https://keras.io/api/layers/convolution_layers/convolution2d/) - a 2D convolution layer;\n",
        "- [**AvgPool2D**](https://keras.io/api/layers/pooling_layers/average_pooling2d/) - a 2D average pooling layer;\n",
        "- [**Flatten**](https://keras.io/api/layers/reshaping_layers/flatten/) - a simple layer used to flatten the input;\n",
        "- [**Dense**](https://keras.io/api/layers/core_layers/dense/) - a fully-connected layer.\n",
        "\n",
        "Using such layers we are able to define an LeNet-5 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egpjHz5UaIOH"
      },
      "source": [
        "def build_lenet5(input_shape=(32, 32, 1),output_class_count=10):\n",
        "    model=keras.Sequential(\n",
        "            [\n",
        "                layers.Input(shape=input_shape,name='Input'),\n",
        "                layers.Conv2D(filters=6, kernel_size=5, strides=1,activation='tanh',padding='valid',name='C1'),\n",
        "                layers.AvgPool2D(pool_size=2, strides=2,name='S2'),\n",
        "                layers.Conv2D(filters=16, kernel_size=5,strides=1,activation='tanh',padding='valid',name='C3'),\n",
        "                layers.AvgPool2D(pool_size=2, strides=2,name='S4'),\n",
        "                layers.Conv2D(filters=120, kernel_size=5,strides=1,activation='tanh',padding='valid',name='C5'),\n",
        "                layers.Flatten(),\n",
        "                layers.Dense(84, activation='tanh',name='F6'),\n",
        "                layers.Dense(units=output_class_count,activation='softmax',name='Output')\n",
        "            ]\n",
        "        )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmcTDRD9aeDe"
      },
      "source": [
        "## **Model creation**\n",
        "The following code creates an *LeNet-5* model by calling the **build_lenet5** function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHQatB1mafUP"
      },
      "source": [
        "model=build_lenet5(train_x[0].shape,class_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBeH3rKVa4E4"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed using the [**summary**](https://keras.io/api/models/model/#summary-method) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI4V58G1a5N2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99ot5rTma8VW"
      },
      "source": [
        "The summary is useful for simple models, but can be confusing for complex models.\n",
        "\n",
        "Function [**keras.utils.plot_model**](https://keras.io/api/utils/model_plotting_utils/) creates a plot of the neural network graph that can make more complex models easier to understand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK3jBlvl5Bym"
      },
      "source": [
        "keras.utils.plot_model(model,show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JZfUMJR67L4"
      },
      "source": [
        "## **Model compilation**\n",
        "The compilation is the final step in configuring the model for training. Keras model provides a method, [**compile**](https://keras.io/api/models/model_training_apis/#compile-method) to compile the model.\n",
        "The important arguments are:\n",
        "- the optimization algorithm (*optimizer*);\n",
        "- the loss function (*loss*);\n",
        "- the metrics used to evaluate the performance of the model (*metrics*).\n",
        "\n",
        "The most common [optimization algorithms](https://keras.io/api/optimizers/#available-optimizers), [loss functions](https://keras.io/api/losses/#available-losses) and [metrics](https://keras.io/api/metrics/#available-metrics) are already available in Keras. You can either pass them to **compile** as an instance or by the corresponding string identifier. In the latter case, the default parameters will be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bQYu7ct67-2"
      },
      "source": [
        "optimizer=keras.optimizers.SGD()\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMk_N4ho7KEG"
      },
      "source": [
        "## **Training**\n",
        "Now we are ready to train our model by calling the [**fit**](https://keras.io/api/models/model_training_apis/#fit-method) method. It trains the model for a fixed number of epochs (*epoch_count*) using the training set (*train_x* and *train_y*) divided into mini-batches of *batch_size* elements. During the training process, the performances will be evaluated on both training and validation (*validation_x* and *validation_x*) sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWt7VzI27K9W"
      },
      "source": [
        "batch_size=250\n",
        "epoch_count=10\n",
        "\n",
        "history =model.fit(train_x,train_y,batch_size,epoch_count,validation_data=(val_x,val_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDJPxvXG7YaP"
      },
      "source": [
        "### **Visualize the training process**\n",
        "We can learn a lot about our model by observing the graph of its performance over time during training.\n",
        "\n",
        "The **fit** method returns an object containing loss and metrics values at successive epochs for both training and validation sets.\n",
        "\n",
        "The following code draws in a graph the loss and accuracy trend over epochs on both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNV8XdV2dzvI"
      },
      "source": [
        "plot_history(history,metric='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4YaZvLk_B1o"
      },
      "source": [
        "## **Save and load the entire model**\n",
        "Once the model has been trained, it is a good idea to save it for later use without repeating the entire training phase again.\n",
        "\n",
        "The [**save**](https://keras.io/api/models/model_saving_apis/#save-method) method (or alternatively the [**save_model**](https://keras.io/api/models/model_saving_apis/#savemodel-function) function) saves the entire model given the output file path.\n",
        "\n",
        "The resulting file will include:\n",
        "- the model's architecture;\n",
        "- the model's weights;\n",
        "- the compilation information (if **compile** was called);\n",
        "- the optimizer and its state, if any (this enables you to restart training where you left)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uHYEg4RAHTC"
      },
      "source": [
        "model.save('LeNet5.h5')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl_LlNk8AYtl"
      },
      "source": [
        "To load a model previously saved, the [**load_model**](https://keras.io/api/models/model_saving_apis/#loadmodel-function) function can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK0xfJU8AY2G"
      },
      "source": [
        "model=keras.models.load_model('LeNet5.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD8KPrBQFE5F"
      },
      "source": [
        "The model is re-instantiated in the exact same state, without the need of any code for model definition or compilation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCdi-HJP9iGQ"
      },
      "source": [
        "## **Save and load model weights**\n",
        "Sometimes could be useful to save only the model's weights:\n",
        "- if you only need the model for inference;\n",
        "- if you are doing transfer learning. \n",
        "\n",
        "In such case, the [**save_weights**](https://keras.io/api/models/model_saving_apis/#saveweights-method) method can be used to save all layer weights given the path to the file to save them to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95CdXH1B97br"
      },
      "source": [
        "model.save_weights('LeNet5_weights.h5')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Poh_hfP_CXPd"
      },
      "source": [
        "To load weights previously saved, the [**load_weights**](https://keras.io/api/models/model_saving_apis/#loadweights-method) method can be used. \n",
        "\n",
        "<u>Note that:</u>\n",
        "- a model with the same architecture needs to be created in advance;\n",
        "- if you want to use the model for training, it must be compiled because no information about the optimizer, the loss function and metrics have been stored in the weight file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbav6vO2CXnk"
      },
      "source": [
        "model=build_lenet5(train_x[0].shape,output_class_count=class_count)\n",
        "model.load_weights('LeNet5_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_EjV1RnQ_7A"
      },
      "source": [
        "## **Manual training**\n",
        "To have more control on the training phase, it is possible to call the **fit** method setting the *epochs* parameter equal to 1.\n",
        "\n",
        "In this case, the performance evaluation needs to be manually executed using the [**evaluate**](https://keras.io/api/models/model_training_apis/#evaluate-method) method. It returns the loss and metrics values on the dataset passed as input.\n",
        "\n",
        "In this manner, we will be able to save the model not only at the end of the training phase but also during the training itself.\n",
        "\n",
        "<u>Be sure to create and compile a new model before executing the following code otherwise the training will be performed on a model already trained.</u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPaWUsC_RJZV"
      },
      "source": [
        "history_train_metrics=[]\n",
        "history_valid_metrics=[]\n",
        "\n",
        "for epoch in range(epoch_count):\n",
        "    print('Epoch {}/{}'.format(epoch+1,epoch_count), end = '')\n",
        "    \n",
        "    model.fit(train_x, train_y, batch_size=batch_size, epochs=1, verbose = 0)\n",
        "\n",
        "    train_metrics = model.evaluate(train_x, train_y, verbose = 0)\n",
        "    valid_metrics = model.evaluate(val_x, val_y, verbose = 0)\n",
        "    history_train_metrics.append(train_metrics)\n",
        "    history_valid_metrics.append(valid_metrics)\n",
        "    print('\\tTRAIN', end = '')\n",
        "    for i in range(len(model.metrics_names)):\n",
        "      print(' {}={:.4f}'.format(model.metrics_names[i],train_metrics[i]), end = '')\n",
        "    print(' VAL', end = '')\n",
        "    for i in range(len(model.metrics_names)):\n",
        "      print(' {}={:.4f}'.format(model.metrics_names[i],valid_metrics[i]), end = '')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tcpBsxk8He4"
      },
      "source": [
        "## **Performance evaluation on the test set**\n",
        "The performance on the test set can be easily measured by calling the **evaluate** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXFZ9-VS8ING"
      },
      "source": [
        "results = model.evaluate(test_x, test_y, batch_size=batch_size,verbose=0)\n",
        "print('Loss: {:.3f} Accuracy: {:.3f}'.format(results[0],results[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ROVnU6aQWyY"
      },
      "source": [
        "### **Confusion matrix**\n",
        "To evaluate the classification accuracy could be useful to compute the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
        "\n",
        "The following code calls the [**predict**](https://keras.io/api/models/model_training_apis/#predict-method) method to generate output predictions (*test_conf_pred*) for the test set (*test_x*). The output predictions contain, for each evaluated image, the probability values that the image belongs to each of the classes of the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2dyIUf58fQG"
      },
      "source": [
        "test_conf_pred=model.predict(test_x)\n",
        "print('Output predictions shape: ',test_conf_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zusC8rJyIMNo"
      },
      "source": [
        "The predicted class of each evaluated image (*test_y_pred*) can be obtained by selecting the class with the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz2mtrEVINOJ"
      },
      "source": [
        "test_y_pred=np.argsort(test_conf_pred,axis=1)[:,-1]\n",
        "print('Class predictions shape: ',test_y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW8rjE6XLzMI"
      },
      "source": [
        "The evaluated images correctly classified can be identified by comparing the predicted classes (*test_y_pred*) with respect to the ground truth (*test_y*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7MVlhjTLzn4"
      },
      "source": [
        "correct = np.equal(test_y_pred,test_y)\n",
        "accuracy=correct.sum()/len(correct)\n",
        "print('Test set accuracy: {:.3f}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj9bHFQKZTcY"
      },
      "source": [
        "Scikit-learn library provides the function [**confusion_matrix**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to compute the confusion matrix given the grouhd truth (*test_y*) and the predicted classes (*test_y_pred*) as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0V33xnvQepu"
      },
      "source": [
        "conf_matrix=confusion_matrix(test_y, test_y_pred, normalize='true')\n",
        "print(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1INqTXaRvIJ"
      },
      "source": [
        "The following code visualizes the 2D confusion matrix as a color-coded image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8H68rBjRvTf"
      },
      "source": [
        "show_confusion_matrix(conf_matrix,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT8htD-_8clG"
      },
      "source": [
        "### **Visualization of misclassified images**\n",
        "To better understand a model limits and to try to overcome them, it is important to analyze its errors.\n",
        "\n",
        "The following code shows randomly selected images erroneously classified by the model.\n",
        "The ground truth is reported on top of each image while on the right side the most probable classes returned by the model are shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbVii7doBCBn"
      },
      "source": [
        "images_to_show=12\n",
        "\n",
        "error_indices = np.where(correct == False)[0]\n",
        "\n",
        "if error_indices.shape[0] > 0:\n",
        "  image_per_row = 4\n",
        "  top_class_count = 3\n",
        "\n",
        "  selected_indices=[]\n",
        "  for i in range(min(images_to_show,error_indices.shape[0])):\n",
        "    random_idx=random.randint(0,error_indices.shape[0])\n",
        "    selected_indices.append(random_idx)\n",
        "  error_indices=error_indices[selected_indices]\n",
        "\n",
        "  row_count=math.ceil(len(error_indices)/image_per_row)\n",
        "  column_count=image_per_row\n",
        "  plt.rcParams.update({'font.size': 12})\n",
        "  _, axs = plt.subplots(row_count, column_count,figsize=(25, 4*row_count),squeeze=False)\n",
        "\n",
        "  for i in range(row_count):\n",
        "    for j in range(column_count):\n",
        "      axs[i,j].axis('off')\n",
        "\n",
        "  for i in range(len(error_indices)):\n",
        "    q = i // image_per_row\n",
        "    r = i % image_per_row\n",
        "    idx = error_indices[i]\n",
        "\n",
        "    axs[q,r].imshow(test_x[idx].squeeze(),cmap='gray')\n",
        "    axs[q,r].set_title(class_names[test_y[idx]])\n",
        "\n",
        "    sorted_conf_indices=np.argsort(test_conf_pred[idx])\n",
        "    best_indices=sorted_conf_indices[-top_class_count:]\n",
        "        \n",
        "    text=''\n",
        "    for j in range(len(best_indices)-1,-1,-1):\n",
        "        text+='{}: {:.3f}\\n'.format(class_names[best_indices[j]],test_conf_pred[idx][best_indices[j]])\n",
        "\n",
        "    axs[q,r].text(35, 10, text, horizontalalignment='left', verticalalignment='center')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiomaVDKbMfx"
      },
      "source": [
        "# **Exercise 1**\n",
        "Train *LeNet-5* to classify **digits MNIST** images:\n",
        "1. execute the training process multiple times to find the optimal hyperparameters;\n",
        "2. save the best model;\n",
        "3. compute the accuracy on the test set.\n",
        "\n",
        "It is recommended to evaluate the following hyperparameters (listed in priority order):\n",
        "1. the number of training epochs (*epoch_count*);\n",
        "2. the optimization algorithm (*optimizer*);\n",
        "3. the parameters of the optimization algorithm (e.g., learning rate);\n",
        "4. the mini-batch size (*batch_size*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EXD5VDOdon5"
      },
      "source": [
        "# **Exercise 2**\n",
        "Repeat *Exercise 1* on the other datasets:\n",
        "- **fashion MNIST**;\n",
        "- **CIFAR10**;\n",
        "- **CIFAR100**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBgrMhR-i61y"
      },
      "source": [
        "# **Exercise 3**\n",
        "Train *AlexNet* to classify **CIFAR10** and **CIFAR100** images:\n",
        "1. load and prepare the dataset;\n",
        "2. define the *AlexNet* model implementing the **build_alexnet** function;\n",
        "3. execute the training process multiple times to find the optimal hyperparameters;\n",
        "4. save the best model;\n",
        "5. compute the accuracy on the test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cvt9d2kjmFR"
      },
      "source": [
        "## **Dataset**\n",
        "The following code loads in memory the selected dataset and creates the validation set by randomly selecting *val_size* patterns from the original training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0msNs4rZeJfp"
      },
      "source": [
        "dataset='cifar10' #   'cifar10'   'cifar100'\n",
        "val_size=10000\n",
        "\n",
        "if dataset=='cifar10':\n",
        "    (data_train_x,data_train_y), (data_test_x,data_test_y) = keras.datasets.cifar10.load_data()\n",
        "    class_names=('airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck')\n",
        "    data_train_y=data_train_y.squeeze()\n",
        "    data_test_y=data_test_y.squeeze()\n",
        "elif dataset=='cifar100':\n",
        "    (data_train_x,data_train_y), (data_test_x,data_test_y) = keras.datasets.cifar100.load_data()\n",
        "    class_names=('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle','bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel','can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock','cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur','dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster','house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion','lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse','mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear','pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine','possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose','sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake','spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table','tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout','tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman','worm')\n",
        "    data_train_y=data_train_y.squeeze()\n",
        "    data_test_y=data_test_y.squeeze()\n",
        "\n",
        "class_count=len(class_names)\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(data_train_x, data_train_y, test_size=val_size, random_state=42,shuffle=True)\n",
        "train_x=np.array(train_x)\n",
        "val_x=np.array(val_x)\n",
        "\n",
        "test_x=data_test_x\n",
        "test_y=data_test_y\n",
        "\n",
        "print('Train shape: ',train_x.shape)\n",
        "print('Validation shape: ',val_x.shape)\n",
        "print('Test shape: ',test_x.shape)\n",
        "print('Number of classes: ',class_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiJ2SfUtfX26"
      },
      "source": [
        "### **Preprocessing**\n",
        "To normalize pixel intensity values in the range [0;1] all images are divided by 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_kXR2lNt_Dj"
      },
      "source": [
        "train_x=train_x/255\n",
        "val_x=val_x/255\n",
        "test_x=test_x/255\n",
        "print('Min value: ',train_x.min())\n",
        "print('Max value: ',train_x.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLOV9KfsuLPR"
      },
      "source": [
        "*AlexNet* has been originally designed to receive as input a 227x227x3 image. To maintain the original input shape, all images need to be resized from 32x32x3 to 227x227x3.\n",
        "\n",
        "Unfortunately, while 60000 images of 32x32x3 can be kept in memory (it requires about 700MB), this is not possible after the resize (it would require about 35GB).\n",
        "\n",
        "To overcome this problem, TensorFlow provides functions and operations to easily manipulate and modify large datasets without the need to maintain them into memory. To use these methods and procedures, it is necessary to transform our dataset into an efficient TensorFlow data representation called [**Dataset**](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMi-xwFcjm3y"
      },
      "source": [
        "train_dataset=tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
        "val_dataset=tf.data.Dataset.from_tensor_slices((val_x,val_y))\n",
        "test_dataset=tf.data.Dataset.from_tensor_slices((test_x,test_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZfo7Xxcj50Z"
      },
      "source": [
        "Then the resize operation can be applyed (using the [**resize_with_pad**](https://www.tensorflow.org/api_docs/python/tf/image/resize_with_pad) function) to prepare the data.\n",
        "\n",
        "The operation is not directly applied to all images but it will be applied only when an image is requested.\n",
        "\n",
        "A detailed guide on how preprocess data using the class **Dataset** can be found [here](https://www.tensorflow.org/guide/data?hl=en#preprocessing_data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT4BDuOzj6EU"
      },
      "source": [
        "resize_mapping=lambda X,y : (tf.image.resize_with_pad(X, 227, 227), y)\n",
        "train_dataset=train_dataset.map(resize_mapping)\n",
        "val_dataset=val_dataset.map(resize_mapping)\n",
        "test_dataset=test_dataset.map(resize_mapping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Di3u624kDwh"
      },
      "source": [
        "Finally mini-batches need to be created. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKLP6GAkkEFS"
      },
      "source": [
        "batch_size=250\n",
        "\n",
        "train_dataset=train_dataset.batch(batch_size)\n",
        "val_dataset=val_dataset.batch(batch_size)\n",
        "test_dataset=test_dataset.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZT-wYht60ly"
      },
      "source": [
        "Note that, the elements contained into a **Dataset** can be accessed only sequentially, no direct access using indices is permitted. For this reason, if it is necessary to split data into separate sets (e.g., train and validation sets) or to shuffle them, it is better to use Scikit-learn functionalities before creating a **Dataset** instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAz_ypwYinzK"
      },
      "source": [
        "## **AlexNet**\n",
        "*AlexNet* consists of:\n",
        "- five **convolutional** layers;\n",
        "- three **max pooling** layers;\n",
        "- three **fully-connected** layers.\n",
        "\n",
        "![alt text](https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/CNN/AlexNet.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeTYk4naLseC"
      },
      "source": [
        "### **Batch normalization**\n",
        "In *AlexNet*, *local response normalization* has been applied after the first two convolutional layers to help speed up convergence. Nowadays, it is usually replaced by the [*batch normalization*](https://en.wikipedia.org/wiki/Batch_normalization) because it has been proven that it performs better.\n",
        "\n",
        "Keras provides a [**BatchNormalization**](https://keras.io/api/layers/normalization_layers/batch_normalization/) layer that maintains the mean output close to 0 and the output standard deviation close to 1.\n",
        "\n",
        "In the **build_alexnet** function, add this layer after the first two convolutional layers.\n",
        "\n",
        "Note that, *batch normalization* is usually added between the output of a layer and its activation. To do this:\n",
        "- set the *activation* parameter of the layer before the **BatchNormalization** layer as *None*. In this way no activation is applied;  \n",
        "- add the **BatchNormalization** layer;\n",
        "- add an [**Activation**](https://keras.io/api/layers/core_layers/activation/) layer to apply the desired activation function (*Relu* for *AlexNet*) to the output of the **BatchNormalization** layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isw33vyMxxNr"
      },
      "source": [
        "### **Dropout**\n",
        "In the first two fully-connected layers [*dropout*](https://en.wikipedia.org/wiki/Dilution_(neural_networks)#Dropout) is applied during training. It consists of setting to zero the output of each neuron in the layer with probability *p* (equals to 0.5 in *AlexNet*).\n",
        "\n",
        "![alt text](https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/CNN/Dropout.png)\n",
        "\n",
        "Keras provides a [**Dropout**](https://keras.io/api/layers/regularization_layers/dropout/) layer that randomly sets input units to 0 at each step during training time.\n",
        "\n",
        "In the **build_alexnet** function, add this layer after the first two fully-connected layers with a *rate* parameter equal to 0.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHwf8hxr6ZpX"
      },
      "source": [
        "### **Model definition**\n",
        "Implement the following function to create an *AlexNet* model given:\n",
        "- the shape of the input images (*input_shape*);\n",
        "- the number of output classes (*output_class_count*).\n",
        "\n",
        "To create 2D max pooling layers, the [**MaxPooling2D**](https://keras.io/api/layers/pooling_layers/max_pooling2d/) class provided by Keras can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5pAJJN2i77x"
      },
      "source": [
        "def build_alexnet(input_shape=(227, 227, 3),output_class_count=1000):\n",
        "    #..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgmJih3O-rCZ"
      },
      "source": [
        "### **Model creation**\n",
        "Call the **build_alexnet** function to create an *AlexNet* model with the default input shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvYU_8TdkNLq"
      },
      "source": [
        "model=build_alexnet(output_class_count=class_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZdZFp8n_bRo"
      },
      "source": [
        "### **Model visualization**\n",
        "Visualize the created model to verify its correctness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67hm7MYdklGZ"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUek5tTZA_Cb"
      },
      "source": [
        "keras.utils.plot_model(model,show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WM5qtxkARfa"
      },
      "source": [
        "### **Model compilation**\n",
        "Compile the model for training by setting up:\n",
        "- the optimization algorithm;\n",
        "- the loss function;\n",
        "- the metrics used to evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok6E4DHdlLbq"
      },
      "source": [
        "optimizer=keras.optimizers.SGD()\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}