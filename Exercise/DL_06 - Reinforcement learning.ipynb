{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_06 - Reinforcement learning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLTOT13PyXVU"
      },
      "source": [
        "# **Reinforcement learning tutorial**\n",
        "In today's tutorial you will learn how apply Q-learning algorithm and deep Q-networks on simulated environments.\n",
        "\n",
        "We will use [**TensorFlow**](https://ekababisong.org/gcp-ml-seminar/tensorflow/) framework and [**Keras**](https://keras.io/) open-source library to rapidly prototype deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywHWGctblX3r"
      },
      "source": [
        "# **Preliminary operations**\n",
        "The following code installs:\n",
        "- some supplementary modules useful to visualization purposes;\n",
        "- the [**atary-py**](https://github.com/openai/atari-py) module necessary to use Atari environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDXhM5SNlONs"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install gym==0.19.0\n",
        "!pip install pyglet\n",
        "\n",
        "!pip install atari-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4iMSieQ8bAE"
      },
      "source": [
        "The following code downloads all the necessary material into the remote machine. At the end of the execution select the **File** tab to verify that everything has been correctly downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8XJ6Dyf8dNX"
      },
      "source": [
        "!wget http://www.atarimania.com/roms/Roms.rar\n",
        "!wget https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/RL/BreakoutDeterministic-v4_DQN_Weights.zip\n",
        "\n",
        "!unrar e -o+ Roms.rar\n",
        "!unzip BreakoutDeterministic-v4_DQN_Weights.zip\n",
        "\n",
        "!rm Roms.rar\n",
        "!rm BreakoutDeterministic-v4_DQN_Weights.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM9OkuNOaWnU"
      },
      "source": [
        "In order to import [Atari 2600 ROMS](http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html), you need to run the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQBqOexBaXL9"
      },
      "source": [
        "!python -m atari_py.import_roms /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ogh0deFhkSd"
      },
      "source": [
        "# **OpenAI Gym**\n",
        "[**Gym**](https://www.gymlibrary.ml/) is a toolkit, developed by the [**OpenAI**](https://openai.com/) company, providing an easy to set up, general-intelligence benchmark with a wide variety of different environments for developing and comparing reinforcement learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzHxqnm30C9F"
      },
      "source": [
        "# **Useful modules import**\n",
        "First of all, it is necessary to import useful modules used during the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_LtjnUptc_J"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "import statistics\n",
        "import cv2\n",
        "import uuid\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from IPython.display import clear_output\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from skimage import transform\n",
        "from skimage.color import rgb2gray\n",
        "from collections import deque\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGP7KdAG1x_H"
      },
      "source": [
        "# **Utility functions**\n",
        "Execute the following code to define some utility functions used in the tutorial:\n",
        "- **print_taxi_step** draws a single frame of the taxi environment with the corresponding information;\n",
        "- **print_taxi_single_episode** visualizes an entire episode of the taxi environment;\n",
        "- **plot_training_rewards** draws in a graph the total reward trend and its moving average reached during the different episodes of the training process;\n",
        "- **plot_stacked_frames** shows a stack of frames;\n",
        "- **create_mp4_video_from_frames** creates an MP4 video file from a list of frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZlpBsy_10fD"
      },
      "source": [
        "def print_taxi_step(frame,episode,step,state,action,reward,current_total_reward):\n",
        "  print(frame)\n",
        "  if episode!=None:\n",
        "    print('Episode: ',episode)\n",
        "  if step!=None:\n",
        "    print('Step: ',step)\n",
        "  print('State: ',state)\n",
        "  print('Action: ',action)\n",
        "  print('Reward: ',reward)\n",
        "  print('Total reward: ',current_total_reward)\n",
        "\n",
        "def print_taxi_single_episode(frames,max_steps=50,seconds_to_sleep=.1,episode=None):\n",
        "  for i, frame in enumerate(frames):\n",
        "    if i>=max_steps:\n",
        "      break\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print_taxi_step(frame['frame'],episode,i,frame['state'],frame['action'],frame['reward'],frame['total_reward'])\n",
        "    time.sleep(seconds_to_sleep)\n",
        "\n",
        "def plot_training_rewards(rewards,moving_avg_window_size=None):\n",
        "  if moving_avg_window_size is not None:\n",
        "    moving_avg_total_reward=[]\n",
        "    for i in range(len(rewards)):\n",
        "      window=rewards[max(0,i-moving_avg_window_size+1):i+1]\n",
        "      window_avg=statistics.mean(window)\n",
        "      moving_avg_total_reward.append(window_avg)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(15, 5))\n",
        "\n",
        "  ax.plot(range(len(rewards)),rewards,label='Total reward',color='orange')\n",
        "  ax.set_xlabel('Episodes')\n",
        "  if moving_avg_window_size is not None:\n",
        "    ax.plot(range(len(moving_avg_total_reward)),moving_avg_total_reward,label='Total reward moving average')\n",
        "    ax.legend(loc='upper left')\n",
        "\n",
        "def plot_stacked_frames(stacked_frames):\n",
        "  _,axs=plt.subplots(1,stacked_frames.shape[2],figsize=(15,5))\n",
        "  for i in range(stacked_frames.shape[2]):\n",
        "    axs[i].axis('off')\n",
        "    axs[i].imshow(stacked_frames[:,:,i],cmap='gray')\n",
        "\n",
        "def create_mp4_video_from_frames(frames,fps):\n",
        "  temp_video_path='tempfile.mp4'\n",
        "  compressed_path='{}.mp4'.format(str(uuid.uuid4()))\n",
        "  \n",
        "  size=(frames[0].shape[1],frames[0].shape[0])\n",
        "  out = cv2.VideoWriter(temp_video_path,cv2.VideoWriter_fourcc(*'mp4v'), fps, size)\n",
        "  \n",
        "  for i in range(len(frames)):\n",
        "      out.write(frames[i][...,::-1].copy())  #rgb[...,::-1].copy()\n",
        "  out.release()\n",
        "\n",
        "  os.system(f\"ffmpeg -i {temp_video_path} -vcodec libx264 {compressed_path}\")\n",
        "\n",
        "  os.remove(temp_video_path)\n",
        "\n",
        "  return compressed_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYjwygcU0RMd"
      },
      "source": [
        "# **Q-learning with self-driving taxi**\n",
        "In this section, the Q-Learning algorithm is applied to a self-driving taxi that will need to learn how to transport its passengers to the desired destination. \n",
        "\n",
        "To this purpose, the [**Taxi**](https://www.gymlibrary.ml/environments/toy_text/taxi/) environment provided by Gym will be used:\n",
        "\n",
        "\"*There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHsAL3OJ02aE"
      },
      "source": [
        "## **Taxi environment**\n",
        "The environment is represented by a training area for the self-driving taxi where to learn transport people in four different locations (R, G, Y, B):\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/RL/Reinforcement_Learning_Taxi_Env.png width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvPFemVmjyFt"
      },
      "source": [
        "### **Creation**\n",
        "The following code creates the Taxi environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4hCHiFouptT"
      },
      "source": [
        "env = gym.make('Taxi-v3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbwXGzkCjMel"
      },
      "source": [
        "### **Visualization**\n",
        "To visualize the environment, the [**render**](https://www.gymlibrary.ml/content/api/#rendering) method can be used:\n",
        "- the filled square represents the taxi, which is yellow without a passenger and green with a passenger;\n",
        "- the pipe (\"|\") represents a wall which the taxi cannot cross;\n",
        "- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3qxaLHOjMpl"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdKE599ophLI"
      },
      "source": [
        "### **State space**\n",
        "Assuming self-driving taxi is the only vehicle in this parking lot, the parking lot can be divided into a $5\\times 5$ grid, obtaining 25 possible taxi locations. Four of them are locations where the taxi can pick up and drop off a passenger: R, G, Y, B or $[(0,0), (0,4), (4,0), (4,3)]$ in $(row, col)$ coordinates.\n",
        "\n",
        "Moreover, there are 4 possible destinations and 5 (R, G, Y, B or inside the taxi) passenger locations.\n",
        "\n",
        "The taxi environment has $5\\times 5\\times 5\\times 4=500$ total possible states.\n",
        "\n",
        "Every Gym environment comes with an [**observation_space**](https://www.gymlibrary.ml/content/api/#attributes) attribute describing the format of valid observations (or states). If the space is composed by a fixed number of discrete values, an instance of the [**Discrete**](https://www.gymlibrary.ml/content/api/#spaces) space class is returned and the **n** property can be used to get the cardinality of the space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hjWXtTM-AAM"
      },
      "source": [
        "print('There are {} possible states'.format(env.observation_space.n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkJakNGIptmG"
      },
      "source": [
        "### **Action space**\n",
        "The action space is composed by six possible actions:\n",
        "- south;\n",
        "- north;\n",
        "- east;\n",
        "- west;\n",
        "- pickup;\n",
        "- dropoff.\n",
        "\n",
        "Every Gym environment comes with an [**action_space**](https://www.gymlibrary.ml/content/api/#additional-environment-api) attribute describing the format of valid actions. As for the state space, the action space is discrete and the number of possible actions can be retrieved by the **n** property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoUgl4f4-Lcc"
      },
      "source": [
        "print('There are {} possible actions'.format(env.action_space.n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8d2IlWCgw5F"
      },
      "source": [
        "### **Rewards**\n",
        "The rewards are:\n",
        "- -1 for each step;\n",
        "- -1 for every wall hit (the taxi will not move anywhere);\n",
        "- +20 for successfully deliver the passenger;\n",
        "- -10 for illegal actions (pick up or put down the passenger in the wrong location)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrcYWOR4vx-w"
      },
      "source": [
        "## **Exercise 1: Q-learning algorithm**\n",
        "Implement the **q_learning** function given:\n",
        "- the environment (*env*);\n",
        "- the number of episodes (*episode_count*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*);\n",
        "- the maximum value of $\\epsilon$ (*max_epsilon*);\n",
        "- the minimum value of $\\epsilon$ (*min_epsilon*);\n",
        "- the $\\epsilon$ decay value (*epsilon_decay*);\n",
        "- the learning rate $\\alpha$ (alpha);\n",
        "- the discount factor $\\gamma$ (gamma).\n",
        "\n",
        "It returns the learned Q-table (*Q*) and a **List** (*learning_history*) containing useful information to visualize the agent progresses during the learning process.\n",
        "\n",
        "The following image shows the pseudo-code of the *Q-learning* algorithm.\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/RL/Q_learning_algorithm.png width=\"800\">\n",
        "\n",
        "The following parts need to be implemented:\n",
        "1. initialize *Q* as a Numpy zero matrix with as many rows as the states and as many columns as the actions. The [**zeros**](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html) function provided by the Numpy module can be used;\n",
        "2. initialize $\\epsilon$ equal to *max_epsilon*;\n",
        "3. call the **reset** method provided by the Gym environment interface to reset the environment and get the random initial state ($s_0$);\n",
        "4. decide whether to pick a random action or to exploit the already computed Q-values. To this purpose, the **uniform** function provided by the [**random**](https://docs.python.org/3/library/random.html) module can be used to generate a random number in the range $[0;1]$ to be compared against $\\epsilon$. If a random action needs to be selected from the set of all possible actions, the **action_space.sample** method provided by the Gym environment can be exploited. Otherwise, use the [**argmax**](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) function provided by Numpy to select the optimal action for state $s_t$;\n",
        "5. call the **step** method provided by the Gym environment interface to\n",
        "execute the action $a_t$ selected at the previous point. It returns the new observation (or state) $s_{t+1}$, the reward $r_{t+1}$ achieved by action $a_t$, a boolean flag indicating if the episode has terminated (*True*) or not (*False*) and other additional information;\n",
        "6. update $Q(s_t,a_t)$ using the *Bellman* equation. The Numpy [**amax**](https://numpy.org/doc/stable/reference/generated/numpy.amax.html) function can be used to find the maximum value of $Q(s_{t+1},a)$;\n",
        "7. reduce $\\epsilon$. It is progressively reduced during the learning process because, while the agent learns more and more about the environment, less and less exploration will be needed;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDapFnFEMRF6"
      },
      "source": [
        "def q_learning(env,episode_count,episode_max_steps,max_epsilon,min_epsilon,epsilon_decay,alpha,gamma):\n",
        "  # 1. initialize Q as a zero matrix with as many rows as the states and as many columns as the actions \n",
        "  Q = #...\n",
        "\n",
        "  learning_history = [] # for visualization purposes\n",
        "\n",
        "  # 2. inizialize epsilon equal to max_epsilon\n",
        "  epsilon= #...\n",
        "\n",
        "  for episode in range(episode_count):\n",
        "      # for visualization purposes\n",
        "      episode_history = [] \n",
        "      episode_total_reward=0\n",
        "      \n",
        "      # 3. reset the environment\n",
        "      state = #...\n",
        "      \n",
        "      step_count=0  #t\n",
        "      done=False\n",
        "      while (step_count<episode_max_steps) and (not done):\n",
        "          # 4. decide whether to pick a random action or to exploit the already computed Q-values\n",
        "          #...\n",
        "    \n",
        "          # 5. take the action and observe the outcome state and reward\n",
        "          new_state, reward, done, _ = #...\n",
        "\n",
        "          # 6. update Q(s,a)\n",
        "          Q[state][action] = #...\n",
        "\n",
        "          # put the current frame into a list for visualization purposes\n",
        "          episode_total_reward+=reward\n",
        "          episode_history.append(\n",
        "                                  {\n",
        "                                  'frame': env.render(mode='ansi'),\n",
        "                                  'state': new_state,\n",
        "                                  'action': action,\n",
        "                                  'reward': reward,\n",
        "                                  'total_reward' : episode_total_reward,\n",
        "                                  }\n",
        "                                )\n",
        "\n",
        "          # update the current state\n",
        "          state = new_state\n",
        "\n",
        "          # increase the step count\n",
        "          step_count+=1\n",
        "          \n",
        "      # 7. reduce epsilon\n",
        "      epsilon= #...\n",
        "\n",
        "      # put all the episode frames into a list for visualization purposes\n",
        "      learning_history.append(episode_history)\n",
        "\n",
        "  # return Q-table and history\n",
        "  return Q, learning_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8z0zryiIvhy"
      },
      "source": [
        "## **Learning**\n",
        "Now we are ready to start the learning process by calling the **q_learning** function.\n",
        "\n",
        "The following parameters must be set before being passed to the function:\n",
        "- the number of episodes (*episode_count*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*);\n",
        "- the maximum value of $\\epsilon$ (*max_epsilon*);\n",
        "- the minimum value of $\\epsilon$ (*min_epsilon*);\n",
        "- the $\\epsilon$ decay value (*epsilon_decay*);\n",
        "- the learning rate $\\alpha$ (alpha);\n",
        "- the discount factor $\\gamma$ (gamma)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOBFLWhvAoRR"
      },
      "source": [
        "episode_count = 2000          # Total number of training episodes\n",
        "episode_max_steps = 200       # Maximum number of steps per episode\n",
        "\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.001           # Minimum exploration probability \n",
        "epsilon_decay = 0.01          # Decay for exploration probability\n",
        "\n",
        "alpha = 0.5                   # Learning rate\n",
        "gamma = 0.99                  # Discount factor\n",
        "\n",
        "start_time = time.time()\n",
        "Q,learning_history=q_learning(env,episode_count,episode_max_steps,max_epsilon,min_epsilon,epsilon_decay,alpha,gamma)\n",
        "print('Learning time: {:.1f}s'.format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oVrsaOPqsT2"
      },
      "source": [
        "### **Visualize the learning process**\n",
        "It is important to observe how performance changes over time during the learning process.\n",
        "\n",
        "The following code draws in a graph the number of steps and the total reward reached during the different episodes of the learning process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le8wb4UaI3OC"
      },
      "source": [
        "steps=[]\n",
        "total_rewards=[]\n",
        "for episode in learning_history:\n",
        "  steps.append(len(episode))\n",
        "  total_rewards.append(episode[-1]['total_reward'])\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(15, 5))\n",
        "\n",
        "ax1.plot(range(len(steps)),steps,color='orange')\n",
        "ax1.tick_params(axis='y', labelcolor='orange')\n",
        "ax1.set_xlabel('Episodes')\n",
        "ax1.set_ylabel('Steps',color = 'orange')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(range(len(total_rewards)),total_rewards,color='#1f77b4')\n",
        "ax2.tick_params(axis='y', labelcolor='#1f77b4')\n",
        "ax2.set_ylabel('Total reward',color='#1f77b4')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1arMQr_huEJF"
      },
      "source": [
        "To better highlight how the self-driving taxi improves during the learning process, it is useful to visualize single episodes using the **print_taxi_single_episode** function defined above. The following code shows the first episode of the learning process (only the first 25 steps per episode are displayed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmPDvIaG60QW"
      },
      "source": [
        "print_taxi_single_episode(learning_history[0],25,0.5,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5WKpR2MuboM"
      },
      "source": [
        "While initially the taxi randomly moves exploring the environment, at the end of the learning process the agent is perfectly capable to pickup a passenger and to transport him to the desired destination. The following code shows the last episode of the learning process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuzdpjtFvxpm"
      },
      "source": [
        "print_taxi_single_episode(learning_history[-1],25,0.5,len(learning_history)-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGsTYVMq2q2i"
      },
      "source": [
        "## **Performance evaluation**\n",
        "Now it is time to evaluate the performance of the self-driving taxi agent. It can be evaluated according to the following metrics:\n",
        "- **average number of steps per episode**: the smaller the number, the shorter the path taken by the agent to reach the destination;\n",
        "- **average total reward per episode**: a higher average reward means that the agent reaches the destination as fast as possible with the least penalties;\n",
        "- **average number of penalties per episode**: it is computed as the average number of time per episode the agent executes an illegal action (pick up or put down the passenger in the wrong location). The smaller the number, the better the performance of our agent.\n",
        "\n",
        "By running the following code the evaluation metrics will be computed as the average of the results on several (*test_episode_count*) episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4AhzWZ_v8Uq"
      },
      "source": [
        "test_episode_count = 1000     # Total number of test episodes\n",
        "\n",
        "sum_steps=0\n",
        "sum_total_reward=0\n",
        "sum_penalties=0\n",
        "\n",
        "for episode in range(test_episode_count):\n",
        "    state = env.reset()\n",
        "    for step in range(episode_max_steps):\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(Q[state])\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        sum_steps+=1\n",
        "        sum_total_reward+=reward\n",
        "        if reward == -10:\n",
        "          sum_penalties += 1\n",
        "\n",
        "        if done:\n",
        "          break\n",
        "        state = new_state\n",
        "\n",
        "print ('Average number of steps per episode: {:.1f}'.format(sum_steps/test_episode_count))\n",
        "print ('Average total reward per episode: {:.1f}'.format(sum_total_reward/test_episode_count))\n",
        "print ('Average number of penalties per episode: {:.1f}'.format(sum_penalties/test_episode_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaFRxFG9xJpq"
      },
      "source": [
        "## **Single episode visualization**\n",
        "The following code shows the self-driving in action on a single episode using the **print_taxi_step** function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov9PI0hbtr4L"
      },
      "source": [
        "episode_total_reward = 0\n",
        "\n",
        "state = env.reset()\n",
        "done=False\n",
        "step=0\n",
        "while not done:\n",
        "    action = np.argmax(Q[state])\n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    \n",
        "    episode_total_reward+=reward\n",
        "    clear_output(wait=True)\n",
        "    print_taxi_step(env.render(mode='ansi'),None,step,new_state,action,reward,episode_total_reward)\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    state = new_state\n",
        "    step+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFcs1UtzErww"
      },
      "source": [
        "# **Deep Q-network to solve the CartPole environment**\n",
        "In this section, a Deep Q-Network (DQN) model is trained to solve the [**CartPole**](https://www.gymlibrary.ml/environments/classic_control/cart_pole/) environment provided by Gym."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFfk_p18avWw"
      },
      "source": [
        "## **CartPole environment**\n",
        "The environment is represented by a pole attached to a cart which moves along a frictionless track. The objective is to prevent pole from falling over. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves far from the center."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvIwaKujbbZA"
      },
      "source": [
        "### **Creation**\n",
        "The following code creates the CartPole environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNj9XgWxbetA"
      },
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNu7MSFhbsSn"
      },
      "source": [
        "### **Visualization**\n",
        "To visualize the environment on *Colab*, [**PyVirtualDisplay**](https://pypi.org/project/PyVirtualDisplay/) is used to create a virtual display.\n",
        "\n",
        "The following code creates a virtual display where the environment can be rendered. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWUqfgoU4-dG"
      },
      "source": [
        "display = Display()\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PExv4V66640p"
      },
      "source": [
        "To visualize the current state of the environment, the **render** method can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms8qQunW5CNB"
      },
      "source": [
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.axis('off')\n",
        "plt.imshow(prev_screen)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpCQj-ndQQcZ"
      },
      "source": [
        "### **State space**\n",
        "Differently from Taxi environment, the CartPole state space is represented by 4 continuous real values: \n",
        "- cart Position;\n",
        "- cart Velocity;\n",
        "- pole angle;\n",
        "- pole angular velocity.\n",
        "\n",
        "In this case, the **observation_space** attribute returns a [**Box**](https://www.gymlibrary.ml/content/api/#spaces) space class instance where valid states are in the form of *n*-dimensional arrays. The *shape* of the space can be obtained by the **shape** property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov0HK7xvQOTz"
      },
      "source": [
        "print('State space shape: ',env.observation_space.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Si9Hj-sL82"
      },
      "source": [
        "### **Action space**\n",
        "The action space is composed by two possible actions:\n",
        "- push cart to the left;\n",
        "- push cart to the right.\n",
        "\n",
        "The following code uses the **n** property of the action space to print the number of valid actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KqXRJFHs2Xx"
      },
      "source": [
        "print('There are {} possible actions'.format(env.action_space.n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNYRi3OAtPd5"
      },
      "source": [
        "### **Rewards**\n",
        "Reward is 1 for every step taken, including the termination step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec8i63BqG3C9"
      },
      "source": [
        "## **Model definition**\n",
        "The following function creates a simple neural network to be used as DQN given:\n",
        "- the number of input features (*input_count*);\n",
        "- the number of neurons for each hidden layer (*neuron_count_per_hidden_layer*);\n",
        "- the number of valid actions (*action_count*).\n",
        "\n",
        "In Keras, a sequential is a stack of layers where each layer has exactly one input and one output. It can be created by passing a list of layers to the  constructor [**keras.Sequential**](https://keras.io/guides/sequential_model/).\n",
        "\n",
        "[**Keras layers API**](https://keras.io/api/layers/) offers a wide range of built-in layers ready for use, including:\n",
        "- [**Input**](https://keras.io/api/layers/core_layers/input/) - the input of the model. Note that, you can also omit the **Input** layer. In that case the model doesn't have any weights until the first call to a training/evaluation method (since it is not yet built);\n",
        "- [**Dense**](https://keras.io/api/layers/core_layers/dense/) - a fully-connected layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwgvCwIBG79t"
      },
      "source": [
        "def build_simple_dqn(input_count,neuron_count_per_hidden_layer,action_count):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=input_count,name='Input'))\n",
        "\n",
        "    for n in neuron_count_per_hidden_layer:\n",
        "        model.add(layers.Dense(n,activation='relu'))\n",
        "\n",
        "    model.add(layers.Dense(action_count,name='Output'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njN42JeqJqb9"
      },
      "source": [
        "## **Action and target models creation**\n",
        "To solve the *moving target problem*, two models (*action* and *target*) are used during the DQN training process.\n",
        "\n",
        "The following code creates two identical models by calling the **build_simple_dqn** function defined above. To be identical, the two models need to share initial random weights: [**get_weights**](https://keras.io/api/models/model_saving_apis/#getweights-method) and [**set_weights**](https://keras.io/api/models/model_saving_apis/#setweights-method) methods are used to replace target weights to those of the action model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTT-qE2pJyTL"
      },
      "source": [
        "neuron_count_per_hidden_layer=[64,32]\n",
        "\n",
        "simple_dqn_action_model=build_simple_dqn(env.observation_space.shape,neuron_count_per_hidden_layer,env.action_space.n)\n",
        "simple_dqn_target_model=build_simple_dqn(env.observation_space.shape,neuron_count_per_hidden_layer,env.action_space.n)\n",
        "simple_dqn_target_model.set_weights(simple_dqn_action_model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bxm50-MNPG4"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed using the [**summary**](https://keras.io/api/models/model/#summary-method) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6bs9YZNNRfV"
      },
      "source": [
        "simple_dqn_action_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCRi_phgNe3f"
      },
      "source": [
        "The summary is useful for simple models, but can be confusing for complex models.\n",
        "\n",
        "Function [**keras.utils.plot_model**](https://keras.io/api/utils/model_plotting_utils/) creates a plot of the neural network graph that can make more complex models easier to understand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idq-ImEsNfpv"
      },
      "source": [
        "keras.utils.plot_model(simple_dqn_action_model,show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlIggmE4OATx"
      },
      "source": [
        "## **Action model compilation**\n",
        "The compilation is the final step in configuring the model for training. \n",
        "\n",
        "The following code uses the [**compile**](https://keras.io/api/models/model_training_apis/#compile-method) method to compile the model.\n",
        "The important arguments are:\n",
        "- the optimization algorithm (*optimizer*);\n",
        "- the loss function (*loss*).\n",
        "\n",
        "The most common [optimization algorithms](https://keras.io/api/optimizers/#available-optimizers) and [loss functions](https://keras.io/api/losses/#available-losses) are already available in Keras. You can either pass them to **compile** as an instance or by the corresponding string identifier. In the latter case, the default parameters will be used.\n",
        "\n",
        "<u>Note that, only the action model is compiled because, during the training process, only its weights are updated using *gradient descent* algorithm. Target model weights are periodically replaced by those of the action model.</u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdoq1IBXOIIA"
      },
      "source": [
        "simple_dqn_action_model.compile(optimizer='adam', loss = 'mse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feB3RlCDUPIq"
      },
      "source": [
        "## **Experience replay**\n",
        "To remove correlations between consecutive transitions and make the DQN training more stable, the *experience replay* technique is used.\n",
        "\n",
        "The replay memory is implemented as a [**deque**](https://docs.python.org/3/library/collections.html#collections.deque) (Doubly Ended Queue) object providing an O(1) time complexity for append and pop operations from both the ends of the queue. Moreover, if the *maxlen* parameter is specified, the **deque** is bounded to the specified maximum length. Once a bounded length deque is full, when new items are added, a corresponding number of items are discarded from the opposite end.\n",
        "\n",
        "The DQN authors suggest to populate the replay memory before starting the learning process. For each step $t$, a random action is chosen and executed then the transition $<s_t,a_t,r_{t+1},s_{t+1}>$ is stored in the replay memory. The following function initializes the replay memory given:\n",
        "- the environment (*env*);\n",
        "- the replay memory (*replay_memory*);\n",
        "- the number of transitions to be stored in the replay memory (*replay_memory_init_size*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiCx7_GnUQBv"
      },
      "source": [
        "def simple_dqn_replay_memory_init(env,replay_memory,replay_memory_init_size,episode_max_steps):\n",
        "    while True:\n",
        "        state = env.reset()\n",
        "        done=False\n",
        "        step_count=0\n",
        "        while (step_count<episode_max_steps) and (not done):\n",
        "            action = env.action_space.sample()\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            replay_memory.append([state,action,reward,new_state,done])\n",
        "            state=new_state\n",
        "            step_count+=1\n",
        "            if len(replay_memory)>=replay_memory_init_size:\n",
        "                return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKCPCG1NbNgF"
      },
      "source": [
        "During training, when the action model needs to be updated, a mini-batch is randomly sampled from the replay memory and used instead of the most recent transition. The following function returns a mini-batch containing transitions randomly chosen from the replay memory given:\n",
        "- the replay memory (*replay_memory*);\n",
        "- the mini-batch size (*batch_size*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvNAIYg8bOgj"
      },
      "source": [
        "def get_random_batch_from_replay_memory(replay_memory,batch_size):\n",
        "    minibatch_indices = np.random.choice(range(len(replay_memory)), size=batch_size)\n",
        "    minibatch = [replay_memory[i] for i in minibatch_indices]\n",
        "    \n",
        "    state_batch = np.array([sample[0] for sample in minibatch])\n",
        "    action_batch = np.array([sample[1] for sample in minibatch])\n",
        "    reward_batch = np.array([sample[2] for sample in minibatch])\n",
        "    new_state_batch = np.array([sample[3] for sample in minibatch])\n",
        "    done_batch = np.array([sample[4] for sample in minibatch])\n",
        "\n",
        "    return [state_batch,action_batch,reward_batch,new_state_batch,done_batch]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo6cZDT9ticw"
      },
      "source": [
        "## **Exercise 2: DQN training algorithm**\n",
        "Implement the following function to train a DQN model given:\n",
        "- the environment (*env*);\n",
        "- the action model (*dqn_action_model*);\n",
        "- the target model (*dqn_target_model*);\n",
        "- the number of episodes (*episode_count*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*);\n",
        "- the maximum number of transitions stored into the replay memory (*replay_memory_max_size*);\n",
        "- the number of transitions stored into the replay memory before starting the training process (*replay_memory_init_size*);\n",
        "- the mini-batch size (*batch_size*);\n",
        "- the number ($u$) of total steps executed ($T$) between successive updates of the action model weights (*step_per_update*);\n",
        "- the number ($c$) of total steps executed ($T$) between successive replaces of the target model weights with the weights of the action model (*step_per_update_target_model*); \n",
        "- the maximum value of $\\epsilon$ (*max_epsilon*);\n",
        "- the minimum value of $\\epsilon$ (*min_epsilon*);\n",
        "- the $\\epsilon$ decay value (*epsilon_decay*);\n",
        "- the discount factor $\\gamma$ (gamma);\n",
        "- the number of consecutive episodes to be considered in the calculation of the total reward moving average (*moving_avg_window_size*);\n",
        "- the minimum value of the total reward moving average to consider the task solved (*moving_avg_stop_thr*). A value of *None* force to execute all *episode_count* episodes.\n",
        "\n",
        "It returns a **List** (*train_rewards*) containing the total rewards of all training episodes.\n",
        "\n",
        "The following image shows the pseudo-code of the DQN training algorithm.\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/RL/DQN_training_algorithm.png width=\"800\">\n",
        "\n",
        "The following parts need to be implemented:\n",
        "1. call the **simple_dqn_replay_memory_init** function to populate the replay memory before starting the training process;\n",
        "2. initialize $\\epsilon$ equal to *max_epsilon*;\n",
        "3. call the **reset** method provided by the Gym environment interface to reset the environment and get the random initial state ($s_0$);\n",
        "4. decide whether to pick a random action or to exploit the already computed Q-values. To this purpose, the **uniform** function provided by the random module can be used to generate a random number in the range $[0;1]$ to be compared against $\\epsilon$. If a random action needs to be selected from the set of all possible actions, the **action_space.sample** method provided by the Gym environment can be exploited. Otherwise, use the [**predict**](https://keras.io/api/models/model_training_apis/#predict-method) method of the action model to obtain the Q-values of each available actions given the current state ($s_t$) and the  [**argmax**](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) function provided by Numpy to select the optimal one;\n",
        "5. call the **step** method provided by the Gym environment interface to\n",
        "execute the action $a_t$ selected at the previous point. It returns the new observation (or state) $s_{t+1}$, the reward $r_{t+1}$ achieved by action $a_t$, a boolean flag indicating if the episode has terminated (*True*) or not (*False*) and other additional information;\n",
        "6. call the **get_random_batch_from_replay_memory** function to get a mini-batch of randomly selected transitions from the replay memory;\n",
        "7. copy weights from action to target model using the **get_weights** and **set_weights** methods;\n",
        "8. reduce $\\epsilon$ at each step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G4gXI4_amyr"
      },
      "source": [
        "def simple_dqn_training(env,dqn_action_model,dqn_target_model,episode_count,episode_max_steps,\n",
        "                        replay_memory_max_size,replay_memory_init_size,batch_size,\n",
        "                        step_per_update,step_per_update_target_model,\n",
        "                        max_epsilon,min_epsilon,epsilon_decay,gamma,\n",
        "                        moving_avg_window_size=20,moving_avg_stop_thr=None):\n",
        "    # experience replay memory declaration\n",
        "    replay_memory= deque(maxlen=replay_memory_max_size)\n",
        "    \n",
        "    # 1. replay memory initial population\n",
        "    if replay_memory_init_size>0:\n",
        "        print('Replay memory initial population')\n",
        "        #...\n",
        "\n",
        "    # 2. inizialize epsilon equal to max_epsilon\n",
        "    epsilon = #...\n",
        "\n",
        "    train_rewards=[]  # for visualization purposes\n",
        "\n",
        "    train_step_count=0  #T\n",
        "    for n in range(episode_count): \n",
        "        # for visualization purposes\n",
        "        episode_start_time = time.time()    \n",
        "        episode_reward = 0\n",
        "        episode_epsilon=epsilon\n",
        "\n",
        "        # 3. reset the environment\n",
        "        state = #...\n",
        "\n",
        "        step_count=0  #t\n",
        "        done=False\n",
        "        while step_count<episode_max_steps and not done: \n",
        "            # 4. decide whether to pick a random action or to exploit the already computed Q-values\n",
        "            if random.uniform(0,1) <= epsilon:\n",
        "                action = #...\n",
        "            else:\n",
        "                action = #...\n",
        "            \n",
        "            # 5. take the action and observe the outcome state and reward\n",
        "            new_state, reward, done, _ = #...\n",
        "            \n",
        "            # store transition in the replay memory \n",
        "            replay_memory.append([state,action,reward,new_state,done])\n",
        "            \n",
        "            # update the current state\n",
        "            state=new_state\n",
        "                        \n",
        "            if train_step_count % step_per_update == 0 and len(replay_memory)>=batch_size:\n",
        "                # 6. get a random mini-batch from the replay memory\n",
        "                mini_batch=#...\n",
        "                \n",
        "                # update the action model weights calling the simple_dqn_update function (see below)\n",
        "                dqn_action_model=simple_dqn_update(dqn_action_model,dqn_target_model,mini_batch,gamma)\n",
        "                \n",
        "            if train_step_count % step_per_update_target_model ==0:\n",
        "                # 7. copy weights from action to target model\n",
        "                #...\n",
        "\n",
        "            # 8. reduce epsilon\n",
        "            if epsilon > min_epsilon:\n",
        "                epsilon=#...\n",
        "\n",
        "            # increase episode step count and total step count        \n",
        "            step_count+=1\n",
        "            train_step_count+=1\n",
        "\n",
        "            # add the current reward to the episode total reward\n",
        "            episode_reward += reward \n",
        "        \n",
        "        # put the episode total reward into a list for visualization purposes\n",
        "        train_rewards.append(episode_reward)\n",
        "\n",
        "        # for visualization purposes\n",
        "        episode_finish_time=time.time()\n",
        "        episode_elapsed_time=episode_finish_time-episode_start_time\n",
        "        episode_avg_step_time=episode_elapsed_time/step_count\n",
        "        moving_avg_reward=statistics.mean(train_rewards[-moving_avg_window_size:])\n",
        "        print(\"Episode: {} Steps: {}[{}] Epsilon: {:.3f} Time: {:.1f}s[{:.2f}s]  Total reward: {}[{:.1f}]\".format(n,\n",
        "                                                                                                                  step_count,\n",
        "                                                                                                                  train_step_count,\n",
        "                                                                                                                  episode_epsilon,\n",
        "                                                                                                                  episode_elapsed_time,\n",
        "                                                                                                                  episode_avg_step_time,\n",
        "                                                                                                                  episode_reward,\n",
        "                                                                                                                  moving_avg_reward))\n",
        "        \n",
        "        # condition to consider the task solved\n",
        "        if (moving_avg_stop_thr is not None) and moving_avg_reward>moving_avg_stop_thr:\n",
        "            break\n",
        "\n",
        "    # return a list containing the total rewards of all training episodes  \n",
        "    return train_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44z83bpLjYXD"
      },
      "source": [
        "### **Action model update**\n",
        "Implement the **simple_dqn_update** function to update action model weights using *gradient descent* algorithm given:\n",
        "- the action model (*dqn_action_model*);\n",
        "- the target model (*dqn_target_model*);\n",
        "- a mini-batch containing transitions $<s_i,a_i,r_{i+1},s_{i+1}>$ randomly selected from the replay memory (*mini_batch*); \n",
        "- the discount factor $\\gamma$ (gamma).\n",
        "\n",
        "The following parts need to be implemented:\n",
        "1. estimate $\\hat{Q}(s_{i+1},a)$ for all possible actions using the **predict** method of the target model with *new_state_batch* ($s_{i+1}$) as input parameter;\n",
        "2. estimate $Q(s_i,a)$ for all possible actions using the **predict** method of the action model with *state_batch* ($s_i$) as input parameter;\n",
        "3. update weights of the action model using the [**train_on_batch**](https://keras.io/api/models/model_training_apis/#trainonbatch-method) method with *state_batch* ($s_i$) and *predicted_state_q_values* ($y_i$) as *x* and *y* input parameters, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c4d_07hbT5k"
      },
      "source": [
        "def simple_dqn_update(dqn_action_model,dqn_target_model,mini_batch,gamma):\n",
        "    # the transition mini-batch is divided into a mini-batch for each element of a transition\n",
        "    state_batch,action_batch,reward_batch,new_state_batch,done_batch=mini_batch\n",
        "\n",
        "    # 1. find the target model Q values for all possible actions given the new state batch\n",
        "    target_new_state_q_values = #...\n",
        "    \n",
        "    # 2. find the action model Q values for all possible actions given the current state batch\n",
        "    predicted_state_q_values = #...\n",
        "    \n",
        "    # estimate the target values y_i\n",
        "    # for the action we took, use the target model Q values  \n",
        "    # for other actions, use the action model Q values\n",
        "    # in this way, loss function will be 0 for other actions\n",
        "    for i,(a,r,new_state_q_values, done) in enumerate(zip(action_batch,reward_batch,target_new_state_q_values, done_batch)): \n",
        "        if not done:  \n",
        "          target_value = r + gamma * np.amax(new_state_q_values)\n",
        "        else:         \n",
        "          target_value = r\n",
        "        predicted_state_q_values[i][a] = target_value #y_i\n",
        "    \n",
        "    # 3. update weights of action model using the train_on_batch method \n",
        "    #...\n",
        "    \n",
        "    # return the updated action model\n",
        "    return dqn_action_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqK5u_MugIa4"
      },
      "source": [
        "## **Training**\n",
        "Now we are ready to start the training process by calling the **simple dqn_learning** function.\n",
        "\n",
        "The following parameters must be set before being passed to the function:\n",
        "- the number of episodes (*episode_count*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*);\n",
        "- the maximum number of transitions stored into the replay memory (*replay_memory_max_size*);\n",
        "- the number of transitions stored into the replay memory before starting the training process (*replay_memory_init_size*);\n",
        "- the mini-batch size (*batch_size*);\n",
        "- the number of total steps executed between successive updates of the action model weights (*step_per_update*);\n",
        "- the number of total steps executed between successive replaces of the target model weights with the weights of the action model (*step_per_update_target_model*); \n",
        "- the maximum value of $\\epsilon$ (*max_epsilon*);\n",
        "- the minimum value of $\\epsilon$ (*min_epsilon*);\n",
        "- the $\\epsilon$ decay value (*epsilon_decay*);\n",
        "- the discount factor $\\gamma$ (gamma);\n",
        "- the number of consecutive episodes to be considered in the calculation of the total reward moving average (*moving_avg_window_size*);\n",
        "- the minimum value of the total reward moving average to consider the task solved (*moving_avg_stop_thr*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNoeJ8_TgikQ"
      },
      "source": [
        "episode_count = 500               # Total number of training episodes\n",
        "episode_max_steps=400             # Maximum number of steps per episode\n",
        "\n",
        "replay_memory_max_size = 100000   # Maximum number of transitions stored into the replay memory\n",
        "replay_memory_init_size=1000      # Maximum number of transitions stored into the replay memory\n",
        "batch_size = 64                   # Mini-batch size\n",
        "\n",
        "step_per_update = 4               # Number of total steps executed between successive updates of the action model weights\n",
        "step_per_update_target_model=8    # Number of total steps executed between successive replaces of the target model weights\n",
        "\n",
        "max_epsilon=1.0                   # Exploration probability at start\n",
        "min_epsilon=0.01                  # Minimum exploration probability\n",
        "epsilon_decay=0.0002              # Decay for exploration probability\n",
        "\n",
        "gamma = 0.99                      # Discount factor\n",
        "\n",
        "moving_avg_window_size=20         # Number of consecutive episodes to be considered in the calculation of the total reward moving average\n",
        "moving_avg_stop_thr=100           # Minimum value of the total reward moving average to consider the task solved\n",
        "\n",
        "train_start_time=time.time()\n",
        "train_rewards=simple_dqn_training(env,\n",
        "                                  simple_dqn_action_model,\n",
        "                                  simple_dqn_target_model,\n",
        "                                  episode_count,\n",
        "                                  episode_max_steps,\n",
        "                                  replay_memory_max_size,\n",
        "                                  replay_memory_init_size,\n",
        "                                  batch_size,\n",
        "                                  step_per_update,\n",
        "                                  step_per_update_target_model,\n",
        "                                  max_epsilon,\n",
        "                                  min_epsilon,\n",
        "                                  epsilon_decay,\n",
        "                                  gamma,\n",
        "                                  moving_avg_window_size,\n",
        "                                  moving_avg_stop_thr)\n",
        "\n",
        "train_finish_time=time.time()\n",
        "train_elapsed_time=train_finish_time-train_start_time\n",
        "train_avg_episode_time=train_elapsed_time/episode_count\n",
        "print(\"Train time: {:.1f}m [{:.1f}s]\".format(train_elapsed_time/60.0,train_avg_episode_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5BZGXqQnYO3"
      },
      "source": [
        "### **Visualize the training process**\n",
        "It is important to observe how performance changes over time during the training process.\n",
        "\n",
        "The **plot_training_rewards** function defined above is used to draw in a graph the total reward and its moving average reached during the different episodes of the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I60VmdG10X9"
      },
      "source": [
        "plot_training_rewards(train_rewards,moving_avg_window_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_57XUhJ2ePR"
      },
      "source": [
        "## **Performance evaluation**\n",
        "The CartPole environment can be considered solved when the average total reward over 100 consecutive trials is greater than or equal to 195."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-8f12qB3Ni5"
      },
      "source": [
        "episode_count=5\n",
        "\n",
        "sum_episode_rewards=0\n",
        "for episode in range(episode_count):\n",
        "  state=env.reset()\n",
        "\n",
        "  episode_reward = 0\n",
        "  done=False\n",
        "  step_count=0\n",
        "  while not done:\n",
        "      q_values = simple_dqn_action_model.predict(state[np.newaxis])\n",
        "      action = np.argmax(q_values)\n",
        "      \n",
        "      new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      state = new_state\n",
        "      episode_reward+=reward\n",
        "\n",
        "  sum_episode_rewards+=episode_reward\n",
        "\n",
        "  print('Episode: {} Total reward: {}'.format(episode,episode_reward))\n",
        "\n",
        "print('Average total reward: {:.1f}'.format(sum_episode_rewards/episode_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o99SPQh5NNt"
      },
      "source": [
        "## **Single episode visualization**\n",
        "To show the CartPole agent in action on *Colab*, it is not possible to render each frame at real time.\n",
        "\n",
        "It is necessary to:\n",
        "1. execute an entire episode storing all episode frames;\n",
        "2. create an MP4 video with all frames;\n",
        "3. visualize the video.\n",
        "\n",
        "The following code executes an episode using the trained DQN network and store all frames into a list (*frames*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvKVS2z3OqhK"
      },
      "source": [
        "state=env.reset()\n",
        "\n",
        "frames=[]\n",
        "done=False\n",
        "episode_reward=0\n",
        "while not done:\n",
        "    frame = env.render(mode='rgb_array')\n",
        "    frames.append(frame)\n",
        "    \n",
        "    q_values = simple_dqn_action_model.predict(state[np.newaxis])\n",
        "    action = np.argmax(q_values)\n",
        "    \n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    state = new_state\n",
        "\n",
        "    episode_reward+=reward\n",
        "\n",
        "print('Total reward: ',episode_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vbC3ILJUzsK"
      },
      "source": [
        "Then the MP4 video file, created using the **create_mp4_video_from_frames** function, is included into an HTML video tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3PbO71QPuub"
      },
      "source": [
        "vide_file_name=create_mp4_video_from_frames(frames,30)\n",
        "\n",
        "mp4 = open(vide_file_name,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\" <video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl_p_Fp7atuw"
      },
      "source": [
        "# **Deep Q-network to solve the Atari Breakout environment**\n",
        "In this section, a DQN model is trained to solve the [**Atari Breakout**](https://www.gymlibrary.ml/environments/atari/breakout/) environment provided by Gym."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MntVGgoESaX8"
      },
      "source": [
        "## **Atari Breakout environment**\n",
        "Breakout is an arcade game developed and published by Atari. In Breakout there are eight rows of bricks on the top of the screen and the player must knock down as many bricks as possible using a paddle, located at the bottom of the screen, to bounce a ball against them.\n",
        "\n",
        "In this environment, the observation is an RGB image of the screen with a shape of (210, 160, 3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgf_MmfNxDLy"
      },
      "source": [
        "### **Creation**\n",
        "The following code creates the Atari Breakout environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGQQm_wjHB_4"
      },
      "source": [
        "env=gym.make('BreakoutDeterministic-v4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hloR2FnFKfTU"
      },
      "source": [
        "### **Visualization**\n",
        "To visualize the current state of the environment, the **render** method can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBvBNCjBKmfw"
      },
      "source": [
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.axis('off')\n",
        "plt.imshow(prev_screen)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDaOS8LQxtxP"
      },
      "source": [
        "### **State space**\n",
        "The state returned by the environment is an RGB image of $210\\times 160$ pixels as reported by the **shape** property of the corresponding **observation_space** attribute. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOiikVWIHO4_"
      },
      "source": [
        "print(\"The shape of the state space is: \", env.observation_space.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh10XCXXxv4n"
      },
      "source": [
        "### **Action space**\n",
        "The action space is composed by four possible actions:\n",
        "- noop;\n",
        "- fire;\n",
        "- right;\n",
        "- left.\n",
        "\n",
        "The following code uses the **n** property of the action space to print the number of valid actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5j8g8W5xwxA"
      },
      "source": [
        "print('There are {} possible actions'.format(env.action_space.n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3U0UE2ZTfq1"
      },
      "source": [
        "## **Exercise 3: model definition**\n",
        "Implement the following function to create a DQN model given:\n",
        "- the shape of the input images (*input_shape*);\n",
        "- the number of valid actions (*action_count*).\n",
        "\n",
        "The following image shows the architecture of DQN.\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/RL/DQN_architecture.png width=\"1200\">\n",
        "\n",
        "Keras offers a wide range of built-in layers ready for use, including:\n",
        "- [**Conv2D**](https://keras.io/api/layers/convolution_layers/convolution2d/) - a 2D convolution layer;\n",
        "- [**Flatten**](https://keras.io/api/layers/reshaping_layers/flatten/) - a simple layer used to flatten the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgaPWe1Ab868"
      },
      "source": [
        "def build_dqn(input_shape=(84, 84, 4),action_count=9):\n",
        "    #..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrATMdU9TAOO"
      },
      "source": [
        "## **Action and target models creation**\n",
        "The following code creates the action and target models by calling the **build_dqn** function defined above. To be identical, the two models need to share initial random weights: **get_weights** and **set_weights** methods are used to replace target weights to those of the action model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqwLRHkbTIf9"
      },
      "source": [
        "dqn_action_model=build_dqn(action_count=env.action_space.n)\n",
        "dqn_target_model=build_dqn(action_count=env.action_space.n)\n",
        "dqn_target_model.set_weights(dqn_action_model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkhNDJhoUbif"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtWuwDhsUe8J"
      },
      "source": [
        "dqn_action_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_vPXaLfUizn"
      },
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBAaeEOKUlEv"
      },
      "source": [
        "keras.utils.plot_model(dqn_action_model,show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iPucjXnVGVA"
      },
      "source": [
        "## **Action model compilation**\n",
        "The following code compiles the DQN model using the **compile** method.\n",
        "\n",
        "Even if in the Deepmind paper the *RMSProp* algorithm has been used as optimizer, the *Adam* optimizer has been demonstrated to improve the training time.\n",
        "\n",
        "Moreover, to increase the training stability, the *Huber* loss function is used instead of the *mean squared error* (MSE).\n",
        "\n",
        "<u>Note that, only the action model is compiled because, during the training process, only its weights are updated using *gradient descent* algorithm. Target model weights are periodically replaced by those of the action model.</u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-rwCSrrVGlU"
      },
      "source": [
        "learning_rate=0.00025\n",
        "\n",
        "optimizer=keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
        "dqn_action_model.compile(optimizer=optimizer, loss = 'huber') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrcF-baLSPbG"
      },
      "source": [
        "## **Input preprocessing**\n",
        "To reduce the state complexity, and consequently the computation time, each frame is:\n",
        "1. transformed in grayscale;\n",
        "2. cropped to select the region of interest;\n",
        "3. resized to 84×84.\n",
        "\n",
        "Moreover, to reduce the memory occupation of the replay buffer, pixel values are stored as bytes (in the range [0;255]) and converted in floating-point values (in the range [0;1]) only when needed as input for the *action* or *target* models.\n",
        "\n",
        "The following function performs such operations, given:\n",
        "- the input frame (*frame*); \n",
        "- the coordinates of the region of interest (*top_crop*, *bottom_crop*, *left_crop* and *right_crop*);\n",
        "- the dimension of the resided frame (*resized_shape*).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_DJNtIoSUOT"
      },
      "source": [
        "def preprocess_frame(frame,top_crop,bottom_crop,left_crop,right_crop,resized_shape):\n",
        "    # 1. the input RGB frame is transformed in grayscale\n",
        "    gray = rgb2gray(frame)\n",
        "        \n",
        "    # 2. the region of interest is cropped\n",
        "    cropped_frame = gray[top_crop:bottom_crop,left_crop:right_crop]\n",
        "    \n",
        "    # 3. the resulting images is resized\n",
        "    preprocessed_frame = transform.resize(cropped_frame, resized_shape)\n",
        "    \n",
        "    # convert float to byte\n",
        "    byte_preprocessed_frame=(preprocessed_frame*255).astype('uint8')\n",
        "\n",
        "    return byte_preprocessed_frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEoHE8INKS9n"
      },
      "source": [
        "The code below shows an example of input frame and the result obtained by the **preprocess_frame** function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgtKg4cCKTxf"
      },
      "source": [
        "top_crop = 30             # y coordinate of the top of the region of interest\n",
        "bottom_crop = 195         # y coordinate of the bottom of the region of interest\n",
        "left_crop = 5             # x coordinate of the left of the region of interest\n",
        "right_crop = 155          # x coordinate of the right of the region of interest\n",
        "resized_shape = (84,84)   # shape of the resized frame\n",
        "\n",
        "preprocessed_prev_frame=preprocess_frame(prev_screen,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "\n",
        "dpi=50\n",
        "\n",
        "plt.figure(figsize=(prev_screen.shape[1]/dpi,prev_screen.shape[0]/dpi))\n",
        "plt.axis('off')\n",
        "plt.imshow(prev_screen)\n",
        "plt.show()\n",
        "\n",
        "plt.axis('off')\n",
        "plt.figure(figsize=(preprocessed_prev_frame.shape[1]/dpi,preprocessed_prev_frame.shape[0]/dpi))\n",
        "plt.axis('off')\n",
        "plt.imshow(preprocessed_prev_frame.squeeze(),cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STNUjCIUTCnu"
      },
      "source": [
        "## **Frame stacking**\n",
        "To solve the problem of temporal limitation and give the network the sense of motion, DQN takes a stack of frames as input.\n",
        "\n",
        "The following function stacks frames together given:\n",
        "- the new frame to add (*new_frame*);\n",
        "- the number of frames to stack (*frame_count*);\n",
        "- the previous frames contained in a **deque** object (*deque_frames*).\n",
        "\n",
        "For the first *new_frame*, *frame_count* frames identical to *new_frame* are added to a new **deque** object. Otherwise, the *new_frame* is appended to the deque that automatically removes the oldest frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D8rNXF5VvKv"
      },
      "source": [
        "def stack_frames(new_frame,frame_count,deque_frames=None):\n",
        "  if deque_frames is None:\n",
        "    deque_frames = deque([new_frame for i in range(frame_count)],maxlen=frame_count)\n",
        "  else:\n",
        "    deque_frames.append(new_frame)\n",
        "\n",
        "  return np.stack(deque_frames, axis=2),deque_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXCws76FVuY_"
      },
      "source": [
        "The code below shows the result obtained by the **stack_frame** funciton with an initial frame as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYB3pBxunKSx"
      },
      "source": [
        "stacked_frame_count=4     # number of stacked frames\n",
        "\n",
        "stacked_frames,deque_frames=stack_frames(preprocessed_prev_frame,stacked_frame_count)\n",
        "\n",
        "print('Shape of the stacked frames',stacked_frames.shape)\n",
        "\n",
        "plot_stacked_frames(stacked_frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NpnB3MYgE_"
      },
      "source": [
        "The following code shows an example of stacked frames obtained performing random actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8H4ZIJvp7my"
      },
      "source": [
        "step_count=4\n",
        "\n",
        "for i in range(step_count):\n",
        "  new_state,_,_,_=env.step(1 if i==0 else env.action_space.sample())  # the first action is 'fire' to start the game\n",
        "  new_frame=preprocess_frame(new_state,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "  stacked_frames,deque_frames=stack_frames(new_frame,stacked_frame_count,deque_frames)\n",
        "  plot_stacked_frames(stacked_frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIWDG-PDSdGS"
      },
      "source": [
        "## **Redefinition of some functions**\n",
        "Since now the state of the environment is represented by a stack of preprocessed frames, some functions previously defined need to be modified accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQP8NaeGMkeY"
      },
      "source": [
        "### **Replay memory initialization**\n",
        "The following function initializes the replay memory given:\n",
        "- the environment (*env*);\n",
        "- the replay memory (*replay_memory*);\n",
        "- the number of transitions to be stored in the replay memory (*replay_memory_init_size*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*);\n",
        "- the region of interest coordinates (*top_crop*, *bottom_crop*, *left_crop* and *right_crop*);\n",
        "- the dimension of the resided frames (*resized_shape*);\n",
        "- the number of stacked frames (*stacked_frame_count*) as input parameters.\n",
        "\n",
        "Differently from **simple_dqn_replay_memory_init** function defined above, **dqn_replay_memory_init** uses the just defined **preprocess_frame** and **stack_frames** functions to create a stack of preprocessed frames to be stored into the replay memory from the states returned by the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTADuafxN0nA"
      },
      "source": [
        "def dqn_replay_memory_init(env,replay_memory,replay_memory_init_size,episode_max_steps,\n",
        "                           top_crop,bottom_crop,left_crop,right_crop,resized_shape,stacked_frame_count):\n",
        "    while True:\n",
        "        state = env.reset()\n",
        "\n",
        "        # the state is preprocessed to obtain the initial frame\n",
        "        frame = preprocess_frame(state,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "        \n",
        "        # a new stack of frames is created from the initial frame \n",
        "        stacked_frames,deque_frames = stack_frames(frame,stacked_frame_count)\n",
        "\n",
        "        done=False\n",
        "        step_count=0\n",
        "        while (step_count<episode_max_steps) and (not done):\n",
        "            action = env.action_space.sample()\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # the new frame is obtained by new state preprocessing\n",
        "            new_frame = preprocess_frame(new_state,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "            \n",
        "            # the stack is updated by adding the new frame and removing the oldest one\n",
        "            new_stacked_frames,deque_frames = stack_frames(new_frame,stacked_frame_count,deque_frames)\n",
        "\n",
        "            replay_memory.append([stacked_frames,action,reward,new_stacked_frames,done])\n",
        "            stacked_frames=new_stacked_frames\n",
        "            step_count+=1\n",
        "            if len(replay_memory)>=replay_memory_init_size:\n",
        "                return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RMnsTDC0E2J"
      },
      "source": [
        "### **Action model update**\n",
        "The following function updates action model weights using *gradient descent* algorithm given:\n",
        "- the action model (*dqn_action_model*);\n",
        "- the target model (*dqn_target_model*);\n",
        "- a mini-batch containing transitions $<s_i,a_i,r_{i+1},s_{i+1}>$ randomly selected from the replay memory (*mini_batch*); \n",
        "- the discount factor $\\gamma$ (gamma).\n",
        "\n",
        "With respect to **simple_dqn_update** function, here the pixel values contained in $s_i$ and $s_{i+1}$ (*state_batch* and *new_state_batch*, respectively) are normalized in the range $[0;1]$ before they being used as input of **predict** and **train_on_batch** methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG8D9vnK0l5L"
      },
      "source": [
        "def dqn_update(dqn_action_model,dqn_target_model,mini_batch,gamma):\n",
        "  # the transition mini-batch is divided into a mini-batch for each element of a transition\n",
        "  state_batch,action_batch,reward_batch,new_state_batch,done_batch=mini_batch\n",
        "\n",
        "  # pixel values are normalized in the range [0;1]\n",
        "  norm_state_batch=state_batch/255.0\n",
        "  norm_new_state_batch=new_state_batch/255.0\n",
        "\n",
        "  # find the target model Q values for all possible actions given the new state batch\n",
        "  target_new_state_q_values = dqn_target_model.predict(norm_new_state_batch)\n",
        "  \n",
        "  # find the action model Q values for all possible actions given the current state batch\n",
        "  predicted_state_q_values = dqn_action_model.predict(norm_state_batch)\n",
        "  \n",
        "  # estimate the target values y_i\n",
        "  # for the action we took, use the target model Q values  \n",
        "  # for other actions, use the action model Q values\n",
        "  # in this way, loss function will be 0 for other actions\n",
        "  for i,(a,r,new_state_q_values, done) in enumerate(zip(action_batch,reward_batch,target_new_state_q_values, done_batch)): \n",
        "      if not done:  \n",
        "        target_value = r + gamma * np.amax(new_state_q_values)\n",
        "      else:         \n",
        "        target_value = r\n",
        "      predicted_state_q_values[i][a] = target_value #y_i\n",
        "  \n",
        "  # update weights of action model using the train_on_batch method \n",
        "  dqn_action_model.train_on_batch(norm_state_batch, predicted_state_q_values)\n",
        "  \n",
        "  # return the updated action model\n",
        "  return dqn_action_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB9GeVcUX0Eb"
      },
      "source": [
        "## **Exercise 4: Atari DQN training algorithm**\n",
        "Implement the following function to train an Atari DQN model given:\n",
        "- the environment (*env*);\n",
        "- the action model (*dqn_action_model*);\n",
        "- the target model (*dqn_target_model*);\n",
        "- the number of episodes (*episode_count*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*);\n",
        "- the maximum number of transitions stored into the replay memory (*replay_memory_max_size*);\n",
        "- the number of transitions stored into the replay memory before starting the training process (*replay_memory_init_size*);\n",
        "- the mini-batch size (*batch_size*);\n",
        "- the number ($u$) of total steps executed ($T$) between successive updates of the action model weights (*step_per_update*);\n",
        "- the number ($c$) of total steps executed ($T$) between successive replaces of the target model weights with the weights of the action model (*step_per_update_target_model*); \n",
        "- the maximum value of $\\epsilon$ (*max_epsilon*);\n",
        "- the minimum value of $\\epsilon$ (*min_epsilon*);\n",
        "- the $\\epsilon$ decay value (*epsilon_decay*);\n",
        "- the discount factor $\\gamma$ (gamma);\n",
        "- the coordinates of the region of interest (*top_crop*, *bottom_crop*, *left_crop* and *right_crop*);\n",
        "- the dimension of the resided frames (*resized_shape*);\n",
        "- the number of stacked frames (*stacked_frame_count*);\n",
        "- the number of consecutive episodes to be considered in the calculation of the total reward moving average (*moving_avg_window_size*);\n",
        "- the minimum value of the total reward moving average to consider the task solved (*moving_avg_stop_thr*). A value of *None* force to execute all *episode_count* episodes.\n",
        "\n",
        "It could be useful to start from **simple_dqn_training** function defined above.\n",
        "\n",
        "The following parts need to be modified with respect to **simple_dqn_training** function:\n",
        "1. call the **dqn_replay_memory_init** function to populate the replay memory before starting the training process;\n",
        "2. at the beginning of a new episode, use the **preprocess_frame** function to elaborate the initial state returned by the **reset** method of the environment (*env*) to obtain the first frame;\n",
        "3. use the **stack_frames** function to create a new stack of frames initialized with the first frame; \n",
        "4. before using the **predict** method of the action model to obtain the Q-values of each available actions given the current state ($s_t$), the stacked frames need to be normalized in the range $[0;1]$;\n",
        "5. after the selected action $a_t$ has been executed, call the **preprocess_frame** function to obtain the new frame from the new state $s_{t+1}$;\n",
        "6. use the **stack_frames** function to update the stack of frames by adding the new frame and removing the oldest one;\n",
        "7. if the action model weights need to be updated, call the **dqn_update** function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ9x2wKDdR4E"
      },
      "source": [
        "def dqn_training(env,dqn_action_model,dqn_target_model,episode_count,episode_max_steps,replay_memory_max_size,replay_memory_init_size,\n",
        "                 batch_size,step_per_update,step_per_update_target_model,max_epsilon,min_epsilon,epsilon_decay,gamma,\n",
        "                 top_crop,bottom_crop,left_crop,right_crop,resized_shape,stacked_frame_count,\n",
        "                 moving_avg_window_size=100,moving_avg_stop_thr=None):\n",
        "    #..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyqtTc91CUJ2"
      },
      "source": [
        "## **Training**\n",
        "Now we are ready to start the training process by calling the **dqn_learning** function.\n",
        "\n",
        "The following parameters must be set before being passed to the function:\n",
        "- the number of episodes (*episode_count*);\n",
        "- the maximum number of steps per episode (*episode_max_steps*);\n",
        "- the maximum number of transitions stored into the replay memory (*replay_memory_max_size*);\n",
        "- the number of transitions stored into the replay memory before starting the training process (*replay_memory_init_size*);\n",
        "- the mini-batch size (*batch_size*);\n",
        "- the number of total steps executed between successive updates of the action model weights (*step_per_update*);\n",
        "- the number of total steps executed between successive replaces of the target model weights with the weights of the action model (*step_per_update_target_model*); \n",
        "- the maximum value of $\\epsilon$ (*max_epsilon*);\n",
        "- the minimum value of $\\epsilon$ (*min_epsilon*);\n",
        "- the $\\epsilon$ decay value (*epsilon_decay*);\n",
        "- the discount factor $\\gamma$ (gamma);\n",
        "- the coordinates of the region of interest (*top_crop*, *bottom_crop*, *left_crop* and *right_crop*);\n",
        "- the dimension of the resided frame (*resized_shape*);\n",
        "- the number of stacked frames (*stacked_frame_count*);\n",
        "- the number of consecutive episodes to be considered in the calculation of the total reward moving average (*moving_avg_window_size*);\n",
        "- the minimum value of the total reward moving average to consider the task solved (*moving_avg_stop_thr*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mk2Cb-BCjNf"
      },
      "source": [
        "episode_count = 10                  # Total number of training episodes\n",
        "episode_max_steps= 10000            # Maximum number of steps per episode\n",
        "\n",
        "replay_memory_max_size = 100000     # The maximum number of transitions stored into the replay memory. The Deepmind paper suggests 1M however this may cause memory issues.\n",
        "replay_memory_init_size= 5000       # The maximum number of transitions stored into the replay memory. The Deepmind paper suggests 50K.\n",
        "batch_size = 32                     # The mini-batch size\n",
        "\n",
        "step_per_update = 4                 # The number of total steps executed between successive updates of the action model weights\n",
        "step_per_update_target_model=10000  # The number of total steps executed between successive replaces of the target model weights\n",
        "\n",
        "max_epsilon=1.0                     # Exploration probability at start\n",
        "min_epsilon=0.1                     # Minimum exploration probability\n",
        "epsilon_decay=(max_epsilon-min_epsilon) / 1000000.0  # Decay for exploration probability\n",
        "\n",
        "gamma = 0.99                        # Discount factor\n",
        "\n",
        "top_crop = 30                       # y coordinate of the top of the region of interest\n",
        "bottom_crop = 195                   # y coordinate of the bottom of the region of interest\n",
        "left_crop = 5                       # x coordinate of the left of the region of interest\n",
        "right_crop = 155                    # x coordinate of the right of the region of interest\n",
        "resized_shape = (84,84)             # shape of the resized frame\n",
        "stacked_frame_count=4               # number of stacked frames\n",
        "\n",
        "moving_avg_window_size=100          # Number of consecutive episodes to be considered in the calculation of the total reward moving average\n",
        "moving_avg_stop_thr=60              # Minimum value of the total reward moving average to consider the task solved\n",
        "\n",
        "train_start_time=time.time()\n",
        "train_rewards=dqn_training(env,\n",
        "                          dqn_action_model,\n",
        "                          dqn_target_model,\n",
        "                          episode_count,\n",
        "                          episode_max_steps,\n",
        "                          replay_memory_max_size,\n",
        "                          replay_memory_init_size,\n",
        "                          batch_size,\n",
        "                          step_per_update,\n",
        "                          step_per_update_target_model,\n",
        "                          max_epsilon,\n",
        "                          min_epsilon,\n",
        "                          epsilon_decay,\n",
        "                          gamma,\n",
        "                          top_crop,\n",
        "                          bottom_crop,\n",
        "                          left_crop,\n",
        "                          right_crop,\n",
        "                          resized_shape,\n",
        "                          stacked_frame_count,\n",
        "                          moving_avg_window_size,\n",
        "                          moving_avg_stop_thr)\n",
        "\n",
        "train_finish_time=time.time()\n",
        "train_elapsed_time=train_finish_time-train_start_time\n",
        "train_avg_episode_time=train_elapsed_time/episode_count\n",
        "print(\"Train time: {:.1f}m [{:.1f}s]\".format(train_elapsed_time/60.0,train_avg_episode_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da7_yO-cICdz"
      },
      "source": [
        "### **Visualize the training process**\n",
        "The following code uses the **plot_training_rewards** function to draw in a graph the total reward and its moving average reached during the different episodes of the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRJ1mloDIOmJ"
      },
      "source": [
        "plot_training_rewards(train_rewards,moving_avg_window_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeDH-WMOTlAI"
      },
      "source": [
        "## **Performance evaluation**\n",
        "The Deepmind original DQN model has been trained for a total of 50 million frames (about 38 days of game experience).\n",
        "\n",
        "Unfortunately, we do not have neither enough time nor hardware resources on *Colab* to train such model.\n",
        "\n",
        "In this section a DQN model trained for more than 10 million frames (22751 episodes) in about 11 days is evaluated.\n",
        "\n",
        "The training process has been performed on an *Intel Xeon W-2133 CPU @3.60GHz 32GB of RAM with an NVIDIA GeForce GTX 1080 Ti GPU* changing the following hyper-parameters:\n",
        "- the number of episodes (*episode_count=50000*);\n",
        "- the maximum number of transitions stored into the replay memory (*replay_memory_max_size=250000*);\n",
        "- the number of transitions stored into the replay memory before starting the training process (*replay_memory_init_size=50000*).\n",
        "\n",
        "To evaluate the model improvement during the training phase, its weights have been saved after 5000 and 15000 episodes.\n",
        "\n",
        "The graph below shows the total reward trend over the entire training process.\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/RL/BreakoutDeterministic-v4_DQNTrainingRewards.png width=\"1200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJRisa3SAyl6"
      },
      "source": [
        "### **Model trained for 5000 episodes**\n",
        "The DQN model weights obtained after 5000 episodes (about 1.9 million frames) can be loaded by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fATtD5mySag7"
      },
      "source": [
        "dqn_model=build_dqn(action_count=env.action_space.n)\n",
        "dqn_model.load_weights('BreakoutDeterministic-v4_DQN_Weights_5000ep.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBO2ZcEvIkIb"
      },
      "source": [
        "The following code executes an episode using the pre-trained DQN network and stores all frames into a list (*visualization_frames*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2BtbUVUIvRi"
      },
      "source": [
        "eval_max_step=1500\n",
        "\n",
        "state=env.reset()\n",
        "frame = preprocess_frame(state,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "stacked_frames,deque_frames = stack_frames(frame,stacked_frame_count)\n",
        "\n",
        "visualization_frames=[]\n",
        "episode_reward=0\n",
        "step_count=0\n",
        "done=False\n",
        "while step_count<eval_max_step and not done:\n",
        "    visualization_frame = env.render(mode='rgb_array')\n",
        "    visualization_frames.append(visualization_frame)\n",
        "    \n",
        "    q_values = dqn_model.predict(stacked_frames[np.newaxis]/255.0)\n",
        "    action = np.argmax(q_values)\n",
        "    \n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    \n",
        "    new_frame = preprocess_frame(new_state,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "    new_stacked_frames,deque_frames = stack_frames(new_frame,stacked_frame_count,deque_frames)\n",
        "    stacked_frames=new_stacked_frames\n",
        "\n",
        "    episode_reward+=reward\n",
        "    step_count+=1\n",
        "    \n",
        "print('Steps: {} Total reward: {}'.format(step_count,episode_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuDyuw70JrRE"
      },
      "source": [
        "Then the MP4 video file, created using the **create_mp4_video_from_frames** function, is included into an HTML video tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teCaGzcHJvsZ"
      },
      "source": [
        "vide_file_name=create_mp4_video_from_frames(visualization_frames,30)\n",
        "\n",
        "mp4 = open(vide_file_name,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\" <video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHVwxYUuMMpi"
      },
      "source": [
        "The agent is able to follow and hit the ball but without a specific game strategy. Moreover, when the ball increases its speed, the agent is no longer able to hit it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzrLw3BsQDM5"
      },
      "source": [
        "### **Model trained for 15000 episodes**\n",
        "The DQN model weights obtained after 15000 episodes (about 6.8 million frames) can be loaded by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlPmStCzQNmD"
      },
      "source": [
        "dqn_model=build_dqn(action_count=env.action_space.n)\n",
        "dqn_model.load_weights('BreakoutDeterministic-v4_DQN_Weights_15000ep.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIeTq7njR6A5"
      },
      "source": [
        "The following code executes an episode using the pre-trained DQN network and stores all frames into a list (*visualization_frames*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqGAf_raR6qd"
      },
      "source": [
        "eval_max_step=500\n",
        "\n",
        "state=env.reset()\n",
        "frame = preprocess_frame(state,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "stacked_frames,deque_frames = stack_frames(frame,stacked_frame_count)\n",
        "\n",
        "visualization_frames=[]\n",
        "episode_reward=0\n",
        "step_count=0\n",
        "done=False\n",
        "while step_count<eval_max_step and not done:\n",
        "    visualization_frame = env.render(mode='rgb_array')\n",
        "    visualization_frames.append(visualization_frame)\n",
        "    \n",
        "    q_values = dqn_model.predict(stacked_frames[np.newaxis]/255.0)\n",
        "    action = np.argmax(q_values)\n",
        "    \n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    \n",
        "    new_frame = preprocess_frame(new_state,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "    new_stacked_frames,deque_frames = stack_frames(new_frame,stacked_frame_count,deque_frames)\n",
        "    stacked_frames=new_stacked_frames\n",
        "\n",
        "    episode_reward+=reward\n",
        "    step_count+=1\n",
        "    \n",
        "print('Steps: {} Total reward: {}'.format(step_count,episode_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVbAnpO3R_Fk"
      },
      "source": [
        "Then the MP4 video file, created using the **create_mp4_video_from_frames** function, is included into an HTML video tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU06W3xPSDX-"
      },
      "source": [
        "vide_file_name=create_mp4_video_from_frames(visualization_frames,30)\n",
        "\n",
        "mp4 = open(vide_file_name,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\" <video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN9KQyRkT839"
      },
      "source": [
        "The agent is able to follow and hit the ball but without a specific game strategy. \n",
        "\n",
        "Moreover, to prevent to lose the game, the agent has learned to not pull the last ball (*fire*). To avoid this behavior, we can force the agent to pull the ball at the beginning of each new round using the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MslFkW8dV4Y8"
      },
      "source": [
        "state=env.reset()\n",
        "frame = preprocess_frame(state,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "stacked_frames,deque_frames = stack_frames(frame,stacked_frame_count)\n",
        "\n",
        "visualization_frames=[]\n",
        "episode_reward=0\n",
        "step_count=0\n",
        "prev_lives_count=6\n",
        "cur_lives_count=5\n",
        "done=False\n",
        "while True and not done:\n",
        "    visualization_frame = env.render(mode='rgb_array')\n",
        "    visualization_frames.append(visualization_frame)\n",
        "    \n",
        "    if prev_lives_count>cur_lives_count:\n",
        "      action=1\n",
        "    else:\n",
        "      q_values = dqn_model.predict(stacked_frames[np.newaxis]/255.0)\n",
        "      action = np.argmax(q_values)\n",
        "    \n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    \n",
        "    new_frame = preprocess_frame(new_state,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "    new_stacked_frames,deque_frames = stack_frames(new_frame,stacked_frame_count,deque_frames)\n",
        "    stacked_frames=new_stacked_frames\n",
        "\n",
        "    prev_lives_count=cur_lives_count\n",
        "    cur_lives_count=info['ale.lives']\n",
        "\n",
        "    episode_reward+=reward\n",
        "    step_count+=1\n",
        "    \n",
        "print('Steps: {} Total reward: {}'.format(step_count,episode_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6CS9FwBYtVW"
      },
      "source": [
        "Then the MP4 video file, created using the **create_mp4_video_from_frames** function, is included into an HTML video tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvhbpJLEYvdk"
      },
      "source": [
        "vide_file_name=create_mp4_video_from_frames(visualization_frames,30)\n",
        "\n",
        "mp4 = open(vide_file_name,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\" <video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blzlKukvbdOj"
      },
      "source": [
        "### **Final model**\n",
        "The DQN model weights obtained at the end of the training phase can be loaded by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYWvAv8IdYVd"
      },
      "source": [
        "dqn_model=build_dqn(action_count=env.action_space.n)\n",
        "dqn_model.load_weights('BreakoutDeterministic-v4_DQN_Weights_22751ep.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx-FULA4db28"
      },
      "source": [
        "The following code executes an episode using the pre-trained DQN network and stores all frames into a list (*visualization_frames*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5yBr-4ddgFk"
      },
      "source": [
        "state=env.reset()\n",
        "frame = preprocess_frame(state,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "stacked_frames,deque_frames = stack_frames(frame,stacked_frame_count)\n",
        "\n",
        "visualization_frames=[]\n",
        "episode_reward=0\n",
        "step_count=0\n",
        "prev_lives_count=6\n",
        "cur_lives_count=5\n",
        "done=False\n",
        "while True and not done:\n",
        "    visualization_frame = env.render(mode='rgb_array')\n",
        "    visualization_frames.append(visualization_frame)\n",
        "    \n",
        "    if prev_lives_count>cur_lives_count:\n",
        "      action=1\n",
        "    else:\n",
        "      q_values = dqn_model.predict(stacked_frames[np.newaxis]/255.0)\n",
        "      action = np.argmax(q_values)\n",
        "    \n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    \n",
        "    new_frame = preprocess_frame(new_state,top_crop,bottom_crop,left_crop,right_crop,resized_shape)\n",
        "    new_stacked_frames,deque_frames = stack_frames(new_frame,stacked_frame_count,deque_frames)\n",
        "    stacked_frames=new_stacked_frames\n",
        "\n",
        "    prev_lives_count=cur_lives_count\n",
        "    cur_lives_count=info['ale.lives']\n",
        "\n",
        "    episode_reward+=reward\n",
        "    step_count+=1\n",
        "    \n",
        "print('Steps: {} Total reward: {}'.format(step_count,episode_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fx4JxaVdj1c"
      },
      "source": [
        "Then the MP4 video file, created using the **create_mp4_video_from_frames** function, is included into an HTML video tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzs5jjrmdmOd"
      },
      "source": [
        "vide_file_name=create_mp4_video_from_frames(visualization_frames,30)\n",
        "\n",
        "mp4 = open(vide_file_name,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\" <video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiE-IHMmdosD"
      },
      "source": [
        "Differently from previous models, this one has learned a precise game strategy: its objective is to to dig a tunnel and hit the blocks from above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAzUF0r7Y1hD"
      },
      "source": [
        "# **Exercise 5**\n",
        "Solve another Gym environment chosen from the following list:\n",
        "- [Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/);\n",
        "- [Pendulum](https://www.gymlibrary.ml/environments/classic_control/pendulum/);\n",
        "- [Lunar Lander](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)."
      ]
    }
  ]
}