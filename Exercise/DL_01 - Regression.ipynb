{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_01 - Regression.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0QulwZ6jcWH"
      },
      "source": [
        "# **Regression tutorial**\n",
        "In today's tutorial we will design and train deep neural networks to solve a regression problem.\n",
        "\n",
        "We will use [**TensorFlow**](https://ekababisong.org/gcp-ml-seminar/tensorflow/) framework and [**Keras**](https://keras.io/) open-source library to rapidly prototype deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFQKipVFoCXn"
      },
      "source": [
        "# **Preliminary operations**\n",
        "The following code downloads all the necessary material into the remote machine. At the end of the execution select the **File** tab to verify that everything has been correctly downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8dIUj7Ag3wz"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00501/PRSA2017_Data_20130301-20170228.zip\n",
        "\n",
        "!unzip PRSA2017_Data_20130301-20170228.zip\n",
        "\n",
        "!rm PRSA2017_Data_20130301-20170228.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MKM4GTUnSvY"
      },
      "source": [
        "# **Useful modules import**\n",
        "First of all, it is necessary to import useful modules used during the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf-bThTxjbvf"
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7N8j2YIpZlc"
      },
      "source": [
        "# **Utility functions**\n",
        "Execute the following code to define some utility functions used in the tutorial:\n",
        "- **plot_history** draws in a graph the loss trend over epochs on both training and validation sets. Moreover, if provided, it draws in the same graph also the trend of the given metric;\n",
        "- **plot_prediction_results** plots the predicted and the true values and visualizes the error distribution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqOZDIbuOpMG"
      },
      "source": [
        "def plot_history(history,metric=None):\n",
        "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "  epoch_count=len(history.history['loss'])\n",
        "\n",
        "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],label='train_loss',color='orange')\n",
        "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],label='val_loss',color = line1.get_color(), linestyle = '--')\n",
        "  ax1.set_xlim([1,epoch_count])\n",
        "  ax1.set_ylim([0, max(max(history.history['loss']),max(history.history['val_loss']))])\n",
        "  ax1.set_ylabel('loss',color = line1.get_color())\n",
        "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
        "  ax1.set_xlabel('Epochs')\n",
        "  _=ax1.legend(loc='lower left')\n",
        "\n",
        "  if (metric!=None):\n",
        "    ax2 = ax1.twinx()\n",
        "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],label='train_'+metric)\n",
        "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],label='val_'+metric,color = line2.get_color(), linestyle = '--')\n",
        "    ax2.set_ylim([0, max(max(history.history[metric]),max(history.history['val_'+metric]))])\n",
        "    ax2.set_ylabel(metric,color=line2.get_color())\n",
        "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
        "    _=ax2.legend(loc='upper right')\n",
        "\n",
        "def plot_prediction_results(y,y_pred,output_labels,bin_count=50):\n",
        "  fig, axs = plt.subplots(2,len(output_labels),figsize=(25, 10),squeeze=False)\n",
        "  \n",
        "  for i in range(len(output_labels)):\n",
        "    axs[0,i].set_title(output_labels[i])\n",
        "    axs[0,i].scatter(y[:,i], y_pred[:,i],s=1)\n",
        "    axs[0,i].set_xlabel('True Values')\n",
        "    if i==0:\n",
        "      axs[0,i].set_ylabel('Predictions')\n",
        "    max_value=max(max(y[:,i]),max(y_pred[:,i]))\n",
        "    x_lims = [0, max_value]\n",
        "    y_lims = [min(0,min(y[:,i]),min(y_pred[:,i])), max_value]\n",
        "    axs[0,i].set_xlim(x_lims)\n",
        "    axs[0,i].set_ylim(y_lims)\n",
        "    axs[0,i].plot(y_lims, y_lims, color='k')\n",
        "\n",
        "    errors = y[:,i]-y_pred[:,i]\n",
        "    axs[1,i].hist(errors, bins=bin_count)\n",
        "    axs[1,i].set_xlabel('Prediction Error')\n",
        "    if i==0:\n",
        "      axs[1,i].set_ylabel('Count')\n",
        "    axs[1,i].set_xlim([min(errors),max(errors)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5GWgqpvt_gG"
      },
      "source": [
        "# **Dataset**\n",
        "This tutorial uses the [Beijing Multi-Site Air-Quality Data Data Set](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data) maintained by the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), a public repository containing hundreds of databases useful for the machine learning community.\n",
        "\n",
        "The data set includes hourly air pollutants data from 12 nationally-controlled air-quality monitoring sites of the Beijing municipal environmental monitoring center. It contains 420768 istances with 18 attributes:\n",
        "- *No*: row number\n",
        "- *year*: year of data\n",
        "- *month*: month of data\n",
        "- *day*: day of data\n",
        "- *hour*: hour of data\n",
        "- *PM2.5*: PM2.5 concentration (ug/m^3)\n",
        "- *PM10*: PM10 concentration (ug/m^3)\n",
        "- *SO2*: SO2 concentration (ug/m^3)\n",
        "- *NO2*: NO2 concentration (ug/m^3)\n",
        "- *CO*: CO concentration (ug/m^3)\n",
        "- *O3*: O3 concentration (ug/m^3)\n",
        "- *TEMP*: temperature (degree Celsius)\n",
        "- *PRES*: pressure (hPa)\n",
        "- *DEWP*: dew point temperature (degree Celsius)\n",
        "- *RAIN*: precipitation (mm)\n",
        "- *wd*: wind direction\n",
        "- *WSPM*: wind speed (m/s)\n",
        "- *station*: name of the air-quality monitoring site\n",
        "\n",
        "The dataset is stored in multiple CSV files and can be easily loaded in memory using [**pandas**](https://pandas.pydata.org/), a software library for data manipulation and analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Ao2ziG5mC4"
      },
      "source": [
        "li = []\n",
        "for filename in glob.glob('PRSA_Data_20130301-20170228' + \"/*.csv\"):\n",
        "    df = pd.read_csv(filename, index_col=None, header=0)\n",
        "    li.append(df)\n",
        "\n",
        "dataframe = pd.concat(li, axis=0, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYQTjSp954XX"
      },
      "source": [
        "The variable *dataframe* is an instance of the pandas class [**DataFrame**](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html), a 2-dimensional labeled data structure with columns of potentially different types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcR6wwIL3RqR"
      },
      "source": [
        "## **Visualization**\n",
        "*row_count* randomly selected rows can be shown by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGUfWFgMGasz"
      },
      "source": [
        "row_count=5\n",
        "\n",
        "dataframe.sample(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCMpoCmI3iLg"
      },
      "source": [
        "## **Statistics**\n",
        "The [**info**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) method can be used to print a brief summary of a **DataFrame** including the index and the type of each column, the non-null values and the memory usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-QOWizMnYje"
      },
      "source": [
        "dataframe.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt1VZzIO9s3Y"
      },
      "source": [
        "To show the overall statistics of the dataset can be used the method [**describe**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTbvhN2Sni6M"
      },
      "source": [
        "dataframe.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OJPGfn2-b7_"
      },
      "source": [
        "From the statistics it is clear how each feature covers a very different range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SKLJxr4_IC1"
      },
      "source": [
        "The method [**hist**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html) draws a histogram for each column in the **DataFrame**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGmBM_kAnx9U"
      },
      "source": [
        "dataframe.hist(bins=50, figsize=(20,15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0NCqBGaANwJ"
      },
      "source": [
        "## **Data preparation**\n",
        "Most machine learning algorithms require data to be formatted in a specific way, so datasets generally require some amount of preparation before they can yield useful insights. Some datasets have values that are missing, invalid, or otherwise difficult for an algorithm to process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE0GnLRdOzre"
      },
      "source": [
        "### **Missing values**\n",
        "The dataset contains several missing values as reported by method [**isna**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF-a6wls6WPc"
      },
      "source": [
        "dataframe.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-ilPEmRAvNt"
      },
      "source": [
        "The simplest solution to missing values is to remove the corresponding rows. This can be done by calling the [**dropna**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0hmrIUm6gT0"
      },
      "source": [
        "prepared_dataframe = dataframe.copy()\n",
        "prepared_dataframe = prepared_dataframe.dropna()\n",
        "prepared_dataframe.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6HWPLRz3-U1"
      },
      "source": [
        "### **Encode cyclical data**\n",
        "Air pollution is strongly related to the time of the day (e.g., 9 A.M. or 10 P.M.) and the time of the year (e.g., January or August).\n",
        "\n",
        "The following code plots the *hour* column in a graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kWWGGX-6_Ld"
      },
      "source": [
        "plt.plot(prepared_dataframe['hour'][:130].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8QExNlG9-g6"
      },
      "source": [
        "The graph report the hourly data for a week: a cycle between 0 and 23 that repeats 7 times presenting a **jump discontinuity** at the end of each day, when the hour value goes from  23  to  00.\n",
        "\n",
        "Presenting cyclical data to a machine learning algorithm is a problem. For instance, it would consider the difference between 23 and 00 greater than that between 22 and 23.\n",
        "\n",
        "A common method for encoding cyclical data is to transform the data into two dimensions using a sine and cosine transformation.\n",
        "\n",
        "The hour sine and cosine values are computed and plotted by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eel8Xh0v-HD5"
      },
      "source": [
        "hour_sin = np.sin(2 * np.pi * prepared_dataframe['hour']/23.0)\n",
        "hour_cos = np.cos(2 * np.pi * prepared_dataframe['hour']/23.0)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.xlabel('hour_sin')\n",
        "plt.ylabel('hour_cos')\n",
        "plt.scatter(hour_sin,hour_cos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvXgaZBw-P3E"
      },
      "source": [
        "As expected, the hour information are encoded as a cycle.\n",
        "\n",
        "The following code adds the two new features (*hour_sin* and *hour_cos*) in the **DataFrame** as two new columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCtj61kY-dEt"
      },
      "source": [
        "prepared_dataframe['hour_sin']=hour_sin\n",
        "prepared_dataframe['hour_cos']=hour_cos\n",
        "\n",
        "prepared_dataframe.sample(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdMh30QQ_ufX"
      },
      "source": [
        "The same thing can be done with the *month* column by executing the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6E6rm_3AG-_"
      },
      "source": [
        "month_sin = np.sin(2 * np.pi * prepared_dataframe['month']/12.0)\n",
        "month_cos = np.cos(2 * np.pi * prepared_dataframe['month']/12.0)\n",
        "\n",
        "prepared_dataframe['month_sin']=month_sin\n",
        "prepared_dataframe['month_cos']=month_cos\n",
        "\n",
        "prepared_dataframe.sample(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biabBJvUOerQ"
      },
      "source": [
        "### **Remove unuseful columns**\n",
        "The *No*, *month* and *hour* columns contain no useful information. They can be removed from the dataset using the [**drop**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HvaxwAyOfdf"
      },
      "source": [
        "prepared_dataframe=prepared_dataframe.drop(['No','month','hour'],axis=1)\n",
        "prepared_dataframe.sample(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzRGSpgDmnb0"
      },
      "source": [
        "###**Convert categorical data**\n",
        "The *wd* and *station* columns are categorical, not numeric. Their conversion into numeric format can be done in two ways: \n",
        "- *label encoding*, converting each category to a number;\n",
        "- *one hot encoding*, converting each category value into a new column and assigns a 1 or 0 (True/False) value to the column. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkk9RdeHFjsM"
      },
      "source": [
        "**Label encoding**\n",
        "\n",
        "First of all, if the column type is *object* and not *category*, the [**astype**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html) method can be used to convert a column to a category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muvsXlYMraIb"
      },
      "source": [
        "label_enc_dataframe=prepared_dataframe.copy()\n",
        "\n",
        "label_enc_dataframe['wd'] = prepared_dataframe['wd'].astype('category')\n",
        "label_enc_dataframe['station'] = prepared_dataframe['station'].astype('category')\n",
        "label_enc_dataframe.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtN5AyXNH_SS"
      },
      "source": [
        "Then the encoded values can be assigned to the corresponding column using the **cat.codes** accessor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO5iIQIPov90"
      },
      "source": [
        "label_enc_dataframe['wd'] = label_enc_dataframe['wd'].cat.codes\n",
        "label_enc_dataframe['station'] = label_enc_dataframe['station'].cat.codes\n",
        "label_enc_dataframe.sample(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P24tT1uTJjar"
      },
      "source": [
        "Label encoding has the advantage that it is straightforward but it has the disadvantage that the numeric values can be “misinterpreted” by the algorithms. For example, the value of 0 is obviously less than the value of 4 but does that really correspond to reality (e.g., *station*)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qItYXkkKEeI"
      },
      "source": [
        "**One hot encoding**\n",
        "\n",
        "Pandas supports this feature using the [**get_dummies**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeB66D3Tsx0c"
      },
      "source": [
        "one_hot_enc_dataframe=pd.get_dummies(prepared_dataframe, columns=['wd', 'station'], prefix=['wd', 'station'])\n",
        "one_hot_enc_dataframe.sample(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h0fQaBDMBDI"
      },
      "source": [
        "One hot encoding has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set (it depends by the number of categories in a column)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNcMU8-3uy3h"
      },
      "source": [
        "**What is the best solution?**\n",
        "\n",
        "It depends on the specific dataset used.\n",
        "\n",
        "In this tutorial, because both *wd* and *station* columns contain categorical values without any numerical relation, it is better to use the *one hot encoding* solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKEVPtItOLwR"
      },
      "source": [
        "#used_dataframe=label_enc_dataframe\n",
        "used_dataframe=one_hot_enc_dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O20Kih1QRHE-"
      },
      "source": [
        "## **Split features from target values**\n",
        "\n",
        "The following code separates the target values (the concentration of air pollutants) from the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd7-Pr-KvPkp"
      },
      "source": [
        "target_data=['PM2.5','PM10','SO2','NO2','CO','O3']\n",
        "\n",
        "dataframe_x=used_dataframe.drop(target_data, axis=1)\n",
        "dataframe_y=used_dataframe[target_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0x_g7LneBPo"
      },
      "source": [
        "Some randomly selected feature rows can be shown by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gMQogtZLeHs"
      },
      "source": [
        "dataframe_x.sample(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfaOyfA5eGJH"
      },
      "source": [
        "Some randomly selected target rows can be shown by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLGEUzVzLg8E"
      },
      "source": [
        "dataframe_y.sample(row_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOwUaFCVc4Ic"
      },
      "source": [
        "The Numpy representation of the **DataFrame** can be obtained using the [**values**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html) property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nVOS2hi9G2M"
      },
      "source": [
        "x=dataframe_x.values\n",
        "y=dataframe_y.values\n",
        "\n",
        "print('Feature shape: ',x.shape)\n",
        "print('Target shape: ',y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkYJ1h1TQ1wm"
      },
      "source": [
        "## **Split data into training and test sets**\n",
        "To evaluate the generalization capabilites of the regression model, it is necessary to have a separate dataset (called test set) to use in the final evaluation of our model after the training process. \n",
        "\n",
        "For this reason, *x* is divided into two subsets: training and test sets. \n",
        "\n",
        "**Scikit-learn** library provides the function [**train_test_split**](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to separate a dataset into two parts.\n",
        "\n",
        "The *test_size* parameter represents the percentage (or the absolute number) of patterns to include in the test set.\n",
        "\n",
        "The *shuffle* parameter is used to mix patterns before splitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCEKzR8nwAqq"
      },
      "source": [
        "test_size=0.25\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = test_size,random_state = 1,shuffle=True)\n",
        "\n",
        "print('Train feature shape: ',train_x.shape)\n",
        "print('Train target shape: ',train_y.shape)\n",
        "print('Test feature shape: ',test_x.shape)\n",
        "print('Test target shape: ',test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVygaAuAYJuk"
      },
      "source": [
        "# **Linear regression**\n",
        "As a starting point, we will evaluate the performance of the least squares linear regression using the class [**LinearRegression**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) provided by Scikit-learn.\n",
        "\n",
        "The [**fit**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) method fits a linear model to minimize the residual sum of squares between the target values, and the values predicted by the linear approximation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVp4J31fHZ-Q"
      },
      "source": [
        "linear_model = LinearRegression().fit(train_x, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M6g7LkdahR1"
      },
      "source": [
        "## **Performance evaluation**\n",
        "The following code calls the [**predict**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) method to generate the predictions (*train_y_pred* and *test_y_pred*) of the training and test sets (*train_x* and *test_x*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLPpBodHHjxu"
      },
      "source": [
        "train_y_pred=linear_model.predict(train_x)\n",
        "test_y_pred=linear_model.predict(test_x)\n",
        "\n",
        "print('Train predictions shape: ',train_y_pred.shape)\n",
        "print('Test predictions shape: ',test_y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl1Mj0_hckOY"
      },
      "source": [
        "### **RMSE**\n",
        "The regression accuracy can be measured using the RMSE.\n",
        "\n",
        "Scikit-learn library provides the function [**mean_squared_error**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) to compute MSE and RMSE metrics.\n",
        "\n",
        "If the *squared* parameter is set to False, the function returns the RMSE value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmmY8hSXcTJx"
      },
      "source": [
        "rmse_train = mean_squared_error(train_y,train_y_pred,squared=False)\n",
        "rmse_test = mean_squared_error(test_y,test_y_pred,squared=False)\n",
        "\n",
        "print('RMSE - Train: {:.3f} Test: {:.3f}'.format(rmse_train,rmse_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWiMiv0NewwY"
      },
      "source": [
        "### **True vs predicted values and error distributions**\n",
        "To better analyze the model performance on the test set, it is useful to plot the predicted and the true values and to visualize the error distribution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4TsFsjcWnkq"
      },
      "source": [
        "plot_prediction_results(test_y,test_y_pred,target_data,200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBX_hqAiiC4N"
      },
      "source": [
        "### **Best and worst predictions**\n",
        "To select best and worst predictions the RMSE value for each test instance is computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_Y561IOi2pD"
      },
      "source": [
        "rmse_test_instances=np.sqrt(mean_squared_error(test_y.transpose(),test_y_pred.transpose(),multioutput='raw_values'))\n",
        "\n",
        "rmse_test_instances_sorted_indices=np.argsort(rmse_test_instances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipGTyf2l8hE"
      },
      "source": [
        "The following code shows the best predictions returned by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl6U2xV1l9Vc"
      },
      "source": [
        "with np.printoptions(precision=1, suppress=True):\n",
        "  print('Best RMSE:')\n",
        "  print(rmse_test_instances[rmse_test_instances_sorted_indices[:row_count]])\n",
        "\n",
        "  print('True values:')\n",
        "  print(test_y[rmse_test_instances_sorted_indices[:row_count]])\n",
        "\n",
        "  print('Predicted values:')\n",
        "  print(test_y_pred[rmse_test_instances_sorted_indices[:row_count]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqfb0Paoi3E1"
      },
      "source": [
        "The following code shows the worst predictions of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wtx7OaUi3Y9"
      },
      "source": [
        "with np.printoptions(precision=1, suppress=True):\n",
        "  print('Worst RMSE:')\n",
        "  print(rmse_test_instances[rmse_test_instances_sorted_indices[-row_count:]])\n",
        "\n",
        "  print('True values:')\n",
        "  print(test_y[rmse_test_instances_sorted_indices[-row_count:]])\n",
        "\n",
        "  print('Predicted values:')\n",
        "  print(test_y_pred[rmse_test_instances_sorted_indices[-row_count:]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVvx3IFXuihJ"
      },
      "source": [
        "# **Linear neural network**\n",
        "Before building a DNN model, we start with a simple neural network to apply a linear transformation:\n",
        "\n",
        "$\\boldsymbol{\\rm{y=Wx+b}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX59C9f7IBzh"
      },
      "source": [
        "## **Model definition**\n",
        "Training a model with Keras starts by defining the model architecture.\n",
        "\n",
        "The following function creates a simple linear neural network given:\n",
        "- the number of input features (*input_count*);\n",
        "- the number of output targets (*output_count*).\n",
        "\n",
        "In Keras, a sequential is a stack of layers where each layer has exactly one input and one output. It can be created by passing a list of layers to the  constructor [**keras.Sequential**](https://keras.io/guides/sequential_model/).\n",
        "\n",
        "[**Keras layers API**](https://keras.io/api/layers/) offers a wide range of built-in layers ready for use, including:\n",
        "- [**Input**](https://keras.io/api/layers/core_layers/input/) - the input of the model. Note that, you can also omit the **Input** layer. In that case the model doesn't have any weights until the first call to a training/evaluation method (since it is not yet built);\n",
        "- [**Dense**](https://keras.io/api/layers/core_layers/dense/) - a fully-connected layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyvD8ZXyJjWI"
      },
      "source": [
        "def build_linear_nn(input_count,output_count):\n",
        "\tmodel = keras.Sequential(\n",
        "        [\n",
        "          layers.Input(shape=(input_count)),\n",
        "          layers.Dense(output_count)\n",
        "        ]\n",
        "      )\n",
        "\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSu5Cm7Tux9Y"
      },
      "source": [
        "## **Model creation**\n",
        "The following code creates a linear neural network by calling the **build_linear_nn** function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-xNNWqNyij5"
      },
      "source": [
        "linear_nn=build_linear_nn(train_x.shape[1],train_y.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4mVldVtuzaq"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed using the [**summary**](https://keras.io/api/models/model/#summary-method) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4UnLKDczExw"
      },
      "source": [
        "linear_nn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmL4MxfmvD-w"
      },
      "source": [
        "The summary is useful for simple models, but can be confusing for complex models.\n",
        "\n",
        "Function [**keras.utils.plot_model**](https://keras.io/api/utils/model_plotting_utils/) creates a plot of the neural network graph that can make more complex models easier to understand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dq7lxelKI_O"
      },
      "source": [
        "keras.utils.plot_model(linear_nn,show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxMmlUtdvWzR"
      },
      "source": [
        "## **Model compilation**\n",
        "The compilation is the final step in configuring the model for training. \n",
        "\n",
        "The following code use the [**compile**](https://keras.io/api/models/model_training_apis/#compile-method) method to compile the model.\n",
        "The important arguments are:\n",
        "- the optimization algorithm (*optimizer*);\n",
        "- the loss function (*loss*);\n",
        "- the metrics used to evaluate the performance of the model (*metrics*).\n",
        "\n",
        "The most common [optimization algorithms](https://keras.io/api/optimizers/#available-optimizers), [loss functions](https://keras.io/api/losses/#available-losses) and [metrics](https://keras.io/api/metrics/#available-metrics) are already available in Keras. You can either pass them to **compile** as an instance or by the corresponding string identifier. In the latter case, the default parameters will be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjQvs7poKRmt"
      },
      "source": [
        "linear_nn.compile(loss='mse', optimizer='SGD',metrics=[keras.metrics.RootMeanSquaredError(name='rmse')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWSmscPv9sQQ"
      },
      "source": [
        "## **Split data into training and validation sets**\n",
        "In order to avoid overfitting during training, it is necessary to have a separate dataset (called validation set), in addition to the training and test datasets, to choose the optimal value for the hyperparameters.\n",
        "\n",
        "![alt text](https://biolab.csr.unibo.it/ferrara/Courses/DL/Tutorials/Regression/TrainValTestSets.png)\n",
        "\n",
        "For this reason, *train_x* and *train_y* are divided into training and validation sets using the **train_test_split** function provided by Scikit-learn.\n",
        "\n",
        "The *val_size* variable represents the percentage (or the absolute number) of patterns to include in the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVhSEM7t_IYB"
      },
      "source": [
        "val_size=0.25\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size = val_size,random_state = 1,shuffle=True)\n",
        "\n",
        "print('Train feature shape: ',train_x.shape)\n",
        "print('Train target shape: ',train_y.shape)\n",
        "print('Validation feature shape: ',val_x.shape)\n",
        "print('Validation target shape: ',val_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLcRYwsvv8Ih"
      },
      "source": [
        "## **Training**\n",
        "Now we are ready to train our model by calling the [**fit**](https://keras.io/api/models/model_training_apis/#fit-method) method.\n",
        "\n",
        "It trains the model for a fixed number of epochs (*epoch_count*) using the training set (*train_x* and *train_y*) divided into mini-batches of *batch_size* elements. During the training process, the performances will be evaluated on both training and validation (*train_x* and *val_x*) sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwMdnAYvzzu5"
      },
      "source": [
        "epoch_count = 2\n",
        "batch_size = 512\n",
        "\n",
        "history = linear_nn.fit(train_x, train_y,validation_data=(val_x,val_y), epochs=epoch_count, batch_size=batch_size,shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAPutxYgwySA"
      },
      "source": [
        "The neural network does not converge. This is because the features present values in very different ranges (as shown in the table of statistics).\n",
        "\n",
        "This happens because the features are multiplied by the model weights. So the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n",
        "\n",
        "Although a model might converge without feature normalization, normalization makes training much more stable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIwkCGHW0Gqn"
      },
      "source": [
        "### **Data normalization**\n",
        "It is good practice to normalize features that use different scales and ranges.\n",
        "\n",
        "Scikit-learn library provides the class [**StandardScaler**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to normalize features by removing the mean and scaling to unit variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUh7Gi78KWDj"
      },
      "source": [
        "scaler = StandardScaler().fit(train_x)\n",
        "train_x = scaler.transform(train_x)\n",
        "val_x=scaler.transform(val_x)\n",
        "test_x = scaler.transform(test_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2KDHgm8VwwK"
      },
      "source": [
        "Once normalized the features, the training process can be launched again.\n",
        "\n",
        "<u>Note that, it is necessary to create and compile a new model before executing the training process, otherwise it will be performed on a model already trained.</u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr6aqgXbMWvf"
      },
      "source": [
        "epoch_count = 10\n",
        "batch_size = 512\n",
        "\n",
        "linear_nn=build_linear_nn(train_x.shape[1],train_y.shape[1])\n",
        "\n",
        "linear_nn.compile(loss='mse', optimizer='SGD',metrics=[keras.metrics.RootMeanSquaredError(name='rmse')])\n",
        "\n",
        "history = linear_nn.fit(train_x, train_y,validation_data=(val_x,val_y), epochs=epoch_count, batch_size=batch_size,shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Dep2ouX24E"
      },
      "source": [
        "### **Visualize the training process**\n",
        "We can learn a lot about our model by observing the graph of its performance over time during training.\n",
        "\n",
        "The **fit** method returns an object (*history*) containing loss and metrics values at successive epochs for both training and validation sets.\n",
        "\n",
        "The following code calls the **plot_history** function defined above to draw in a graph the loss and RMSE trend over epochs on both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhm6l_k9PQD3"
      },
      "source": [
        "plot_history(history,'rmse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2gYIz9wYdTS"
      },
      "source": [
        "## **Performance evaluation**\n",
        "The following code calls the [**predict**](https://keras.io/api/models/model_training_apis/#predict-method) method to generate the predictions (*train_y_pred*, *val_y_pred* and *test_y_pred*) of the training, validation and test sets (*train_x*, *val_x* and *test_x*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X7edKTaYwLQ"
      },
      "source": [
        "train_y_pred=linear_nn.predict(train_x)\n",
        "val_y_pred=linear_nn.predict(val_x)\n",
        "test_y_pred=linear_nn.predict(test_x)\n",
        "\n",
        "print('Train predictions shape: ',train_y_pred.shape)\n",
        "print('Validation predictions shape: ',val_y_pred.shape)\n",
        "print('Test predictions shape: ',test_y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYmYVzrY4YX"
      },
      "source": [
        "### **RMSE**\n",
        "The regression accuracy can be measured using the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLrH9v0pucX2"
      },
      "source": [
        "rmse_train = mean_squared_error(train_y,train_y_pred,squared=False)\n",
        "rmse_val = mean_squared_error(val_y,val_y_pred,squared=False)\n",
        "rmse_test = mean_squared_error(test_y,test_y_pred,squared=False)\n",
        "\n",
        "print('RMSE - Train: {:.3f} Val: {:.3f} Test: {:.3f}'.format(rmse_train,rmse_val,rmse_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js1uEWOFZM4T"
      },
      "source": [
        "### **True vs predicted values and error distributions**\n",
        "To better analyze the model performance on the test set, it is useful to plot the predicted and the true values and to visualize the error distribution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHbUx-I6TTU4"
      },
      "source": [
        "plot_prediction_results(test_y,test_y_pred,target_data,200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucdAPENRZcuk"
      },
      "source": [
        "### **Best and worst predictions**\n",
        "To select best and worst predictions the RMSE value for each test instance is computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-IQTJOrZhTM"
      },
      "source": [
        "rmse_test_instances=np.sqrt(mean_squared_error(test_y.transpose(),test_y_pred.transpose(),multioutput='raw_values'))\n",
        "\n",
        "rmse_test_instances_sorted_indices=np.argsort(rmse_test_instances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iouTF8fNZlXx"
      },
      "source": [
        "The following code shows the best predictions returned by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LYF-RLAZltU"
      },
      "source": [
        "with np.printoptions(precision=1, suppress=True):\n",
        "  print('RMSE:')\n",
        "  print(rmse_test_instances[rmse_test_instances_sorted_indices[:row_count]])\n",
        "\n",
        "  print('True values:')\n",
        "  print(test_y[rmse_test_instances_sorted_indices[:row_count]])\n",
        "\n",
        "  print('Predicted values:')\n",
        "  print(test_y_pred[rmse_test_instances_sorted_indices[:row_count]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkUKv82pZ13W"
      },
      "source": [
        "The following code shows the worst predictions of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_-rfTYcZ2DM"
      },
      "source": [
        "with np.printoptions(precision=1, suppress=True):\n",
        "  print('RMSE:')\n",
        "  print(rmse_test_instances[rmse_test_instances_sorted_indices[-row_count:]])\n",
        "\n",
        "  print('True values:')\n",
        "  print(test_y[rmse_test_instances_sorted_indices[-row_count:]])\n",
        "\n",
        "  print('Predicted values:')\n",
        "  print(test_y_pred[rmse_test_instances_sorted_indices[-row_count:]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fup6Z6CkaAyL"
      },
      "source": [
        "# **Deep neural network**\n",
        "The previous section implemented a simple linear neural network.\n",
        "\n",
        "This section implements a DNN model. The code is basically the same except the model is expanded to include some *hidden* non-linear layers. The nonlinearity is introduced using the *ReLU* activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlePJ7AGgZ4G"
      },
      "source": [
        "## **Model definition**\n",
        "The following function creates a DNN model given:\n",
        "- the number of input features (*input_count*);\n",
        "- the number of output targets (*output_count*);\n",
        "- the number of neurons for each hidden layer (*neuron_count_per_hidden_layer*);\n",
        "- the string identifier of the activation function of the hidden layers (*activation*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOAobpA3xF2h"
      },
      "source": [
        "def build_dnn(input_count,output_count,neuron_count_per_hidden_layer=[128,128],activation='relu'):\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Input(shape=(input_count)))\n",
        "\n",
        "  for n in neuron_count_per_hidden_layer:\n",
        "    model.add(layers.Dense(n,activation=activation))\n",
        "\n",
        "  model.add(layers.Dense(output_count))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpey1ycRgkws"
      },
      "source": [
        "## **Model creation**\n",
        "The following code creates a DNN model by calling the **build_dnn** function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jONMjoRMYJk5"
      },
      "source": [
        "dnn=build_dnn(train_x.shape[1],train_y.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_lQ9UrQmboZ"
      },
      "source": [
        "## **Model visualization**\n",
        "A string summary of the network can be printed by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixSbGZdoYRXC"
      },
      "source": [
        "dnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCx-418NmqZY"
      },
      "source": [
        "Alternatively, a plot of the neural network graph can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku2Bhg6HYV0k"
      },
      "source": [
        "keras.utils.plot_model(dnn,show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ7xBSBJnHNZ"
      },
      "source": [
        "## **Model compilation**\n",
        "The following code compiles the model as already done for the linear neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEwa0Veqy-wg"
      },
      "source": [
        "dnn.compile(loss='mse', optimizer='SGD',metrics=[keras.metrics.RootMeanSquaredError(name='rmse')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co-VgxMgouCB"
      },
      "source": [
        "## **Training**\n",
        "Now we are ready to train our model by calling the **fit** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhpiU0XlYpC0"
      },
      "source": [
        "epoch_count = 2\n",
        "batch_size = 512\n",
        "\n",
        "history = dnn.fit(train_x, train_y,validation_data=(val_x,val_y), epochs=epoch_count, batch_size=batch_size,shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3qqCgw1oYvb"
      },
      "source": [
        "The neural network does not converge. This is because the learning rate is too high.\n",
        "\n",
        "The learning rate needs to be reduced before the training process can be launched again.\n",
        "\n",
        "<u>Note that, it is necessary to create and compile a new model before executing the training process, otherwise it will be performed on a model already trained.</u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvGrYFMRpnlH"
      },
      "source": [
        "epoch_count = 5\n",
        "batch_size = 512\n",
        "learning_rate=0.0001\n",
        "\n",
        "dnn=build_dnn(train_x.shape[1],train_y.shape[1])\n",
        "\n",
        "optimizer=keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "dnn.compile(loss='mse', optimizer=optimizer,metrics=[keras.metrics.RootMeanSquaredError(name='rmse')])\n",
        "\n",
        "history = dnn.fit(train_x, train_y,validation_data=(val_x,val_y), epochs=epoch_count, batch_size=batch_size,shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLWBwFbADupo"
      },
      "source": [
        "### **Stop the training process in advance**\n",
        "Break training when a metric or the loss has stopped improving on the validation set, helps to avoid overfitting.\n",
        "\n",
        "For this purpose, Keras provides a class called [**EarlyStopping**](https://keras.io/api/callbacks/early_stopping/). Important class parameters are:\n",
        "- *monitor* - the name of the metric or the loss to be observed; \n",
        "- *patience* - the number of epochs with no improvement after which training will be stopped;\n",
        "- *restore_best_weights* - whether to restore model weights from the epoch with the best value of the monitored quantity.\n",
        "\n",
        "Once created an instance of the **EarlyStopping** class, it can be passed to the **fit** method in the *callbacks* parameter.\n",
        "\n",
        "<u>Note that, it is necessary to create and compile a new model before executing the training process, otherwise it will be performed on a model already trained.</u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1E5eHhsECqd"
      },
      "source": [
        "epoch_count = 100\n",
        "batch_size = 512\n",
        "learning_rate=0.0001\n",
        "patience=5\n",
        "\n",
        "dnn=build_dnn(train_x.shape[1],train_y.shape[1])\n",
        "\n",
        "optimizer=keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "dnn.compile(loss='mse', optimizer=optimizer,metrics=[keras.metrics.RootMeanSquaredError(name='rmse')])\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "history = dnn.fit(train_x, train_y,validation_data=(val_x,val_y), epochs=epoch_count, batch_size=batch_size,shuffle = True,callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-isBS04Qpn4_"
      },
      "source": [
        "### **Visualize the training process**\n",
        "The following code calls the **plot_history** function defined above to draw in a graph the loss and RMSE trend over epochs on both training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ab0IeMYY2DC"
      },
      "source": [
        "plot_history(history,'rmse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvkQQ1r_OgNz"
      },
      "source": [
        "## **Performance evaluation**\n",
        "The following code calls the **predict** method to generate the predictions (*train_y_pred*, *val_y_pred* and *test_y_pred*) of the training, validation and test sets (*train_x*, *val_x* and *test_x*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC-q1l43PBX8"
      },
      "source": [
        "train_y_pred=dnn.predict(train_x)\n",
        "val_y_pred=dnn.predict(val_x)\n",
        "test_y_pred=dnn.predict(test_x)\n",
        "\n",
        "print('Train predictions shape: ',train_y_pred.shape)\n",
        "print('Validation predictions shape: ',val_y_pred.shape)\n",
        "print('Test predictions shape: ',test_y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE2X60F7PFRq"
      },
      "source": [
        "### **RMSE**\n",
        "The regression accuracy can be measured using the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VifbQjpWPIPr"
      },
      "source": [
        "rmse_train = mean_squared_error(train_y,train_y_pred,squared=False)\n",
        "rmse_val = mean_squared_error(val_y,val_y_pred,squared=False)\n",
        "rmse_test = mean_squared_error(test_y,test_y_pred,squared=False)\n",
        "\n",
        "print('RMSE - Train: {:.3f} Val: {:.3f} Test: {:.3f}'.format(rmse_train,rmse_val,rmse_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-HtHCwpPN_S"
      },
      "source": [
        "### **True vs predicted values and error distributions**\n",
        "To better analyze the model performance on the test set, it is useful to plot the predicted and the true values and to visualize the error distribution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zReT67JiZktn"
      },
      "source": [
        "plot_prediction_results(test_y,test_y_pred,target_data,200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79r0haj6PRgi"
      },
      "source": [
        "### **Best and worst predictions**\n",
        "To select best and worst predictions the RMSE value for each test instance is computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzc-AXpHPWX5"
      },
      "source": [
        "rmse_test_instances=np.sqrt(mean_squared_error(test_y.transpose(),test_y_pred.transpose(),multioutput='raw_values'))\n",
        "\n",
        "rmse_test_instances_sorted_indices=np.argsort(rmse_test_instances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xWs9B4sPZRZ"
      },
      "source": [
        "The following code shows the best predictions returned by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV3IQ85UPZjZ"
      },
      "source": [
        "with np.printoptions(precision=1, suppress=True):\n",
        "  print('RMSE:')\n",
        "  print(rmse_test_instances[rmse_test_instances_sorted_indices[:row_count]])\n",
        "\n",
        "  print('True values:')\n",
        "  print(test_y[rmse_test_instances_sorted_indices[:row_count]])\n",
        "\n",
        "  print('Predicted values:')\n",
        "  print(test_y_pred[rmse_test_instances_sorted_indices[:row_count]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni87UpmfPgUb"
      },
      "source": [
        "The following code shows the worst predictions of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0g9I_nLPgbE"
      },
      "source": [
        "with np.printoptions(precision=1, suppress=True):\n",
        "  print('RMSE:')\n",
        "  print(rmse_test_instances[rmse_test_instances_sorted_indices[-row_count:]])\n",
        "\n",
        "  print('True values:')\n",
        "  print(test_y[rmse_test_instances_sorted_indices[-row_count:]])\n",
        "\n",
        "  print('Predicted values:')\n",
        "  print(test_y_pred[rmse_test_instances_sorted_indices[-row_count:]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZNeB-FyTJst"
      },
      "source": [
        "# **Exercise 1**\n",
        "Improve the performance of the DNN model. It is recommended to evaluate the following hyperparameters (listed in priority order):\n",
        "1. the depth of the network and the number of neurons per hidden layer (*neuron_count_per_hidden_layer*);\n",
        "2. the number of training epochs (*epoch_count*);\n",
        "3. the optimization algorithm (*optimizer*);\n",
        "4. the learning rate (*learning_rate*);\n",
        "5. the mini-batch size (*batch_size*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7Vx1gB4UbB7"
      },
      "source": [
        "# **Exercise 2**\n",
        "Solve another regression problem chosen from the following list:\n",
        "- [Seoul Bike Sharing Demand Data Set](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand);\n",
        "- [\n",
        "Bike Sharing Dataset Data Set](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset);\n",
        "- [PM2.5 Data of Five Chinese Cities Data Set](https://archive.ics.uci.edu/ml/datasets/PM2.5+Data+of+Five+Chinese+Cities);\n",
        "- [\n",
        "Metro Interstate Traffic Volume Data Set](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume);\n",
        "- [Wine Quality Data Set](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)."
      ]
    }
  ]
}